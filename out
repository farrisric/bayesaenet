nohup: ignoring input
[[36m2024-05-06 10:35:28,209[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-05-06 10:35:28,210[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodule.aenet_datamodule.AenetDataModule>[0m
0.24999999999999997 0.1 0.65
[12833, 10600, 4868, 5601, 6351]
0.24999999999999997 0.1 0.65
[297, 7446, 3021, 11251, 9173]
[[36m2024-05-06 10:35:44,594[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.bnn.BNN>[0m
[[36m2024-05-06 10:35:44,610[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-05-06 10:35:44,611[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2024-05-06 10:35:44,617[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2024-05-06 10:35:44,619[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2024-05-06 10:35:44,620[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2024-05-06 10:35:44,621[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-05-06 10:35:44,622[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-05-06 10:35:44,637[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-05-06 10:35:44,715[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-05-06 10:35:44,912[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
Restoring states from the checkpoint path at /home/riccardo/bin/repos/aenet-bnn/src/logs/train_lrt/runs/2024-05-03_14-45-16/checkpoints/epoch_305-step_11628.ckpt
/home/riccardo/anaconda3/envs/bayesian/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: The dirpath has changed from '/home/riccardo/bin/repos/aenet-bnn/src/logs/train_lrt/runs/2024-05-03_14-45-16/checkpoints' to '/home/riccardo/bin/repos/aenet-bnn/src/logs/train_lrt/runs/2024-05-06_10-35-28/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
/home/riccardo/anaconda3/envs/bayesian/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type       â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net                           â”‚ NetAtom    â”‚  1.9 K â”‚
â”‚ 1  â”‚ net.linear                    â”‚ Identity   â”‚      0 â”‚
â”‚ 2  â”‚ net.tanh                      â”‚ Tanh       â”‚      0 â”‚
â”‚ 3  â”‚ net.sigmoid                   â”‚ Sigmoid    â”‚      0 â”‚
â”‚ 4  â”‚ net.functions                 â”‚ ModuleList â”‚  1.9 K â”‚
â”‚ 5  â”‚ net.functions.0               â”‚ Sequential â”‚    966 â”‚
â”‚ 6  â”‚ net.functions.0.Linear_Sp1_F1 â”‚ Linear     â”‚    795 â”‚
â”‚ 7  â”‚ net.functions.0.Linear_Sp1_F2 â”‚ Linear     â”‚    160 â”‚
â”‚ 8  â”‚ net.functions.0.Linear_Sp1_F3 â”‚ Linear     â”‚     11 â”‚
â”‚ 9  â”‚ net.functions.1               â”‚ Sequential â”‚    966 â”‚
â”‚ 10 â”‚ net.functions.1.Linear_Sp2_F1 â”‚ Linear     â”‚    795 â”‚
â”‚ 11 â”‚ net.functions.1.Linear_Sp2_F2 â”‚ Linear     â”‚    160 â”‚
â”‚ 12 â”‚ net.functions.1.Linear_Sp2_F3 â”‚ Linear     â”‚     11 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Restored all states from the checkpoint at /home/riccardo/bin/repos/aenet-bnn/src/logs/train_lrt/runs/2024-05-03_14-45-16/checkpoints/epoch_305-step_11628.ckpt
/home/riccardo/anaconda3/envs/bayesian/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (38) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/riccardo/anaconda3/envs/bayesian/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
