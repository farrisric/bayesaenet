/home/g15telari/.conda/envs/bayesian/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-01-29 12:52:39,777] Using an existing study with name 'bnn_lrt' instead of creating a new one.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/g15telari/.conda/envs/bayesian/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:183: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15telari/.conda/envs/bayesian/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15telari/.conda/envs/bayesian/lib/python3.11/site-packages/tyxe/likelihoods.py:203: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)
  agg_scale = precision.reciprocal().mean(dim).add(loc.var(dim)).sqrt()
/home/g15telari/.conda/envs/bayesian/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:134: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 12:53:32,641] Trial 7 finished with value: 10416.8154296875 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 0.21592388039429916, 'q_scale': 0.0036303760732131728, 'batch_size': 256}. Best is trial 7 with value: 10416.8154296875.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/g15telari/.conda/envs/bayesian/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 12:54:29,398] Trial 8 finished with value: 8847.0 and parameters: {'pretrain_epochs': 0, 'lr': 5.195586024325663e-05, 'mc_samples_train': 2, 'prior_scale': 0.16250224524496343, 'q_scale': 0.0005501758328845987, 'batch_size': 512}. Best is trial 8 with value: 8847.0.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/g15telari/.conda/envs/bayesian/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 12:55:35,082] Trial 9 finished with value: 18308.529296875 and parameters: {'pretrain_epochs': 0, 'lr': 5.367498945698204e-05, 'mc_samples_train': 1, 'prior_scale': 0.04232733477101895, 'q_scale': 0.0073498792462323385, 'batch_size': 128}. Best is trial 8 with value: 8847.0.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 12:57:21,706] Trial 10 finished with value: 240.94915771484375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005473369642905611, 'mc_samples_train': 2, 'prior_scale': 0.017549127225111354, 'q_scale': 0.002561662685923485, 'batch_size': 128}. Best is trial 10 with value: 240.94915771484375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 12:58:22,584] Trial 11 finished with value: 5236.3369140625 and parameters: {'pretrain_epochs': 0, 'lr': 1.3170990774640101e-05, 'mc_samples_train': 1, 'prior_scale': 0.14015302623589543, 'q_scale': 0.001546142648648774, 'batch_size': 128}. Best is trial 10 with value: 240.94915771484375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 12:59:24,205] Trial 12 finished with value: 10185.1923828125 and parameters: {'pretrain_epochs': 0, 'lr': 1.674127903856e-05, 'mc_samples_train': 1, 'prior_scale': 0.01026814371296241, 'q_scale': 0.0017174472922154307, 'batch_size': 128}. Best is trial 10 with value: 240.94915771484375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:00:29,033] Trial 13 finished with value: 6610.50146484375 and parameters: {'pretrain_epochs': 0, 'lr': 3.71964865643354e-05, 'mc_samples_train': 1, 'prior_scale': 0.021500386108909676, 'q_scale': 0.0005816140699158816, 'batch_size': 128}. Best is trial 10 with value: 240.94915771484375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:03:54,117] Trial 14 finished with value: 1001.8811645507812 and parameters: {'pretrain_epochs': 0, 'lr': 0.00029994809793642315, 'mc_samples_train': 1, 'prior_scale': 0.015205659078436342, 'q_scale': 0.0002873671566774025, 'batch_size': 32}. Best is trial 10 with value: 240.94915771484375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:05:42,986] Trial 15 finished with value: 223585.828125 and parameters: {'pretrain_epochs': 0, 'lr': 7.478015213333154e-05, 'mc_samples_train': 2, 'prior_scale': 0.1156399334005538, 'q_scale': 0.002582231235363856, 'batch_size': 128}. Best is trial 10 with value: 240.94915771484375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:06:47,020] Trial 16 finished with value: 911.1034545898438 and parameters: {'pretrain_epochs': 0, 'lr': 2.01904291186572e-05, 'mc_samples_train': 1, 'prior_scale': 0.41347469058863023, 'q_scale': 0.0009135206248796449, 'batch_size': 128}. Best is trial 10 with value: 240.94915771484375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:09:45,957] Trial 17 finished with value: 285.9009704589844 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008289681104465045, 'mc_samples_train': 2, 'prior_scale': 0.03823730602613338, 'q_scale': 0.00011667005469831107, 'batch_size': 64}. Best is trial 10 with value: 240.94915771484375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:12:40,032] Trial 18 finished with value: 152.4683074951172 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009233768742572698, 'mc_samples_train': 2, 'prior_scale': 0.04155761071630431, 'q_scale': 0.000282794630712143, 'batch_size': 64}. Best is trial 18 with value: 152.4683074951172.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:15:55,462] Trial 19 finished with value: 119.87238311767578 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008502926265357448, 'mc_samples_train': 2, 'prior_scale': 0.038872148158361784, 'q_scale': 0.00020149174745737484, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:19:00,242] Trial 20 finished with value: 182.0004119873047 and parameters: {'pretrain_epochs': 0, 'lr': 0.000237679721845533, 'mc_samples_train': 2, 'prior_scale': 0.05423451401435726, 'q_scale': 0.00012295598717575185, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:21:57,344] Trial 21 finished with value: 262.82373046875 and parameters: {'pretrain_epochs': 0, 'lr': 0.00015502343844271322, 'mc_samples_train': 2, 'prior_scale': 0.07858851068156882, 'q_scale': 0.0002793988911578557, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:25:00,033] Trial 22 finished with value: 1350.5101318359375 and parameters: {'pretrain_epochs': 0, 'lr': 0.000988930941074991, 'mc_samples_train': 2, 'prior_scale': 0.02921529625606385, 'q_scale': 0.0002304510696926467, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:28:03,817] Trial 23 finished with value: 215.74880981445312 and parameters: {'pretrain_epochs': 0, 'lr': 0.00044152349471208714, 'mc_samples_train': 2, 'prior_scale': 0.07763628198952244, 'q_scale': 0.00018798953024619426, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 13:28:54,648] Trial 24 pruned. Trial was pruned at epoch 13.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:34:32,669] Trial 25 finished with value: 399077.28125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005406716580137696, 'mc_samples_train': 2, 'prior_scale': 0.05333603148015694, 'q_scale': 0.0003438375037660087, 'batch_size': 32}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 13:35:15,625] Trial 26 pruned. Trial was pruned at epoch 13.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:38:20,466] Trial 27 finished with value: 157.07766723632812 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006478052804958617, 'mc_samples_train': 2, 'prior_scale': 0.09855675519924312, 'q_scale': 0.0009084026670046142, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:41:15,544] Trial 28 finished with value: 141.1053466796875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006999012061296653, 'mc_samples_train': 2, 'prior_scale': 0.08925068204770094, 'q_scale': 0.0009758331399319219, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:44:23,035] Trial 29 finished with value: 128.9434814453125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009822738838894843, 'mc_samples_train': 2, 'prior_scale': 0.05588603674646447, 'q_scale': 0.0004208241098184904, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:47:27,582] Trial 30 finished with value: 124.78716278076172 and parameters: {'pretrain_epochs': 0, 'lr': 0.000355095635140913, 'mc_samples_train': 2, 'prior_scale': 0.2219879222483795, 'q_scale': 0.0004222135288725894, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:50:29,557] Trial 31 finished with value: 122.05052185058594 and parameters: {'pretrain_epochs': 0, 'lr': 0.00041016150694920075, 'mc_samples_train': 2, 'prior_scale': 0.4474240109073772, 'q_scale': 0.0004496822904983401, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:53:25,031] Trial 32 finished with value: 190.47923278808594 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003673698761803349, 'mc_samples_train': 2, 'prior_scale': 0.4947197597705924, 'q_scale': 0.0006877834816318357, 'batch_size': 64}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 13:54:15,325] Trial 33 pruned. Trial was pruned at epoch 3.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 13:55:32,989] Trial 34 finished with value: 1842.6815185546875 and parameters: {'pretrain_epochs': 0, 'lr': 0.00011425979626599886, 'mc_samples_train': 2, 'prior_scale': 0.2576383286587367, 'q_scale': 0.00019423740348167414, 'batch_size': 256}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:01:18,044] Trial 35 finished with value: 203.35829162597656 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004457335897217443, 'mc_samples_train': 2, 'prior_scale': 0.19267685617852706, 'q_scale': 0.00013933896680443688, 'batch_size': 32}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:02:33,108] Trial 36 finished with value: 822.6741333007812 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002595978889062746, 'mc_samples_train': 2, 'prior_scale': 0.36431129257678685, 'q_scale': 0.0003639148823158211, 'batch_size': 256}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:03:29,281] Trial 37 finished with value: 4801.22509765625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00020197368136483485, 'mc_samples_train': 2, 'prior_scale': 0.2699974906378166, 'q_scale': 0.00953852085011636, 'batch_size': 512}. Best is trial 19 with value: 119.87238311767578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:06:29,609] Trial 38 finished with value: 114.2909927368164 and parameters: {'pretrain_epochs': 0, 'lr': 0.000699750841233633, 'mc_samples_train': 2, 'prior_scale': 0.0633355892251288, 'q_scale': 0.00044020601039185595, 'batch_size': 64}. Best is trial 38 with value: 114.2909927368164.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:09:30,402] Trial 39 finished with value: 113.63158416748047 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004508400849318655, 'mc_samples_train': 2, 'prior_scale': 0.22126123832240108, 'q_scale': 0.0007331480213181823, 'batch_size': 64}. Best is trial 39 with value: 113.63158416748047.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:12:42,389] Trial 40 finished with value: 217.75558471679688 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006562535399799725, 'mc_samples_train': 2, 'prior_scale': 0.12967088475385097, 'q_scale': 0.0006896551083382534, 'batch_size': 64}. Best is trial 39 with value: 113.63158416748047.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:15:46,865] Trial 41 finished with value: 254.5745849609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005246416728160125, 'mc_samples_train': 2, 'prior_scale': 0.16634135569838657, 'q_scale': 0.0014387568637267293, 'batch_size': 64}. Best is trial 39 with value: 113.63158416748047.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 14:16:23,831] Trial 42 pruned. Trial was pruned at epoch 10.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:18:13,490] Trial 43 finished with value: 177.04296875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007215554711822654, 'mc_samples_train': 1, 'prior_scale': 0.4807909475146314, 'q_scale': 0.00010009551624494411, 'batch_size': 64}. Best is trial 39 with value: 113.63158416748047.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:21:37,256] Trial 44 finished with value: 729.6215209960938 and parameters: {'pretrain_epochs': 0, 'lr': 3.549567604023254e-05, 'mc_samples_train': 2, 'prior_scale': 0.32306755693734457, 'q_scale': 0.001335854980800041, 'batch_size': 64}. Best is trial 39 with value: 113.63158416748047.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:23:20,181] Trial 45 finished with value: 122.57865905761719 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005761555036104578, 'mc_samples_train': 1, 'prior_scale': 0.03535324905877871, 'q_scale': 0.0005644472331402217, 'batch_size': 64}. Best is trial 39 with value: 113.63158416748047.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 14:24:51,753] Trial 46 pruned. Trial was pruned at epoch 5.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 14:25:10,034] Trial 47 pruned. Trial was pruned at epoch 5.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:26:46,840] Trial 48 finished with value: 109.4292221069336 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005895919819344915, 'mc_samples_train': 1, 'prior_scale': 0.03352173831519786, 'q_scale': 0.0005452138985963308, 'batch_size': 64}. Best is trial 48 with value: 109.4292221069336.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:28:24,531] Trial 49 finished with value: 98.90962982177734 and parameters: {'pretrain_epochs': 0, 'lr': 0.000479176835878153, 'mc_samples_train': 1, 'prior_scale': 0.04655190078294168, 'q_scale': 0.0011533711221256957, 'batch_size': 64}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:30:01,493] Trial 50 finished with value: 108.39057922363281 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005161328263446798, 'mc_samples_train': 1, 'prior_scale': 0.04925545309022382, 'q_scale': 0.001192147326342391, 'batch_size': 64}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:31:38,655] Trial 51 finished with value: 107.1543197631836 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005132181404886026, 'mc_samples_train': 1, 'prior_scale': 0.047668101519369864, 'q_scale': 0.0011976027839201656, 'batch_size': 64}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 14:32:12,564] Trial 52 pruned. Trial was pruned at epoch 6.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 14:32:43,465] Trial 53 pruned. Trial was pruned at epoch 4.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:34:20,373] Trial 54 finished with value: 141.35952758789062 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004865685348678195, 'mc_samples_train': 1, 'prior_scale': 0.02034511171511876, 'q_scale': 0.004676705379766735, 'batch_size': 64}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 14:34:44,843] Trial 55 pruned. Trial was pruned at epoch 11.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:36:24,315] Trial 56 finished with value: 114.49989318847656 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005562221157649965, 'mc_samples_train': 1, 'prior_scale': 0.02413493732606569, 'q_scale': 0.002279943142397755, 'batch_size': 64}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:37:23,707] Trial 57 finished with value: 11511.962890625 and parameters: {'pretrain_epochs': 0, 'lr': 1.263926227297739e-05, 'mc_samples_train': 1, 'prior_scale': 0.04613102607309985, 'q_scale': 0.000767745597208852, 'batch_size': 128}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:39:01,484] Trial 58 finished with value: 1958.8333740234375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006345714076296662, 'mc_samples_train': 1, 'prior_scale': 0.06036721629712861, 'q_scale': 0.0008847907940131668, 'batch_size': 64}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:40:38,658] Trial 59 finished with value: 838.104248046875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007819935572081113, 'mc_samples_train': 1, 'prior_scale': 0.07467811035796233, 'q_scale': 0.0011663401182664583, 'batch_size': 64}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:42:15,710] Trial 60 finished with value: 103.87527465820312 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004892606819863511, 'mc_samples_train': 1, 'prior_scale': 0.06753350752446152, 'q_scale': 0.0005855051866323676, 'batch_size': 64}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:43:51,987] Trial 61 finished with value: 110.63069152832031 and parameters: {'pretrain_epochs': 0, 'lr': 0.00047828045025814753, 'mc_samples_train': 1, 'prior_scale': 0.03997591896889042, 'q_scale': 0.0016787806447070407, 'batch_size': 64}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 14:44:22,816] Trial 62 pruned. Trial was pruned at epoch 4.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:47:14,594] Trial 63 finished with value: 2687.303466796875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003693028943888953, 'mc_samples_train': 1, 'prior_scale': 0.027911834437180576, 'q_scale': 0.0025139066034810145, 'batch_size': 32}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 14:47:48,962] Trial 64 pruned. Trial was pruned at epoch 5.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-01-29 14:49:25,197] Trial 65 finished with value: 4135.8818359375 and parameters: {'pretrain_epochs': 0, 'lr': 1.0444968654264178e-05, 'mc_samples_train': 1, 'prior_scale': 0.03459086789895112, 'q_scale': 0.0038638595119283004, 'batch_size': 64}. Best is trial 49 with value: 98.90962982177734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
[I 2025-01-29 14:50:01,334] Trial 66 pruned. Trial was pruned at epoch 5.
Traceback (most recent call last):
  File "/home/g15telari/TiO/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 15, in <module>
    from train import train
  File "/home/g15telari/TiO/bayesaenet/bnn_aenet/tasks/train.py", line 12, in <module>
    from bnn_aenet.models.bnn_h import NN
ModuleNotFoundError: No module named 'bnn_aenet.models.bnn_h'
