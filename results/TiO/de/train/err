Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Error executing job with overrides: ['task_name=TiO_train_de', 'experiment=nn', 'datamodule=TiO', 'trainer.min_epochs=10000', 'trainer.max_epochs=50000', 'datamodule.test_split=0.1', 'datamodule.valid_split=0.1', 'datamodule.batch_size=32', 'model.optimizer.lr=0.0013422359776024458', 'seed=1']
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 150, in main
    train(cfg, trial=None)
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (66) must match the size of tensor b (33) at non-singleton dimension 0

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['task_name=TiO_train_de', 'experiment=nn', 'datamodule=TiO', 'trainer.min_epochs=10000', 'trainer.max_epochs=50000', 'datamodule.test_split=0.1', 'datamodule.valid_split=0.1', 'datamodule.batch_size=32', 'model.optimizer.lr=0.0013422359776024458', 'seed=1']
Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
Restoring states from the checkpoint path at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_12-24-27/0/checkpoints/epoch_6537-step_1274910.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_12-24-27/0/checkpoints/epoch_6537-step_1274910.ckpt
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
Restoring states from the checkpoint path at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_12-24-27/1/checkpoints/epoch_6033-step_1176630.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_12-24-27/1/checkpoints/epoch_6033-step_1176630.ckpt
Error executing job with overrides: ['task_name=TiO_train_de', 'experiment=nn', 'datamodule=TiO', 'trainer.min_epochs=10000', 'trainer.max_epochs=50000', 'datamodule.test_split=0.1', 'datamodule.valid_split=0.1', 'datamodule.batch_size=32', 'model.optimizer.lr=0.0013422359776024458', 'seed=3']
Error in call to target 'bnn_aenet.models.nets.network.NetAtom':
TypeError("NetAtom.__init__() got an unexpected keyword argument 'dropout'")
full_key: model.net

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
Restoring states from the checkpoint path at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/0/checkpoints/epoch_8954-step_1746225.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/0/checkpoints/epoch_8954-step_1746225.ckpt
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
Restoring states from the checkpoint path at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/1/checkpoints/epoch_8097-step_1579110.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/1/checkpoints/epoch_8097-step_1579110.ckpt
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
Restoring states from the checkpoint path at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/2/checkpoints/epoch_9505-step_1853670.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/2/checkpoints/epoch_9505-step_1853670.ckpt
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
Restoring states from the checkpoint path at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/3/checkpoints/epoch_8625-step_1682070.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/3/checkpoints/epoch_8625-step_1682070.ckpt
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
Restoring states from the checkpoint path at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/4/checkpoints/epoch_7497-step_1462110.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/4/checkpoints/epoch_7497-step_1462110.ckpt
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
Restoring states from the checkpoint path at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/5/checkpoints/epoch_9978-step_1945905.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/5/checkpoints/epoch_9978-step_1945905.ckpt
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
Exception in thread Thread-13:
Exception in threading.excepthook:
Exception ignored in thread started by: <bound method Thread._bootstrap of <_AsyncWriterThread(Thread-13, started daemon 47134850520832)>>
Exception ignored in sys.unraisablehook: <built-in function unraisablehook>
Error executing job with overrides: ['task_name=TiO_train_de', 'experiment=nn', 'datamodule=TiO', 'trainer.min_epochs=10000', 'trainer.max_epochs=50000', 'datamodule.test_split=0.1', 'datamodule.valid_split=0.1', 'datamodule.batch_size=32', 'model.optimizer.lr=0.0013422359776024458', 'seed=7']
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1033, in _run_stage
    self.fit_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 141, in run
    self.on_advance_end(data_fetcher)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 295, in on_advance_end
    self.val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 142, in run
    return self.on_run_end()
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 265, in on_run_end
    self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py", line 150, in log_eval_end_metrics
    self.log_metrics(metrics)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py", line 118, in log_metrics
    logger.save()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loggers/tensorboard.py", line 213, in save
    super().save()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/fabric/loggers/tensorboard.py", line 293, in save
    self.experiment.flush()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py", line 1256, in flush
    writer.flush()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py", line 152, in flush
    self.event_writer.flush()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/event_file_writer.py", line 125, in flush
    self._async_writer.flush()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/event_file_writer.py", line 194, in flush
    self._check_worker_status()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/event_file_writer.py", line 212, in _check_worker_status
    raise exception
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/event_file_writer.py", line 244, in run
    self._run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/event_file_writer.py", line 275, in _run
    self._record_writer.write(data)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/record_writer.py", line 40, in write
    self._writer.write(header + header_crc + data + footer_crc)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 773, in write
    self.fs.append(self.filename, file_content, self.binary_mode)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 167, in append
    self._write(filename, file_content, "ab" if binary_mode else "a")
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 171, in _write
    with io.open(filename, mode, encoding=encoding) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error: b'/home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/6/tensorboard/version_0/events.out.tfevents.1740071696.g4noder12.18715.12'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 150, in main
    train(cfg, trial=None)
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 64, in _call_and_handle_interrupt
    _call_callback_hooks(trainer, "on_exception", exception)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 208, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/callbacks/progress/rich_progress.py", line 641, in on_exception
    self._stop_progress()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/callbacks/progress/rich_progress.py", line 614, in _stop_progress
    self.progress.stop()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/rich/progress.py", line 1163, in stop
    self.live.stop()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/rich/live.py", line 147, in stop
    with self.console:
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/rich/console.py", line 865, in __exit__
    self._exit_buffer()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/rich/console.py", line 823, in _exit_buffer
    self._check_buffer()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/rich/console.py", line 2065, in _check_buffer
    self.file.flush()
OSError: [Errno 5] Input/output error

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
