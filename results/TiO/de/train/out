[[36m2025-02-18 17:17:09,996[0m][[35mHYDRA[0m] Launching 10 jobs locally[0m
[[36m2025-02-18 17:17:09,997[0m][[35mHYDRA[0m] 	#0 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=1[0m
[[36m2025-02-18 17:17:10,838[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:17:10,838[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:17:21,359[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:17:24,106[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:17:24,117[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:17:24,125[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:17:24,127[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:17:24,128[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:17:24,129[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:17:24,129[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:17:24,155[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:17:24,476[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:17:25,472[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:17:28,600[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (66) must match the size of tensor b (33) at non-singleton dimension 0
[[36m2025-02-18 17:17:28,854[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:17:28,857[0m][[35mHYDRA[0m] 	#1 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=2[0m
[[36m2025-02-18 17:17:30,705[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:17:30,706[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:17:41,399[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:17:41,411[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:17:41,411[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:17:41,454[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:17:41,456[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:17:41,456[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:17:41,457[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:17:41,457[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:17:41,459[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:17:41,491[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:17:42,037[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:17:42,130[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 0
[[36m2025-02-18 17:17:42,156[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:17:42,161[0m][[35mHYDRA[0m] 	#2 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=3[0m
[[36m2025-02-18 17:17:42,512[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:17:42,513[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:17:53,589[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:17:53,597[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:17:53,598[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:17:53,602[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:17:53,604[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:17:53,605[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:17:53,608[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:17:53,609[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:17:53,612[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:17:53,633[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:17:53,642[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:17:53,694[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (66) must match the size of tensor b (33) at non-singleton dimension 0
[[36m2025-02-18 17:17:54,013[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:17:54,017[0m][[35mHYDRA[0m] 	#3 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=4[0m
[[36m2025-02-18 17:17:54,542[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:17:54,543[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:18:05,795[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:18:05,809[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:18:05,810[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:18:05,815[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:18:05,817[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:18:05,818[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:18:05,819[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:18:05,820[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:18:05,822[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:18:05,848[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:18:06,351[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:18:06,519[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (66) must match the size of tensor b (33) at non-singleton dimension 0
[[36m2025-02-18 17:18:06,560[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:18:06,564[0m][[35mHYDRA[0m] 	#4 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=5[0m
[[36m2025-02-18 17:18:07,235[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:18:07,236[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
[[36m2025-02-18 17:18:45,793[0m][[35mHYDRA[0m] Launching 10 jobs locally[0m
[[36m2025-02-18 17:18:45,794[0m][[35mHYDRA[0m] 	#0 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=1[0m
[[36m2025-02-18 17:18:46,533[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:18:46,534[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:18:57,373[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:19:00,268[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:19:00,268[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:19:00,273[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:19:00,274[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:19:00,275[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:19:00,275[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:19:00,275[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:19:00,289[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:19:00,595[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:19:02,142[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:19:04,448[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (66) must match the size of tensor b (33) at non-singleton dimension 0
[[36m2025-02-18 17:19:04,614[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:19:04,619[0m][[35mHYDRA[0m] 	#1 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=2[0m
[[36m2025-02-18 17:19:05,342[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:19:05,342[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:19:15,116[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:19:15,122[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:19:15,123[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:19:15,125[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:19:15,126[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:19:15,126[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:19:15,127[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:19:15,127[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:19:15,128[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:19:15,165[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:19:15,173[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:19:15,234[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 0
[[36m2025-02-18 17:19:15,292[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:19:15,295[0m][[35mHYDRA[0m] 	#2 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=3[0m
[[36m2025-02-18 17:19:15,512[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:19:15,513[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:19:25,562[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:19:25,569[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:19:25,569[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:19:25,572[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:19:25,573[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:19:25,573[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:19:25,574[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:19:25,574[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:19:25,576[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:19:25,592[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:19:25,646[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:19:25,819[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 0
[[36m2025-02-18 17:19:25,907[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:19:25,910[0m][[35mHYDRA[0m] 	#3 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=4[0m
[[36m2025-02-18 17:19:26,226[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:19:26,226[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:19:36,845[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:19:36,852[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:19:36,853[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:19:36,873[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:19:36,875[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:19:36,875[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:19:36,876[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:19:36,876[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:19:36,878[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:19:36,897[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:19:37,021[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:19:37,242[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (66) must match the size of tensor b (33) at non-singleton dimension 0
[[36m2025-02-18 17:19:37,249[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:19:37,252[0m][[35mHYDRA[0m] 	#4 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=5[0m
[[36m2025-02-18 17:19:38,138[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:19:38,138[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:19:48,355[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:19:48,370[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:19:48,371[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:19:48,375[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:19:48,376[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:19:48,377[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:19:48,378[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:19:48,378[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:19:48,379[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:19:48,412[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:19:48,425[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:19:48,499[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 0
[[36m2025-02-18 17:19:48,518[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:19:48,520[0m][[35mHYDRA[0m] 	#5 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=6[0m
[[36m2025-02-18 17:19:48,814[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:19:48,815[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:19:58,510[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:19:58,524[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:19:58,524[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:19:58,529[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:19:58,531[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:19:58,532[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:19:58,532[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:19:58,533[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:19:58,536[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:19:58,564[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:19:58,677[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:19:58,813[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (66) must match the size of tensor b (33) at non-singleton dimension 0
[[36m2025-02-18 17:19:58,818[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:19:58,821[0m][[35mHYDRA[0m] 	#6 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=7[0m
[[36m2025-02-18 17:19:59,028[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:19:59,029[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:20:08,831[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:20:08,838[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:20:08,839[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:20:08,841[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:20:08,842[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:20:08,842[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:20:08,843[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:20:08,843[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:20:08,844[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:20:08,862[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:20:08,872[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:20:08,912[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 0
[[36m2025-02-18 17:20:09,207[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:20:09,210[0m][[35mHYDRA[0m] 	#7 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=8[0m
[[36m2025-02-18 17:20:09,446[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:20:09,446[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:20:18,819[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:20:18,826[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:20:18,826[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:20:18,828[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:20:18,829[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:20:18,830[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:20:18,830[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:20:18,831[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:20:18,832[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:20:18,860[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:20:18,892[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:20:18,931[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 0
[[36m2025-02-18 17:20:18,934[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:20:18,936[0m][[35mHYDRA[0m] 	#8 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=9[0m
[[36m2025-02-18 17:20:19,219[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:20:19,219[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:20:28,870[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:20:28,877[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:20:28,877[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:20:28,880[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:20:28,881[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:20:28,881[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:20:28,882[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:20:28,882[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:20:28,883[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:20:28,898[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:20:28,908[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:20:28,949[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (66) must match the size of tensor b (33) at non-singleton dimension 0
[[36m2025-02-18 17:20:28,953[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-18 17:20:28,955[0m][[35mHYDRA[0m] 	#9 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=10[0m
[[36m2025-02-18 17:20:29,144[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-18 17:20:29,145[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-18 17:20:38,673[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-18 17:20:38,679[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-18 17:20:38,679[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-18 17:20:38,681[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-18 17:20:38,682[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-18 17:20:38,682[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-18 17:20:38,683[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-18 17:20:38,684[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-18 17:20:38,685[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-18 17:20:38,701[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-18 17:20:38,860[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                              ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                               │ NetAtom    │  2.7 K │
│ 1  │ net.networks                      │ ModuleList │  2.7 K │
│ 2  │ net.networks.0                    │ Sequential │  1.3 K │
│ 3  │ net.networks.0.Linear_Layer_1     │ Linear     │  1.1 K │
│ 4  │ net.networks.0.Activation_Layer_1 │ Tanh       │      0 │
│ 5  │ net.networks.0.Linear_Layer_2     │ Linear     │    240 │
│ 6  │ net.networks.0.Output_Layer       │ Linear     │     32 │
│ 7  │ net.networks.1                    │ Sequential │  1.3 K │
│ 8  │ net.networks.1.Linear_Layer_1     │ Linear     │  1.1 K │
│ 9  │ net.networks.1.Linear_Layer_2     │ Linear     │    240 │
│ 10 │ net.networks.1.Output_Layer       │ Linear     │     32 │
└────┴───────────────────────────────────┴────────────┴────────┘
Trainable params: 2.7 K                                                         
Non-trainable params: 0                                                         
Total params: 2.7 K                                                             
Total estimated model params size (MB): 0                                       

[[36m2025-02-18 17:20:39,056[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1031, in _run_stage
    self._run_sanity_check()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1060, in _run_sanity_check
    val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 291, in validation_step
    mse = self.step(batch)
          ^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 278, in step
    return get_rmse_atom(list_E_ann, grp_energy, grp_N_atom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/models/bnn.py", line 325, in get_rmse_atom
    mse_atom = (list_E_ann - grp_energy) ** 2 / grp_N_atom
                ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (66) must match the size of tensor b (33) at non-singleton dimension 0
[[36m2025-02-18 17:20:39,125[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 10:52:03,939[0m][[35mHYDRA[0m] Launching 10 jobs locally[0m
[[36m2025-02-19 10:52:03,940[0m][[35mHYDRA[0m] 	#0 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=1[0m
[[36m2025-02-19 10:52:04,145[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 10:52:04,145[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-19 10:52:22,349[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-19 10:52:22,406[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 10:52:22,476[0m][[35mHYDRA[0m] 	#1 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=2[0m
[[36m2025-02-19 10:52:24,049[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 10:52:24,049[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-19 10:52:24,054[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-19 10:52:24,131[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 10:52:24,143[0m][[35mHYDRA[0m] 	#2 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=3[0m
[[36m2025-02-19 10:52:24,471[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 10:52:24,471[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-19 10:52:24,473[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-19 10:52:24,501[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 10:52:24,513[0m][[35mHYDRA[0m] 	#3 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=4[0m
[[36m2025-02-19 10:52:25,138[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 10:52:25,138[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-19 10:52:25,140[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-19 10:52:25,175[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 10:52:25,196[0m][[35mHYDRA[0m] 	#4 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=5[0m
[[36m2025-02-19 10:52:25,577[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 10:52:25,577[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-19 10:52:25,579[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-19 10:52:25,586[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 10:52:25,595[0m][[35mHYDRA[0m] 	#5 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=6[0m
[[36m2025-02-19 10:52:25,908[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 10:52:25,908[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-19 10:52:25,910[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-19 10:52:25,921[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 10:52:25,928[0m][[35mHYDRA[0m] 	#6 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=7[0m
[[36m2025-02-19 10:52:26,168[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 10:52:26,169[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-19 10:52:26,170[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-19 10:52:26,229[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 10:52:26,231[0m][[35mHYDRA[0m] 	#7 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=8[0m
[[36m2025-02-19 10:52:26,553[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 10:52:26,553[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-19 10:52:26,555[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-19 10:52:26,693[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 10:52:26,706[0m][[35mHYDRA[0m] 	#8 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=9[0m
[[36m2025-02-19 10:52:27,076[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 10:52:27,077[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-19 10:52:27,079[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-19 10:52:27,092[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 10:52:27,099[0m][[35mHYDRA[0m] 	#9 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=10[0m
[[36m2025-02-19 10:52:27,360[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 10:52:27,360[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-19 10:52:27,362[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-19 10:52:27,433[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 12:24:28,439[0m][[35mHYDRA[0m] Launching 10 jobs locally[0m
[[36m2025-02-19 12:24:28,440[0m][[35mHYDRA[0m] 	#0 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=1[0m
[[36m2025-02-19 12:24:28,980[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 12:24:28,980[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 12:24:39,155[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 12:24:59,560[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-19 12:24:59,561[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-19 12:24:59,566[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-19 12:24:59,567[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-19 12:24:59,568[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-19 12:24:59,569[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-19 12:24:59,569[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-19 12:24:59,583[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-19 12:24:59,880[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-19 12:25:06,167[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  2.6 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  2.6 K │
│ 5  │ net.functions.0               │ Sequential │  1.3 K │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │  1.1 K │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    240 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     16 │
│ 9  │ net.functions.1               │ Sequential │  1.3 K │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │  1.1 K │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    240 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     16 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 2.6 K                                                         
Non-trainable params: 0                                                         
Total params: 2.6 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 9999/49999 ━━━━━━━━━━━━━━ 195/195 0:00:01 •     186.19it/s v_num: 0.000   
                                        0:00:00                  rmse/val: 6.817
                                                                 rmse/train:    
                                                                 4.376          
[[36m2025-02-19 15:37:33,250[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         rmse/test         │    3.3505202476424407     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24/24 0:00:00 • 0:00:00 372.83it/s 
[[36m2025-02-19 15:37:34,216[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_12-24-27/0/checkpoints/epoch_6537-step_1274910.ckpt[0m
[[36m2025-02-19 15:37:34,699[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 15:37:34,784[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_12-24-27/0[0m
[[36m2025-02-19 15:37:34,818[0m][[35mHYDRA[0m] 	#1 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=2[0m
[[36m2025-02-19 15:37:41,567[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 15:37:41,571[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 15:37:51,249[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 15:37:51,263[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-19 15:37:51,264[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-19 15:37:51,268[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-19 15:37:51,269[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-19 15:37:51,269[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-19 15:37:51,270[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-19 15:37:51,270[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-19 15:37:51,271[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-19 15:38:01,938[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-19 15:38:09,520[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  2.6 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  2.6 K │
│ 5  │ net.functions.0               │ Sequential │  1.3 K │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │  1.1 K │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    240 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     16 │
│ 9  │ net.functions.1               │ Sequential │  1.3 K │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │  1.1 K │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    240 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     16 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 2.6 K                                                         
Non-trainable params: 0                                                         
Total params: 2.6 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 9999/49999 ━━━━━━━━━━━━━━ 195/195 0:00:01 •     186.68it/s v_num: 0.000   
                                        0:00:00                  rmse/val: 6.779
                                                                 rmse/train:    
                                                                 4.738          
[[36m2025-02-19 18:14:45,314[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         rmse/test         │    4.4972172800794254     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24/24 0:00:00 • 0:00:00 781.20it/s 
[[36m2025-02-19 18:14:45,428[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_12-24-27/1/checkpoints/epoch_6033-step_1176630.ckpt[0m
[[36m2025-02-19 18:14:45,486[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 18:14:45,901[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_12-24-27/1[0m
[[36m2025-02-19 18:14:45,904[0m][[35mHYDRA[0m] 	#2 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=3[0m
[[36m2025-02-19 18:14:47,199[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 18:14:47,199[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 18:14:56,586[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 18:14:56,592[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: NetAtom.__init__() got an unexpected keyword argument 'dropout'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 59, in train
    model: LightningModule = hydra.utils.instantiate(
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
            ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.models.nets.network.NetAtom':
TypeError("NetAtom.__init__() got an unexpected keyword argument 'dropout'")
full_key: model.net
[[36m2025-02-19 18:14:56,765[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 18:14:56,767[0m][[35mHYDRA[0m] 	#3 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=4[0m
[[36m2025-02-19 18:14:58,873[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 18:14:58,873[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 18:15:08,258[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 18:15:08,263[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: NetAtom.__init__() got an unexpected keyword argument 'dropout'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 59, in train
    model: LightningModule = hydra.utils.instantiate(
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
            ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.models.nets.network.NetAtom':
TypeError("NetAtom.__init__() got an unexpected keyword argument 'dropout'")
full_key: model.net
[[36m2025-02-19 18:15:08,389[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 18:15:08,437[0m][[35mHYDRA[0m] 	#4 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=5[0m
[[36m2025-02-19 18:15:09,201[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 18:15:09,201[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 18:15:18,395[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 18:15:18,400[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: NetAtom.__init__() got an unexpected keyword argument 'dropout'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 59, in train
    model: LightningModule = hydra.utils.instantiate(
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
            ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.models.nets.network.NetAtom':
TypeError("NetAtom.__init__() got an unexpected keyword argument 'dropout'")
full_key: model.net
[[36m2025-02-19 18:15:18,404[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 18:15:18,419[0m][[35mHYDRA[0m] 	#5 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=6[0m
[[36m2025-02-19 18:15:21,627[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 18:15:21,628[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 18:15:30,930[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 18:15:30,935[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: NetAtom.__init__() got an unexpected keyword argument 'dropout'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 59, in train
    model: LightningModule = hydra.utils.instantiate(
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
            ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.models.nets.network.NetAtom':
TypeError("NetAtom.__init__() got an unexpected keyword argument 'dropout'")
full_key: model.net
[[36m2025-02-19 18:15:30,989[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 18:15:30,991[0m][[35mHYDRA[0m] 	#6 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=7[0m
[[36m2025-02-19 18:15:32,821[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 18:15:32,821[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 18:15:42,185[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 18:15:42,190[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: NetAtom.__init__() got an unexpected keyword argument 'dropout'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 59, in train
    model: LightningModule = hydra.utils.instantiate(
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
            ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.models.nets.network.NetAtom':
TypeError("NetAtom.__init__() got an unexpected keyword argument 'dropout'")
full_key: model.net
[[36m2025-02-19 18:15:42,241[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 18:15:42,243[0m][[35mHYDRA[0m] 	#7 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=8[0m
[[36m2025-02-19 18:15:42,873[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 18:15:42,873[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 18:15:52,308[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 18:15:52,313[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: NetAtom.__init__() got an unexpected keyword argument 'dropout'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 59, in train
    model: LightningModule = hydra.utils.instantiate(
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
            ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.models.nets.network.NetAtom':
TypeError("NetAtom.__init__() got an unexpected keyword argument 'dropout'")
full_key: model.net
[[36m2025-02-19 18:15:52,363[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 18:15:52,364[0m][[35mHYDRA[0m] 	#8 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=9[0m
[[36m2025-02-19 18:15:53,659[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 18:15:53,660[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 18:16:03,045[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 18:16:03,050[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: NetAtom.__init__() got an unexpected keyword argument 'dropout'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 59, in train
    model: LightningModule = hydra.utils.instantiate(
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
            ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.models.nets.network.NetAtom':
TypeError("NetAtom.__init__() got an unexpected keyword argument 'dropout'")
full_key: model.net
[[36m2025-02-19 18:16:03,154[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 18:16:03,156[0m][[35mHYDRA[0m] 	#9 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=10[0m
[[36m2025-02-19 18:16:03,494[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 18:16:03,494[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 18:16:12,811[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 18:16:12,816[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: NetAtom.__init__() got an unexpected keyword argument 'dropout'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 59, in train
    model: LightningModule = hydra.utils.instantiate(
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
            ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.models.nets.network.NetAtom':
TypeError("NetAtom.__init__() got an unexpected keyword argument 'dropout'")
full_key: model.net
[[36m2025-02-19 18:16:12,829[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-19 21:40:16,850[0m][[35mHYDRA[0m] Launching 10 jobs locally[0m
[[36m2025-02-19 21:40:16,851[0m][[35mHYDRA[0m] 	#0 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=1[0m
[[36m2025-02-19 21:40:17,076[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-19 21:40:17,077[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-19 21:40:28,445[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-19 21:40:31,151[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-19 21:40:31,151[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-19 21:40:31,159[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-19 21:40:31,162[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-19 21:40:31,163[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-19 21:40:31,164[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-19 21:40:31,165[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-19 21:40:31,183[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-19 21:40:31,452[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-19 21:40:31,884[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  2.6 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  2.6 K │
│ 5  │ net.functions.0               │ Sequential │  1.3 K │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │  1.1 K │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    240 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     16 │
│ 9  │ net.functions.1               │ Sequential │  1.3 K │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │  1.1 K │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    240 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     16 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 2.6 K                                                         
Non-trainable params: 0                                                         
Total params: 2.6 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 9999/49999 ━━━━━━━━━━━━━━ 195/195 0:00:02 •      72.03it/s v_num: 0.000   
                                        0:00:00                  rmse/val: 5.826
                                                                 rmse/train:    
                                                                 4.687          
[[36m2025-02-20 02:11:35,101[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         rmse/test         │     2.990882455248392     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24/24 0:00:00 • 0:00:00 353.85it/s 
[[36m2025-02-20 02:11:35,198[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/0/checkpoints/epoch_8954-step_1746225.ckpt[0m
[[36m2025-02-20 02:11:35,327[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-20 02:11:35,501[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/0[0m
[[36m2025-02-20 02:11:35,504[0m][[35mHYDRA[0m] 	#1 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=2[0m
[[36m2025-02-20 02:11:37,657[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-20 02:11:37,658[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-20 02:11:48,550[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-20 02:11:48,559[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-20 02:11:48,559[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-20 02:11:48,563[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-20 02:11:48,564[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-20 02:11:48,565[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-20 02:11:48,566[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-20 02:11:48,566[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-20 02:11:48,568[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-20 02:11:54,365[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-20 02:11:54,758[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  2.6 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  2.6 K │
│ 5  │ net.functions.0               │ Sequential │  1.3 K │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │  1.1 K │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    240 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     16 │
│ 9  │ net.functions.1               │ Sequential │  1.3 K │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │  1.1 K │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    240 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     16 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 2.6 K                                                         
Non-trainable params: 0                                                         
Total params: 2.6 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 9999/49999 ━━━━━━━━━━━━━━ 195/195 0:00:00 •     259.93it/s v_num: 0.000   
                                        0:00:00                  rmse/val: 5.082
                                                                 rmse/train:    
                                                                 4.471          
[[36m2025-02-20 05:16:44,053[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         rmse/test         │     4.011608628533989     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24/24 0:00:00 • 0:00:00 808.24it/s 
[[36m2025-02-20 05:16:44,189[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/1/checkpoints/epoch_8097-step_1579110.ckpt[0m
[[36m2025-02-20 05:16:44,306[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-20 05:16:44,341[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/1[0m
[[36m2025-02-20 05:16:44,354[0m][[35mHYDRA[0m] 	#2 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=3[0m
[[36m2025-02-20 05:16:46,648[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-20 05:16:46,649[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-20 05:16:55,959[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-20 05:16:55,966[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-20 05:16:55,966[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-20 05:16:55,968[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-20 05:16:55,969[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-20 05:16:55,969[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-20 05:16:55,969[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-20 05:16:55,970[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-20 05:16:55,971[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-20 05:17:00,727[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-20 05:17:02,035[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  2.6 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  2.6 K │
│ 5  │ net.functions.0               │ Sequential │  1.3 K │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │  1.1 K │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    240 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     16 │
│ 9  │ net.functions.1               │ Sequential │  1.3 K │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │  1.1 K │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    240 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     16 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 2.6 K                                                         
Non-trainable params: 0                                                         
Total params: 2.6 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 9999/49999 ━━━━━━━━━━━━━━ 195/195 0:00:00 •     216.49it/s v_num: 0.000   
                                        0:00:00                  rmse/val: 5.494
                                                                 rmse/train:    
                                                                 4.213          
[[36m2025-02-20 07:53:29,524[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         rmse/test         │    3.4517685155570828     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24/24 0:00:00 • 0:00:00 813.28it/s 
[[36m2025-02-20 07:53:29,628[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/2/checkpoints/epoch_9505-step_1853670.ckpt[0m
[[36m2025-02-20 07:53:30,287[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-20 07:53:30,444[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/2[0m
[[36m2025-02-20 07:53:30,446[0m][[35mHYDRA[0m] 	#3 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=4[0m
[[36m2025-02-20 07:53:33,534[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-20 07:53:33,534[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-20 07:53:42,960[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-20 07:53:42,966[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-20 07:53:42,966[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-20 07:53:42,968[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-20 07:53:42,969[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-20 07:53:42,969[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-20 07:53:42,970[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-20 07:53:42,970[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-20 07:53:42,971[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-20 07:53:50,158[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-20 07:53:54,581[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  2.6 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  2.6 K │
│ 5  │ net.functions.0               │ Sequential │  1.3 K │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │  1.1 K │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    240 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     16 │
│ 9  │ net.functions.1               │ Sequential │  1.3 K │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │  1.1 K │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    240 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     16 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 2.6 K                                                         
Non-trainable params: 0                                                         
Total params: 2.6 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 9999/49999 ━━━━━━━━━━━━━━ 195/195 0:00:01 •     123.88it/s v_num: 0.000   
                                        0:00:00                  rmse/val: 8.300
                                                                 rmse/train:    
                                                                 4.922          
[[36m2025-02-20 11:01:02,189[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         rmse/test         │    3.9526020377049593     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24/24 0:00:00 • 0:00:00 261.23it/s 
[[36m2025-02-20 11:01:03,115[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/3/checkpoints/epoch_8625-step_1682070.ckpt[0m
[[36m2025-02-20 11:01:03,510[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-20 11:01:03,558[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/3[0m
[[36m2025-02-20 11:01:03,621[0m][[35mHYDRA[0m] 	#4 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=5[0m
[[36m2025-02-20 11:01:05,229[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-20 11:01:05,230[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-20 11:01:15,165[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-20 11:01:15,179[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-20 11:01:15,179[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-20 11:01:15,188[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-20 11:01:15,189[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-20 11:01:15,189[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-20 11:01:15,190[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-20 11:01:15,190[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-20 11:01:15,192[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-20 11:01:15,765[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-20 11:01:15,781[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  2.6 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  2.6 K │
│ 5  │ net.functions.0               │ Sequential │  1.3 K │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │  1.1 K │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    240 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     16 │
│ 9  │ net.functions.1               │ Sequential │  1.3 K │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │  1.1 K │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    240 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     16 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 2.6 K                                                         
Non-trainable params: 0                                                         
Total params: 2.6 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 9999/49999 ━━━━━━━━━━━━━━ 195/195 0:00:00 •     237.90it/s v_num: 0.000   
                                        0:00:00                  rmse/val: 4.871
                                                                 rmse/train:    
                                                                 4.738          
[[36m2025-02-20 14:09:48,592[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         rmse/test         │    3.6091488587343874     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24/24 0:00:00 • 0:00:00 779.36it/s 
[[36m2025-02-20 14:09:48,683[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/4/checkpoints/epoch_7497-step_1462110.ckpt[0m
[[36m2025-02-20 14:09:48,784[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-20 14:09:48,910[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/4[0m
[[36m2025-02-20 14:09:48,913[0m][[35mHYDRA[0m] 	#5 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=6[0m
[[36m2025-02-20 14:09:50,453[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-20 14:09:50,454[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-20 14:10:00,210[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-20 14:10:00,217[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-20 14:10:00,217[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-20 14:10:00,219[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-20 14:10:00,220[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-20 14:10:00,220[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-20 14:10:00,221[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-20 14:10:00,221[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-20 14:10:00,222[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-20 14:10:07,806[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-20 14:10:07,819[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  2.6 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  2.6 K │
│ 5  │ net.functions.0               │ Sequential │  1.3 K │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │  1.1 K │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    240 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     16 │
│ 9  │ net.functions.1               │ Sequential │  1.3 K │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │  1.1 K │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    240 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     16 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 2.6 K                                                         
Non-trainable params: 0                                                         
Total params: 2.6 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 9999/49999 ━━━━━━━━━━━━━━ 195/195 0:00:02 •      87.80it/s v_num: 0.000   
                                        0:00:00                  rmse/val: 6.606
                                                                 rmse/train:    
                                                                 4.570          
[[36m2025-02-20 18:14:31,149[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         rmse/test         │     4.314368226545488     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24/24 0:00:00 • 0:00:00 439.59it/s 
[[36m2025-02-20 18:14:31,528[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/5/checkpoints/epoch_9978-step_1945905.ckpt[0m
[[36m2025-02-20 18:14:31,681[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-20 18:14:32,002[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/5[0m
[[36m2025-02-20 18:14:32,075[0m][[35mHYDRA[0m] 	#6 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=7[0m
[[36m2025-02-20 18:14:36,666[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-20 18:14:36,666[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
(0.06565926932648217, 6.6588702845000975)
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
Train indices: 6253 [5757, 588, 2127, 6217, 1585]
Valid indices: 781 [5719, 2261, 1413, 2713, 5799]
Test indices: 781 [2496, 6248, 7331, 3091, 4123]
0.06565926932648217 6.6588702845000975
[[36m2025-02-20 18:14:47,784[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2025-02-20 18:14:47,797[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2025-02-20 18:14:47,797[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2025-02-20 18:14:47,806[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2025-02-20 18:14:47,806[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2025-02-20 18:14:47,807[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2025-02-20 18:14:47,809[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2025-02-20 18:14:47,809[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2025-02-20 18:14:47,811[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2025-02-20 18:14:56,042[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2025-02-20 18:14:56,168[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  2.6 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  2.6 K │
│ 5  │ net.functions.0               │ Sequential │  1.3 K │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │  1.1 K │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    240 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     16 │
│ 9  │ net.functions.1               │ Sequential │  1.3 K │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │  1.1 K │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    240 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     16 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 2.6 K                                                         
Non-trainable params: 0                                                         
Total params: 2.6 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 4667/49999 ━━━━━━━━━━━━━━ 195/195 0:00:02 •      76.05it/s v_num: 0.000   
                                        0:00:00                  rmse/val:      
                                                                 10.169         
                                                                 rmse/train:    
                                                                 6.021          [[36m2025-02-20 21:17:08,672[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1033, in _run_stage
    self.fit_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 141, in run
    self.on_advance_end(data_fetcher)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 295, in on_advance_end
    self.val_loop.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 142, in run
    return self.on_run_end()
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 265, in on_run_end
    self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py", line 150, in log_eval_end_metrics
    self.log_metrics(metrics)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py", line 118, in log_metrics
    logger.save()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loggers/tensorboard.py", line 213, in save
    super().save()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/fabric/loggers/tensorboard.py", line 293, in save
    self.experiment.flush()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py", line 1256, in flush
    writer.flush()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py", line 152, in flush
    self.event_writer.flush()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/event_file_writer.py", line 125, in flush
    self._async_writer.flush()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/event_file_writer.py", line 194, in flush
    self._check_worker_status()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/event_file_writer.py", line 212, in _check_worker_status
    raise exception
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/event_file_writer.py", line 244, in run
    self._run()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/event_file_writer.py", line 275, in _run
    self._record_writer.write(data)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/summary/writer/record_writer.py", line 40, in write
    self._writer.write(header + header_crc + data + footer_crc)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 773, in write
    self.fs.append(self.filename, file_content, self.binary_mode)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 167, in append
    self._write(filename, file_content, "ab" if binary_mode else "a")
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 171, in _write
    with io.open(filename, mode, encoding=encoding) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error: b'/home/g15farris/bin/bayesaenet/bnn_aenet/logs/TiO_train_de/multiruns/2025-02-19_21-40-15/6/tensorboard/version_0/events.out.tfevents.1740071696.g4noder12.18715.12'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 92, in train
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 64, in _call_and_handle_interrupt
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 208, in _call_callback_hooks
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/callbacks/progress/rich_progress.py", line 641, in on_exception
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/callbacks/progress/rich_progress.py", line 614, in _stop_progress
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/rich/progress.py", line 1163, in stop
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/rich/live.py", line 147, in stop
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/rich/console.py", line 865, in __exit__
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/rich/console.py", line 823, in _exit_buffer
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/rich/console.py", line 2065, in _check_buffer
OSError: [Errno 5] Input/output error
[[36m2025-02-21 07:18:18,039[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-21 07:18:18,052[0m][[35mHYDRA[0m] 	#7 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=8[0m
[[36m2025-02-21 07:18:19,434[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-21 07:18:19,434[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-21 07:18:28,933[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-21 07:18:29,104[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-21 07:18:29,106[0m][[35mHYDRA[0m] 	#8 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=9[0m
[[36m2025-02-21 07:18:29,458[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-21 07:18:29,458[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-21 07:18:29,460[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-21 07:18:29,464[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2025-02-21 07:18:29,466[0m][[35mHYDRA[0m] 	#9 : task_name=TiO_train_de experiment=nn datamodule=TiO trainer.min_epochs=10000 trainer.max_epochs=50000 datamodule.test_split=0.1 datamodule.valid_split=0.1 datamodule.batch_size=32 model.optimizer.lr=0.0013422359776024458 seed=10[0m
[[36m2025-02-21 07:18:29,675[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2025-02-21 07:18:29,675[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
[[36m2025-02-21 07:18:29,677[0m][[34mutils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
FileNotFoundError(2, 'No such file or directory')
full_key: datamodule
[[36m2025-02-21 07:18:29,679[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
