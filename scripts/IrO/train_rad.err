cat: /scratch/3145290.1.iqtc09.q/.gpus: No such file or directory
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-05-17_09-23-04/checkpoints/epoch_237-step_68544.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-05-17_09-23-04/checkpoints/epoch_237-step_68544.ckpt
cat: /scratch/3145487.1.iqtc09.q/.gpus: No such file or directory
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-05-17_09-23-04/checkpoints/epoch_237-step_68544.ckpt
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:361: The dirpath has changed from '/home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-05-17_09-23-04/checkpoints' to '/home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-05-17_15-07-36/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
Restored all states from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-05-17_09-23-04/checkpoints/epoch_237-step_68544.ckpt
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
`Trainer.fit` stopped: `max_epochs=10000` reached.
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-05-17_09-23-04/checkpoints/epoch_237-step_68544.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-05-17_09-23-04/checkpoints/epoch_237-step_68544.ckpt
