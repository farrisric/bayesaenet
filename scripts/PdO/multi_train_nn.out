[[36m2024-06-03 13:47:24,115[0m][[35mHYDRA[0m] Launching 10 jobs locally[0m
[[36m2024-06-03 13:47:24,115[0m][[35mHYDRA[0m] 	#0 : task_name=train_nn experiment=nn datamodule=PdO trainer.max_epochs=10000 tags=[PdO] seed=1[0m
[[36m2024-06-03 13:47:24,257[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 13:47:24,257[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 13:47:54,988[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 13:47:58,134[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 13:47:58,135[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 13:47:58,149[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 13:47:58,150[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 13:47:58,151[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 13:47:58,153[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 13:47:58,153[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 13:47:58,169[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 13:47:58,494[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 13:47:58,687[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 112/9999 ━━━━━━━━━━━━━━━━ 123/123 0:00:00 •        166.32it/s v_num: 0.000
                                        0:00:00                                 
[[36m2024-06-03 13:49:34,137[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/0/checkpoints/epoch_92-step_11439.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/0/checkpoints/epoch_92-step_11439.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │   0.0005990967038087547   │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 329.73it/s 
[[36m2024-06-03 13:49:34,201[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/0/checkpoints/epoch_92-step_11439.ckpt[0m
[[36m2024-06-03 13:49:34,203[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 13:49:34,205[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/0[0m
[[36m2024-06-03 13:49:34,207[0m][[35mHYDRA[0m] 	#1 : task_name=train_nn experiment=nn datamodule=PdO trainer.max_epochs=10000 tags=[PdO] seed=2[0m
[[36m2024-06-03 13:49:34,376[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 13:49:34,376[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 13:50:03,279[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 13:50:03,286[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 13:50:03,286[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 13:50:03,290[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 13:50:03,291[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 13:50:03,291[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 13:50:03,292[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 13:50:03,292[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 13:50:03,294[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 13:50:03,334[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 13:50:03,342[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 112/9999 ━━━━━━━━━━━━━━━━ 123/123 0:00:00 •        136.00it/s v_num: 0.000
                                        0:00:00                                 
[[36m2024-06-03 13:51:37,688[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/1/checkpoints/epoch_92-step_11439.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/1/checkpoints/epoch_92-step_11439.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │   0.0005990967038087547   │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 284.68it/s 
[[36m2024-06-03 13:51:37,762[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/1/checkpoints/epoch_92-step_11439.ckpt[0m
[[36m2024-06-03 13:51:37,765[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 13:51:37,768[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/1[0m
[[36m2024-06-03 13:51:37,770[0m][[35mHYDRA[0m] 	#2 : task_name=train_nn experiment=nn datamodule=PdO trainer.max_epochs=10000 tags=[PdO] seed=3[0m
[[36m2024-06-03 13:51:37,961[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 13:51:37,961[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 13:52:07,512[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 13:52:07,518[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 13:52:07,518[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 13:52:07,521[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 13:52:07,521[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 13:52:07,522[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 13:52:07,523[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 13:52:07,523[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 13:52:07,524[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 13:52:07,564[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 13:52:07,571[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 112/9999 ━━━━━━━━━━━━━━━━ 123/123 0:00:00 •        161.45it/s v_num: 0.000
                                        0:00:00                                 
[[36m2024-06-03 13:53:51,090[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/2/checkpoints/epoch_92-step_11439.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/2/checkpoints/epoch_92-step_11439.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │   0.0005990967038087547   │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 312.33it/s 
[[36m2024-06-03 13:53:51,156[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/2/checkpoints/epoch_92-step_11439.ckpt[0m
[[36m2024-06-03 13:53:51,158[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 13:53:51,160[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/2[0m
[[36m2024-06-03 13:53:51,162[0m][[35mHYDRA[0m] 	#3 : task_name=train_nn experiment=nn datamodule=PdO trainer.max_epochs=10000 tags=[PdO] seed=4[0m
[[36m2024-06-03 13:53:51,338[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 13:53:51,338[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 13:54:20,615[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 13:54:20,621[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 13:54:20,622[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 13:54:20,625[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 13:54:20,626[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 13:54:20,626[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 13:54:20,628[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 13:54:20,628[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 13:54:20,629[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 13:54:20,680[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 13:54:23,430[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 112/9999 ━━━━━━━━━━━━━━━━ 123/123 0:00:00 •        152.29it/s v_num: 0.000
                                        0:00:00                                 
[[36m2024-06-03 13:56:00,407[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/3/checkpoints/epoch_92-step_11439.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/3/checkpoints/epoch_92-step_11439.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │   0.0005990967038087547   │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 195.98it/s 
[[36m2024-06-03 13:56:00,504[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/3/checkpoints/epoch_92-step_11439.ckpt[0m
[[36m2024-06-03 13:56:00,507[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 13:56:00,510[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/3[0m
[[36m2024-06-03 13:56:00,512[0m][[35mHYDRA[0m] 	#4 : task_name=train_nn experiment=nn datamodule=PdO trainer.max_epochs=10000 tags=[PdO] seed=5[0m
[[36m2024-06-03 13:56:00,693[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 13:56:00,693[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 13:56:29,627[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 13:56:29,633[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 13:56:29,633[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 13:56:29,637[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 13:56:29,638[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 13:56:29,638[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 13:56:29,640[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 13:56:29,640[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 13:56:29,642[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 13:56:29,689[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 13:56:29,698[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 112/9999 ━━━━━━━━━━━━━━━━ 123/123 0:00:00 •        149.78it/s v_num: 0.000
                                        0:00:00                                 
[[36m2024-06-03 13:58:01,523[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/4/checkpoints/epoch_92-step_11439.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/4/checkpoints/epoch_92-step_11439.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │   0.0005990967038087547   │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 293.90it/s 
[[36m2024-06-03 13:58:01,591[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/4/checkpoints/epoch_92-step_11439.ckpt[0m
[[36m2024-06-03 13:58:01,594[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 13:58:01,596[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/4[0m
[[36m2024-06-03 13:58:01,599[0m][[35mHYDRA[0m] 	#5 : task_name=train_nn experiment=nn datamodule=PdO trainer.max_epochs=10000 tags=[PdO] seed=6[0m
[[36m2024-06-03 13:58:01,767[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 13:58:01,767[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 13:58:31,455[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 13:58:31,461[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 13:58:31,461[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 13:58:31,465[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 13:58:31,465[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 13:58:31,466[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 13:58:31,467[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 13:58:31,467[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 13:58:31,468[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 13:58:31,523[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 13:58:31,535[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 112/9999 ━━━━━━━━━━━━━━━━ 123/123 0:00:00 •        138.40it/s v_num: 0.000
                                        0:00:00                                 
[[36m2024-06-03 14:00:11,665[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/5/checkpoints/epoch_92-step_11439.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/5/checkpoints/epoch_92-step_11439.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │   0.0005990967038087547   │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 290.66it/s 
[[36m2024-06-03 14:00:11,734[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/5/checkpoints/epoch_92-step_11439.ckpt[0m
[[36m2024-06-03 14:00:11,736[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 14:00:11,738[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/5[0m
[[36m2024-06-03 14:00:11,740[0m][[35mHYDRA[0m] 	#6 : task_name=train_nn experiment=nn datamodule=PdO trainer.max_epochs=10000 tags=[PdO] seed=7[0m
[[36m2024-06-03 14:00:11,914[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 14:00:11,915[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 14:00:41,350[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 14:00:41,357[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 14:00:41,357[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 14:00:41,361[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 14:00:41,362[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 14:00:41,362[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 14:00:41,363[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 14:00:41,364[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 14:00:41,365[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 14:00:41,402[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 14:00:41,410[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 112/9999 ━━━━━━━━━━━━━━━━ 123/123 0:00:00 •        194.61it/s v_num: 0.000
                                        0:00:00                                 
[[36m2024-06-03 14:02:18,028[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/6/checkpoints/epoch_92-step_11439.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/6/checkpoints/epoch_92-step_11439.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │   0.0005990967038087547   │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 337.90it/s 
[[36m2024-06-03 14:02:18,088[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/6/checkpoints/epoch_92-step_11439.ckpt[0m
[[36m2024-06-03 14:02:18,090[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 14:02:18,092[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/6[0m
[[36m2024-06-03 14:02:18,093[0m][[35mHYDRA[0m] 	#7 : task_name=train_nn experiment=nn datamodule=PdO trainer.max_epochs=10000 tags=[PdO] seed=8[0m
[[36m2024-06-03 14:02:18,247[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 14:02:18,247[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 14:02:47,262[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 14:02:47,268[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 14:02:47,268[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 14:02:47,272[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 14:02:47,273[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 14:02:47,273[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 14:02:47,275[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 14:02:47,275[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 14:02:47,276[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 14:02:47,314[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 14:02:47,320[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 112/9999 ━━━━━━━━━━━━━━━━ 123/123 0:00:00 •        132.78it/s v_num: 0.000
                                        0:00:00                                 
[[36m2024-06-03 14:04:20,929[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/7/checkpoints/epoch_92-step_11439.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/7/checkpoints/epoch_92-step_11439.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │   0.0005990967038087547   │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 289.79it/s 
[[36m2024-06-03 14:04:20,998[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/7/checkpoints/epoch_92-step_11439.ckpt[0m
[[36m2024-06-03 14:04:21,001[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 14:04:21,002[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/7[0m
[[36m2024-06-03 14:04:21,004[0m][[35mHYDRA[0m] 	#8 : task_name=train_nn experiment=nn datamodule=PdO trainer.max_epochs=10000 tags=[PdO] seed=9[0m
[[36m2024-06-03 14:04:21,188[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 14:04:21,188[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 14:04:50,182[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 14:04:50,189[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 14:04:50,190[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 14:04:50,193[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 14:04:50,194[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 14:04:50,195[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 14:04:50,196[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 14:04:50,196[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 14:04:50,197[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 14:04:50,238[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 14:04:50,246[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 112/9999 ━━━━━━━━━━━━━━━━ 123/123 0:00:00 •        160.32it/s v_num: 0.000
                                        0:00:00                                 
[[36m2024-06-03 14:06:30,452[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/8/checkpoints/epoch_92-step_11439.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/8/checkpoints/epoch_92-step_11439.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │   0.0005990967038087547   │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 265.71it/s 
[[36m2024-06-03 14:06:30,526[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/8/checkpoints/epoch_92-step_11439.ckpt[0m
[[36m2024-06-03 14:06:30,528[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 14:06:30,530[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/8[0m
[[36m2024-06-03 14:06:30,532[0m][[35mHYDRA[0m] 	#9 : task_name=train_nn experiment=nn datamodule=PdO trainer.max_epochs=10000 tags=[PdO] seed=10[0m
[[36m2024-06-03 14:06:30,696[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 14:06:30,697[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 14:07:00,690[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 14:07:00,696[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 14:07:00,696[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 14:07:00,701[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 14:07:00,701[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 14:07:00,702[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 14:07:00,703[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 14:07:00,704[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 14:07:00,705[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 14:07:00,748[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 14:07:00,756[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
Epoch 112/9999 ━━━━━━━━━━━━━━━━ 123/123 0:00:00 •        144.13it/s v_num: 0.000
                                        0:00:00                                 
[[36m2024-06-03 14:08:35,663[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/9/checkpoints/epoch_92-step_11439.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/9/checkpoints/epoch_92-step_11439.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │   0.0005990967038087547   │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 198.16it/s 
[[36m2024-06-03 14:08:35,759[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/9/checkpoints/epoch_92-step_11439.ckpt[0m
[[36m2024-06-03 14:08:35,762[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 14:08:35,764[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_13-47-22/9[0m
[[36m2024-06-03 15:00:29,967[0m][[35mHYDRA[0m] Launching 10 jobs locally[0m
[[36m2024-06-03 15:00:29,968[0m][[35mHYDRA[0m] 	#0 : task_name=train_nn experiment=nn datamodule=PdO trainer.min_epochs=10000 tags=[PdO] seed=1[0m
[[36m2024-06-03 15:00:30,247[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 15:00:30,247[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 15:01:08,525[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 15:01:08,574[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 15:01:08,574[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 15:01:08,579[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 15:01:08,580[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 15:01:08,581[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 15:01:08,582[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 15:01:08,583[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 15:01:08,696[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 15:01:09,058[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 15:01:13,314[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4/4  ━━━━━━━━━━━━━━━━━━━ 123/123 0:00:01 • 0:00:00 102.80it/s v_num: 0.000
[[36m2024-06-03 15:01:36,501[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/0/checkpoints/epoch_4-step_615.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/0/checkpoints/epoch_4-step_615.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │    0.08347218483686447    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 120.50it/s 
[[36m2024-06-03 15:01:36,672[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/0/checkpoints/epoch_4-step_615.ckpt[0m
[[36m2024-06-03 15:01:36,674[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 15:01:36,675[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/0[0m
[[36m2024-06-03 15:01:36,676[0m][[35mHYDRA[0m] 	#1 : task_name=train_nn experiment=nn datamodule=PdO trainer.min_epochs=10000 tags=[PdO] seed=2[0m
[[36m2024-06-03 15:01:36,944[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 15:01:36,945[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 15:02:12,399[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 15:02:12,406[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 15:02:12,406[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 15:02:12,410[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 15:02:12,411[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 15:02:12,412[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 15:02:12,414[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 15:02:12,414[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 15:02:12,416[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 15:02:12,474[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 15:02:12,507[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4/4  ━━━━━━━━━━━━━━━━━━━ 123/123 0:00:01 • 0:00:00 112.68it/s v_num: 0.000
[[36m2024-06-03 15:02:19,869[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/1/checkpoints/epoch_4-step_615.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/1/checkpoints/epoch_4-step_615.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │    0.08347218483686447    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 264.77it/s 
[[36m2024-06-03 15:02:19,972[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/1/checkpoints/epoch_4-step_615.ckpt[0m
[[36m2024-06-03 15:02:19,974[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 15:02:19,975[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/1[0m
[[36m2024-06-03 15:02:19,977[0m][[35mHYDRA[0m] 	#2 : task_name=train_nn experiment=nn datamodule=PdO trainer.min_epochs=10000 tags=[PdO] seed=3[0m
[[36m2024-06-03 15:02:20,276[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 15:02:20,277[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 15:02:52,095[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 15:02:52,102[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 15:02:52,102[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 15:02:52,106[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 15:02:52,107[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 15:02:52,107[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 15:02:52,108[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 15:02:52,109[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 15:02:52,110[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 15:02:52,126[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 15:02:52,147[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4/4  ━━━━━━━━━━━━━━━━━━━━ 123/123 0:00:01 • 0:00:00 94.56it/s v_num: 0.000
[[36m2024-06-03 15:02:59,029[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/2/checkpoints/epoch_4-step_615.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/2/checkpoints/epoch_4-step_615.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │    0.08347218483686447    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 273.67it/s 
[[36m2024-06-03 15:02:59,131[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/2/checkpoints/epoch_4-step_615.ckpt[0m
[[36m2024-06-03 15:02:59,134[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 15:02:59,135[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/2[0m
[[36m2024-06-03 15:02:59,136[0m][[35mHYDRA[0m] 	#3 : task_name=train_nn experiment=nn datamodule=PdO trainer.min_epochs=10000 tags=[PdO] seed=4[0m
[[36m2024-06-03 15:02:59,473[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 15:02:59,473[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 15:03:30,069[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 15:03:30,076[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 15:03:30,076[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 15:03:30,091[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 15:03:30,092[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 15:03:30,092[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 15:03:30,095[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 15:03:30,095[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 15:03:30,096[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 15:03:30,139[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 15:03:30,166[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4/4  ━━━━━━━━━━━━━━━━━━━ 123/123 0:00:01 • 0:00:00 121.66it/s v_num: 0.000
[[36m2024-06-03 15:03:35,149[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/3/checkpoints/epoch_4-step_615.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/3/checkpoints/epoch_4-step_615.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │    0.08347218483686447    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 170.46it/s 
[[36m2024-06-03 15:03:35,292[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/3/checkpoints/epoch_4-step_615.ckpt[0m
[[36m2024-06-03 15:03:35,294[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 15:03:35,295[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/3[0m
[[36m2024-06-03 15:03:35,297[0m][[35mHYDRA[0m] 	#4 : task_name=train_nn experiment=nn datamodule=PdO trainer.min_epochs=10000 tags=[PdO] seed=5[0m
[[36m2024-06-03 15:03:35,602[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 15:03:35,603[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 15:04:13,067[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 15:04:13,076[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 15:04:13,077[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 15:04:13,093[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 15:04:13,095[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 15:04:13,095[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 15:04:13,099[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 15:04:13,099[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 15:04:13,101[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 15:04:13,147[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 15:04:13,174[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4/4  ━━━━━━━━━━━━━━━━━━━━ 123/123 0:00:01 • 0:00:00 64.69it/s v_num: 0.000
[[36m2024-06-03 15:04:21,547[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/4/checkpoints/epoch_4-step_615.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/4/checkpoints/epoch_4-step_615.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │    0.08347218483686447    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 123.82it/s 
[[36m2024-06-03 15:04:21,733[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/4/checkpoints/epoch_4-step_615.ckpt[0m
[[36m2024-06-03 15:04:21,736[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 15:04:21,737[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/4[0m
[[36m2024-06-03 15:04:21,748[0m][[35mHYDRA[0m] 	#5 : task_name=train_nn experiment=nn datamodule=PdO trainer.min_epochs=10000 tags=[PdO] seed=6[0m
[[36m2024-06-03 15:04:22,094[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 15:04:22,094[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 15:05:04,075[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 15:05:04,097[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 15:05:04,097[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 15:05:04,112[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 15:05:04,114[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 15:05:04,115[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 15:05:04,117[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 15:05:04,117[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 15:05:04,119[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 15:05:04,164[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 15:05:04,207[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4/4  ━━━━━━━━━━━━━━━━━━━━ 123/123 0:00:01 • 0:00:00 98.63it/s v_num: 0.000
[[36m2024-06-03 15:05:11,238[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/5/checkpoints/epoch_4-step_615.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/5/checkpoints/epoch_4-step_615.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │    0.08347218483686447    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 124.08it/s 
[[36m2024-06-03 15:05:11,412[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/5/checkpoints/epoch_4-step_615.ckpt[0m
[[36m2024-06-03 15:05:11,416[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 15:05:11,417[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/5[0m
[[36m2024-06-03 15:05:11,419[0m][[35mHYDRA[0m] 	#6 : task_name=train_nn experiment=nn datamodule=PdO trainer.min_epochs=10000 tags=[PdO] seed=7[0m
[[36m2024-06-03 15:05:11,723[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 15:05:11,724[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 15:05:50,695[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 15:05:50,702[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 15:05:50,703[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 15:05:50,707[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 15:05:50,709[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 15:05:50,710[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 15:05:50,711[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 15:05:50,711[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 15:05:50,713[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 15:05:50,764[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 15:05:50,787[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4/4  ━━━━━━━━━━━━━━━━━━━ 123/123 0:00:01 • 0:00:00 114.20it/s v_num: 0.000
[[36m2024-06-03 15:05:56,680[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/6/checkpoints/epoch_4-step_615.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/6/checkpoints/epoch_4-step_615.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │    0.08347218483686447    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 229.38it/s 
[[36m2024-06-03 15:05:56,791[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/6/checkpoints/epoch_4-step_615.ckpt[0m
[[36m2024-06-03 15:05:56,793[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 15:05:56,794[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/6[0m
[[36m2024-06-03 15:05:56,796[0m][[35mHYDRA[0m] 	#7 : task_name=train_nn experiment=nn datamodule=PdO trainer.min_epochs=10000 tags=[PdO] seed=8[0m
[[36m2024-06-03 15:05:57,094[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 15:05:57,094[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 15:06:30,271[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 15:06:30,284[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 15:06:30,285[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 15:06:30,291[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 15:06:30,295[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 15:06:30,296[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 15:06:30,298[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 15:06:30,299[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 15:06:30,301[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 15:06:30,345[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 15:06:30,529[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4/4  ━━━━━━━━━━━━━━━━━━━━ 123/123 0:00:02 • 0:00:00 61.12it/s v_num: 0.000
[[36m2024-06-03 15:06:40,147[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/7/checkpoints/epoch_4-step_615.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/7/checkpoints/epoch_4-step_615.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │    0.08347218483686447    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 91.24it/s 
[[36m2024-06-03 15:06:40,336[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/7/checkpoints/epoch_4-step_615.ckpt[0m
[[36m2024-06-03 15:06:40,339[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 15:06:40,341[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/7[0m
[[36m2024-06-03 15:06:40,344[0m][[35mHYDRA[0m] 	#8 : task_name=train_nn experiment=nn datamodule=PdO trainer.min_epochs=10000 tags=[PdO] seed=9[0m
[[36m2024-06-03 15:06:40,664[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 15:06:40,669[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 15:07:19,549[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 15:07:19,562[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 15:07:19,562[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 15:07:19,568[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 15:07:19,570[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 15:07:19,571[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 15:07:19,573[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 15:07:19,573[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 15:07:19,576[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 15:07:19,608[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 15:07:19,622[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4/4  ━━━━━━━━━━━━━━━━━━━ 123/123 0:00:00 • 0:00:00 129.11it/s v_num: 0.000
[[36m2024-06-03 15:07:28,111[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/8/checkpoints/epoch_4-step_615.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/8/checkpoints/epoch_4-step_615.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │    0.08347218483686447    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 319.93it/s 
[[36m2024-06-03 15:07:28,198[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/8/checkpoints/epoch_4-step_615.ckpt[0m
[[36m2024-06-03 15:07:28,200[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 15:07:28,200[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/8[0m
[[36m2024-06-03 15:07:28,202[0m][[35mHYDRA[0m] 	#9 : task_name=train_nn experiment=nn datamodule=PdO trainer.min_epochs=10000 tags=[PdO] seed=10[0m
[[36m2024-06-03 15:07:28,562[0m][[34mutils.utils[0m][[33mWARNING[0m] - Extras config not found! <cfg.extras=null>[0m
[[36m2024-06-03 15:07:28,562[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <bnn_aenet.datamodule.aenet_datamodule.AenetDataModule>[0m
0.1
0.8 0.1 0.1
[12833, 10600, 4868, 5601, 6351]
0.8 0.1 0.1
[297, 7446, 3021, 11251, 9173]
[[36m2024-06-03 15:07:58,472[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <bnn_aenet.models.bnn.NN>[0m
[[36m2024-06-03 15:07:58,490[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2024-06-03 15:07:58,491[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-06-03 15:07:58,504[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-06-03 15:07:58,506[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-06-03 15:07:58,506[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-06-03 15:07:58,508[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2024-06-03 15:07:58,509[0m][[34mutils.utils[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2024-06-03 15:07:58,510[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-06-03 15:07:58,559[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2024-06-03 15:07:58,588[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type       ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ net                           │ NetAtom    │  1.9 K │
│ 1  │ net.linear                    │ Identity   │      0 │
│ 2  │ net.tanh                      │ Tanh       │      0 │
│ 3  │ net.sigmoid                   │ Sigmoid    │      0 │
│ 4  │ net.functions                 │ ModuleList │  1.9 K │
│ 5  │ net.functions.0               │ Sequential │    966 │
│ 6  │ net.functions.0.Linear_Sp1_F1 │ Linear     │    795 │
│ 7  │ net.functions.0.Linear_Sp1_F2 │ Linear     │    160 │
│ 8  │ net.functions.0.Linear_Sp1_F3 │ Linear     │     11 │
│ 9  │ net.functions.1               │ Sequential │    966 │
│ 10 │ net.functions.1.Linear_Sp2_F1 │ Linear     │    795 │
│ 11 │ net.functions.1.Linear_Sp2_F2 │ Linear     │    160 │
│ 12 │ net.functions.1.Linear_Sp2_F3 │ Linear     │     11 │
└────┴───────────────────────────────┴────────────┴────────┘
Trainable params: 1.9 K                                                         
Non-trainable params: 0                                                         
Total params: 1.9 K                                                             
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=5` reached.
Epoch 4/4  ━━━━━━━━━━━━━━━━━━━━ 123/123 0:00:02 • 0:00:00 56.38it/s v_num: 0.000
[[36m2024-06-03 15:08:05,763[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/9/checkpoints/epoch_4-step_615.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/9/checkpoints/epoch_4-step_615.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         mse/test          │    0.08347218483686447    │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15/15 0:00:00 • 0:00:00 149.43it/s 
[[36m2024-06-03 15:08:05,908[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/9/checkpoints/epoch_4-step_615.ckpt[0m
[[36m2024-06-03 15:08:05,920[0m][[34mutils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2024-06-03 15:08:05,922[0m][[34mutils.utils[0m][[32mINFO[0m] - Output dir: /home/g15farris/bin/forks/bayesaenet/src/logs/train_nn/multiruns/2024-06-03_15-00-27/9[0m
