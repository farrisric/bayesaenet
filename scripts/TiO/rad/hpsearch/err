/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-02-24 16:30:25,904] A new study created in RDB with name: rad_20perc
[I 2025-02-24 16:30:26,336] A new study created in RDB with name: rad_100perc
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:203: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  agg_scale = precision.reciprocal().mean(dim).add(loc.var(dim)).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:203: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  agg_scale = precision.reciprocal().mean(dim).add(loc.var(dim)).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:30:46,426] Trial 0 finished with value: 638.2109985351562 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 0.22702608040701588, 'q_scale': 0.13179630432958683, 'batch_size': 256}. Best is trial 0 with value: 638.2109985351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:04,129] Trial 1 finished with value: 628.9982299804688 and parameters: {'pretrain_epochs': 0, 'lr': 5.195586024325663e-05, 'mc_samples_train': 2, 'prior_scale': 0.1374377640063412, 'q_scale': 0.0030269344709026166, 'batch_size': 512}. Best is trial 1 with value: 628.9982299804688.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:07,126] Trial 0 finished with value: 948.5304565429688 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 0.22702608040701588, 'q_scale': 0.13179630432958683, 'batch_size': 256}. Best is trial 0 with value: 948.5304565429688.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:23,449] Trial 2 finished with value: 581.3751831054688 and parameters: {'pretrain_epochs': 0, 'lr': 5.367498945698204e-05, 'mc_samples_train': 1, 'prior_scale': 0.012778268753495938, 'q_scale': 0.540207249341968, 'batch_size': 128}. Best is trial 2 with value: 581.3751831054688.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:48,704] Trial 1 finished with value: 3049.81396484375 and parameters: {'pretrain_epochs': 0, 'lr': 5.195586024325663e-05, 'mc_samples_train': 2, 'prior_scale': 0.1374377640063412, 'q_scale': 0.0030269344709026166, 'batch_size': 512}. Best is trial 0 with value: 948.5304565429688.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:49,426] Trial 3 finished with value: 576.7377319335938 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005473369642905611, 'mc_samples_train': 2, 'prior_scale': 0.002699606552195955, 'q_scale': 0.06562115716452717, 'batch_size': 128}. Best is trial 3 with value: 576.7377319335938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:32:08,016] Trial 4 finished with value: 614.8442993164062 and parameters: {'pretrain_epochs': 0, 'lr': 1.3170990774640101e-05, 'mc_samples_train': 1, 'prior_scale': 0.10583829396911469, 'q_scale': 0.023905570899706415, 'batch_size': 128}. Best is trial 3 with value: 576.7377319335938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:32:26,562] Trial 5 finished with value: 613.4588012695312 and parameters: {'pretrain_epochs': 0, 'lr': 1.674127903856e-05, 'mc_samples_train': 1, 'prior_scale': 0.001047833275436284, 'q_scale': 0.02949625201538112, 'batch_size': 128}. Best is trial 3 with value: 576.7377319335938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:32:39,396] Trial 2 finished with value: 498.3192443847656 and parameters: {'pretrain_epochs': 0, 'lr': 5.367498945698204e-05, 'mc_samples_train': 1, 'prior_scale': 0.012778268753495938, 'q_scale': 0.540207249341968, 'batch_size': 128}. Best is trial 2 with value: 498.3192443847656.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:32:45,402] Trial 6 finished with value: 607.231689453125 and parameters: {'pretrain_epochs': 0, 'lr': 3.71964865643354e-05, 'mc_samples_train': 1, 'prior_scale': 0.0038638969563953266, 'q_scale': 0.0033827492632411567, 'batch_size': 128}. Best is trial 3 with value: 576.7377319335938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:33:23,432] Trial 7 finished with value: 147.95037841796875 and parameters: {'pretrain_epochs': 0, 'lr': 0.00029994809793642315, 'mc_samples_train': 1, 'prior_scale': 0.0020959477484878194, 'q_scale': 0.0008257988273685458, 'batch_size': 32}. Best is trial 7 with value: 147.95037841796875.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:33:37,674] Trial 8 pruned. Trial was pruned at epoch 6.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:33:53,882] Trial 9 pruned. Trial was pruned at epoch 14.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:33:56,476] Trial 3 finished with value: 273.52301025390625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005473369642905611, 'mc_samples_train': 2, 'prior_scale': 0.002699606552195955, 'q_scale': 0.06562115716452717, 'batch_size': 128}. Best is trial 3 with value: 273.52301025390625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:34:47,040] Trial 4 finished with value: 505.29510498046875 and parameters: {'pretrain_epochs': 0, 'lr': 1.3170990774640101e-05, 'mc_samples_train': 1, 'prior_scale': 0.10583829396911469, 'q_scale': 0.023905570899706415, 'batch_size': 128}. Best is trial 3 with value: 273.52301025390625.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:34:50,680] Trial 10 finished with value: 97.16728973388672 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002686321798731397, 'mc_samples_train': 2, 'prior_scale': 0.01116798829495977, 'q_scale': 0.00013611901663306908, 'batch_size': 32}. Best is trial 10 with value: 97.16728973388672.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:35:37,023] Trial 5 finished with value: 502.9070129394531 and parameters: {'pretrain_epochs': 0, 'lr': 1.674127903856e-05, 'mc_samples_train': 1, 'prior_scale': 0.001047833275436284, 'q_scale': 0.02949625201538112, 'batch_size': 128}. Best is trial 3 with value: 273.52301025390625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:35:46,965] Trial 11 finished with value: 74.09391784667969 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002668116545003139, 'mc_samples_train': 2, 'prior_scale': 0.015791568646716513, 'q_scale': 0.00013400032806480455, 'batch_size': 32}. Best is trial 11 with value: 74.09391784667969.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:36:26,900] Trial 6 finished with value: 488.8832702636719 and parameters: {'pretrain_epochs': 0, 'lr': 3.71964865643354e-05, 'mc_samples_train': 1, 'prior_scale': 0.0038638969563953266, 'q_scale': 0.0033827492632411567, 'batch_size': 128}. Best is trial 3 with value: 273.52301025390625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:36:43,636] Trial 12 finished with value: 120.26973724365234 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001741204381065012, 'mc_samples_train': 2, 'prior_scale': 0.01578701912640358, 'q_scale': 0.00010569299297450266, 'batch_size': 32}. Best is trial 11 with value: 74.09391784667969.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:37:17,800] Trial 13 finished with value: 71.5845947265625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009998183169689775, 'mc_samples_train': 2, 'prior_scale': 0.01064012896031672, 'q_scale': 0.00010211784008112604, 'batch_size': 64}. Best is trial 13 with value: 71.5845947265625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:37:52,188] Trial 14 finished with value: 10.052152633666992 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009813379195690634, 'mc_samples_train': 2, 'prior_scale': 0.03462087235603707, 'q_scale': 0.0004237992367412906, 'batch_size': 64}. Best is trial 14 with value: 10.052152633666992.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:38:26,311] Trial 15 finished with value: 0.04856126010417938 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009889182716204107, 'mc_samples_train': 2, 'prior_scale': 0.04295889441861082, 'q_scale': 0.0005750345772627247, 'batch_size': 64}. Best is trial 15 with value: 0.04856126010417938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:38:54,803] Trial 7 finished with value: 123.26927947998047 and parameters: {'pretrain_epochs': 0, 'lr': 0.00029994809793642315, 'mc_samples_train': 1, 'prior_scale': 0.0020959477484878194, 'q_scale': 0.0008257988273685458, 'batch_size': 32}. Best is trial 7 with value: 123.26927947998047.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:39:00,482] Trial 16 finished with value: 15.346548080444336 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009485885170359142, 'mc_samples_train': 2, 'prior_scale': 0.050760342213256066, 'q_scale': 0.0005478446090810961, 'batch_size': 64}. Best is trial 15 with value: 0.04856126010417938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:39:35,319] Trial 17 finished with value: 116.4356689453125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005291959293112635, 'mc_samples_train': 2, 'prior_scale': 0.03204685616520421, 'q_scale': 0.0006411550999815748, 'batch_size': 64}. Best is trial 15 with value: 0.04856126010417938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:40:10,835] Trial 18 finished with value: 267.34136962890625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00014377726650422048, 'mc_samples_train': 2, 'prior_scale': 0.30139191894710987, 'q_scale': 0.00162577924453938, 'batch_size': 64}. Best is trial 15 with value: 0.04856126010417938.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:40:12,647] Trial 8 finished with value: 426.2650146484375 and parameters: {'pretrain_epochs': 0, 'lr': 7.478015213333154e-05, 'mc_samples_train': 2, 'prior_scale': 0.07537199486913888, 'q_scale': 0.0666791815288874, 'batch_size': 128}. Best is trial 7 with value: 123.26927947998047.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:40:46,305] Trial 19 finished with value: 117.00869750976562 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005192545799175073, 'mc_samples_train': 2, 'prior_scale': 0.033569935880950516, 'q_scale': 0.0002942702531995975, 'batch_size': 64}. Best is trial 15 with value: 0.04856126010417938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:41:03,509] Trial 20 finished with value: 557.8419189453125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006120312933045696, 'mc_samples_train': 2, 'prior_scale': 0.5377315111987019, 'q_scale': 0.005383502861555352, 'batch_size': 512}. Best is trial 15 with value: 0.04856126010417938.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:41:04,890] Trial 9 finished with value: 413.6343078613281 and parameters: {'pretrain_epochs': 0, 'lr': 2.01904291186572e-05, 'mc_samples_train': 1, 'prior_scale': 0.714967484470097, 'q_scale': 0.00834519932080496, 'batch_size': 128}. Best is trial 7 with value: 123.26927947998047.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:41:40,011] Trial 21 finished with value: -16.925064086914062 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009674822215525922, 'mc_samples_train': 2, 'prior_scale': 0.05341519839832709, 'q_scale': 0.0004765721941975227, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:42:15,774] Trial 22 finished with value: 21.727869033813477 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007743097196559441, 'mc_samples_train': 2, 'prior_scale': 0.0598932944368543, 'q_scale': 0.00033550995264756994, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:42:51,876] Trial 23 finished with value: 207.6566162109375 and parameters: {'pretrain_epochs': 0, 'lr': 0.00037770394393093684, 'mc_samples_train': 2, 'prior_scale': 0.022872850683908916, 'q_scale': 0.0012584293630723776, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:43:17,510] Trial 24 finished with value: 593.62890625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004114619527568558, 'mc_samples_train': 2, 'prior_scale': 0.006209310061318424, 'q_scale': 0.00025404502440887157, 'batch_size': 256}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:44:01,949] Trial 25 finished with value: 274.30230712890625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00017466657241806927, 'mc_samples_train': 2, 'prior_scale': 0.16500300882824115, 'q_scale': 0.001767415258466624, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:44:48,133] Trial 26 finished with value: 13.655993461608887 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007554624239554741, 'mc_samples_train': 2, 'prior_scale': 0.044058940218130394, 'q_scale': 0.00042333319101991556, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:45:16,293] Trial 10 finished with value: -27.341768264770508 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002686321798731397, 'mc_samples_train': 2, 'prior_scale': 0.01116798829495977, 'q_scale': 0.00013611901663306908, 'batch_size': 32}. Best is trial 10 with value: -27.341768264770508.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:45:25,256] Trial 27 finished with value: 15.725152969360352 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009901856396176453, 'mc_samples_train': 2, 'prior_scale': 0.0868843208773279, 'q_scale': 0.001004025177131389, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:46:03,000] Trial 28 finished with value: 207.4356689453125 and parameters: {'pretrain_epochs': 0, 'lr': 0.00037849684934288615, 'mc_samples_train': 2, 'prior_scale': 0.022881459218529198, 'q_scale': 0.0024217422139077454, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:46:29,253] Trial 29 finished with value: 545.9257202148438 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006881081423441657, 'mc_samples_train': 2, 'prior_scale': 0.19966835833553076, 'q_scale': 0.00022167813755188083, 'batch_size': 256}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:46:43,720] Trial 30 pruned. Trial was pruned at epoch 12.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:47:19,152] Trial 31 finished with value: 6.887058734893799 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007388319317393496, 'mc_samples_train': 2, 'prior_scale': 0.04679461401891192, 'q_scale': 0.0004715575186800082, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:47:54,552] Trial 32 finished with value: 14.726723670959473 and parameters: {'pretrain_epochs': 0, 'lr': 0.000714904111077346, 'mc_samples_train': 2, 'prior_scale': 0.11262228874296885, 'q_scale': 0.0006353991957465905, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:48:30,288] Trial 33 finished with value: 42.140480041503906 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008089641398445686, 'mc_samples_train': 2, 'prior_scale': 0.024278988745211245, 'q_scale': 0.004997174330747524, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:48:51,453] Trial 34 finished with value: 589.7432250976562 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004444675237055703, 'mc_samples_train': 2, 'prior_scale': 0.04989939140645882, 'q_scale': 0.00022410101719808355, 'batch_size': 256}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:49:26,998] Trial 35 finished with value: 269.08184814453125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006028535307698344, 'mc_samples_train': 2, 'prior_scale': 0.0073432978555727875, 'q_scale': 0.20769098014655615, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:49:28,574] Trial 11 finished with value: -5.201004505157471 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002668116545003139, 'mc_samples_train': 2, 'prior_scale': 0.015791568646716513, 'q_scale': 0.00013400032806480455, 'batch_size': 32}. Best is trial 10 with value: -27.341768264770508.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:49:42,004] Trial 36 finished with value: 590.1962890625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009968420423254554, 'mc_samples_train': 1, 'prior_scale': 0.07222803837728979, 'q_scale': 0.0008698505105169447, 'batch_size': 512}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:50:17,204] Trial 37 finished with value: 235.01515197753906 and parameters: {'pretrain_epochs': 0, 'lr': 0.00022177411008405108, 'mc_samples_train': 2, 'prior_scale': 0.134568054372779, 'q_scale': 0.021640621136322986, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:50:42,592] Trial 38 finished with value: 306.0402526855469 and parameters: {'pretrain_epochs': 0, 'lr': 3.6429465259517205e-05, 'mc_samples_train': 1, 'prior_scale': 0.03456722549972178, 'q_scale': 0.002641990092553853, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:51:18,200] Trial 39 finished with value: 310.9234313964844 and parameters: {'pretrain_epochs': 0, 'lr': 1.0774254950974043e-05, 'mc_samples_train': 2, 'prior_scale': 0.09725925376712785, 'q_scale': 0.00045733157387271644, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:51:34,417] Trial 40 finished with value: 580.8063354492188 and parameters: {'pretrain_epochs': 0, 'lr': 0.00047708388283163424, 'mc_samples_train': 1, 'prior_scale': 0.020146678789444695, 'q_scale': 0.0013367701794011566, 'batch_size': 256}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:52:09,660] Trial 41 finished with value: 3.6526060104370117 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007987175984134034, 'mc_samples_train': 2, 'prior_scale': 0.044097739809971025, 'q_scale': 0.0004150498958893591, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:52:45,974] Trial 42 finished with value: 56.963714599609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006273737040919831, 'mc_samples_train': 2, 'prior_scale': 0.037412126650796025, 'q_scale': 0.00018565455630260784, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:53:21,564] Trial 43 finished with value: 211.08377075195312 and parameters: {'pretrain_epochs': 0, 'lr': 0.00032042960346764636, 'mc_samples_train': 2, 'prior_scale': 0.06375640275252532, 'q_scale': 0.0003838227339018109, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:53:38,711] Trial 12 finished with value: -17.93995475769043 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001741204381065012, 'mc_samples_train': 2, 'prior_scale': 0.01578701912640358, 'q_scale': 0.00010569299297450266, 'batch_size': 32}. Best is trial 10 with value: -27.341768264770508.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:53:56,991] Trial 44 finished with value: 9.126142501831055 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008157612900547135, 'mc_samples_train': 2, 'prior_scale': 0.04607345976514929, 'q_scale': 0.0006786212150953987, 'batch_size': 64}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:54:21,226] Trial 45 finished with value: 604.305419921875 and parameters: {'pretrain_epochs': 0, 'lr': 7.429668997342275e-05, 'mc_samples_train': 2, 'prior_scale': 0.05164722455393182, 'q_scale': 0.0008962528764714299, 'batch_size': 128}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:54:59,986] Trial 46 finished with value: -11.160043716430664 and parameters: {'pretrain_epochs': 0, 'lr': 0.000784352231106911, 'mc_samples_train': 1, 'prior_scale': 0.08310032214742986, 'q_scale': 0.0001750410300494953, 'batch_size': 32}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:55:38,907] Trial 47 finished with value: -4.817296028137207 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005658165876521511, 'mc_samples_train': 1, 'prior_scale': 0.11720695712245188, 'q_scale': 0.0001619891078163313, 'batch_size': 32}. Best is trial 21 with value: -16.925064086914062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:55:54,473] Trial 13 finished with value: 0.7419288158416748 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001607573726481525, 'mc_samples_train': 2, 'prior_scale': 0.01063767565528458, 'q_scale': 0.00010194432109815798, 'batch_size': 64}. Best is trial 10 with value: -27.341768264770508.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:56:17,841] Trial 48 finished with value: -17.647350311279297 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005497093704246852, 'mc_samples_train': 1, 'prior_scale': 0.2693884873909341, 'q_scale': 0.00014173134919159532, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:56:57,133] Trial 49 finished with value: 5.545897960662842 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003168551052422103, 'mc_samples_train': 1, 'prior_scale': 0.4224824590044395, 'q_scale': 0.00015503180660798892, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:57:36,417] Trial 50 finished with value: -0.1474144011735916 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005285911799052368, 'mc_samples_train': 1, 'prior_scale': 0.24615274327736644, 'q_scale': 0.0001326710477886506, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:58:15,074] Trial 51 finished with value: -15.382296562194824 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005087648358588085, 'mc_samples_train': 1, 'prior_scale': 0.26789989520674395, 'q_scale': 0.00010164154354987025, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:58:52,576] Trial 52 finished with value: 31.455442428588867 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004981312003472454, 'mc_samples_train': 1, 'prior_scale': 0.9276924226501947, 'q_scale': 0.00010625783562047728, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:59:29,854] Trial 53 finished with value: 49.992584228515625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002294581118532748, 'mc_samples_train': 1, 'prior_scale': 0.30142867899411774, 'q_scale': 0.00015643467994731923, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:00:00,892] Trial 14 finished with value: -43.06855773925781 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009803178104484216, 'mc_samples_train': 2, 'prior_scale': 0.034183494977898285, 'q_scale': 0.00042903850239015034, 'batch_size': 32}. Best is trial 14 with value: -43.06855773925781.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:00:08,492] Trial 54 finished with value: -12.68052864074707 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005391568570199532, 'mc_samples_train': 1, 'prior_scale': 0.20969526902399127, 'q_scale': 0.00011088986251285992, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:00:46,066] Trial 55 finished with value: 3.1866455078125 and parameters: {'pretrain_epochs': 0, 'lr': 0.00035657499749285884, 'mc_samples_train': 1, 'prior_scale': 0.1594491774644655, 'q_scale': 0.0002573251923992516, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:01:23,676] Trial 56 finished with value: -12.04144287109375 and parameters: {'pretrain_epochs': 0, 'lr': 0.00045966577927298755, 'mc_samples_train': 1, 'prior_scale': 0.38115057114944456, 'q_scale': 0.0001092609965403531, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:02:01,216] Trial 57 finished with value: 33.456321716308594 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002687523214750463, 'mc_samples_train': 1, 'prior_scale': 0.4909966024227881, 'q_scale': 0.00010776283196291782, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:02:38,415] Trial 58 finished with value: -10.45925235748291 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004244561930380383, 'mc_samples_train': 1, 'prior_scale': 0.4025899366991379, 'q_scale': 0.00020122170489877218, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:03:17,977] Trial 59 finished with value: -4.9224748611450195 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006354243142602283, 'mc_samples_train': 1, 'prior_scale': 0.6525679762316942, 'q_scale': 0.00010478021773977682, 'batch_size': 32}. Best is trial 48 with value: -17.647350311279297.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:04:15,419] Trial 15 finished with value: -40.93604278564453 and parameters: {'pretrain_epochs': 0, 'lr': 0.000993377292523993, 'mc_samples_train': 2, 'prior_scale': 0.04295889441861082, 'q_scale': 0.0005406309577127726, 'batch_size': 32}. Best is trial 14 with value: -43.06855773925781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:08:58,462] Trial 16 finished with value: -38.1905403137207 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009487405078500493, 'mc_samples_train': 2, 'prior_scale': 0.050760342213256066, 'q_scale': 0.0005478446090810961, 'batch_size': 32}. Best is trial 14 with value: -43.06855773925781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:09:55,657] Trial 17 finished with value: -134.15948486328125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009209263402796966, 'mc_samples_train': 2, 'prior_scale': 0.031696569140069956, 'q_scale': 0.0006411550999815748, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:10:51,773] Trial 18 finished with value: 205.5994110107422 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005390300331561976, 'mc_samples_train': 2, 'prior_scale': 0.24403110296651814, 'q_scale': 0.0015300746242159979, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:11:45,380] Trial 19 finished with value: 144.83404541015625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005516521531804109, 'mc_samples_train': 2, 'prior_scale': 0.025816590783190707, 'q_scale': 0.00029558508798978826, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:14:02,186] Trial 20 finished with value: -22.04224395751953 and parameters: {'pretrain_epochs': 0, 'lr': 0.00012822812473440383, 'mc_samples_train': 2, 'prior_scale': 0.51557206296688, 'q_scale': 0.005431270291843302, 'batch_size': 64}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:14:42,941] Trial 21 finished with value: 1427.0665283203125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009674537623132307, 'mc_samples_train': 2, 'prior_scale': 0.03912520435690872, 'q_scale': 0.0004450080383865306, 'batch_size': 512}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:15:34,731] Trial 22 finished with value: -47.362037658691406 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007741569589310409, 'mc_samples_train': 2, 'prior_scale': 0.025679590189724603, 'q_scale': 0.0013692432863631522, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:16:24,866] Trial 23 finished with value: 410.8078918457031 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004127308888979467, 'mc_samples_train': 2, 'prior_scale': 0.02438186220717312, 'q_scale': 0.001510920615315071, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:17:15,719] Trial 24 finished with value: 46.43812561035156 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006940711612286496, 'mc_samples_train': 2, 'prior_scale': 0.00613746135508126, 'q_scale': 0.0015735696474954964, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:18:08,018] Trial 25 finished with value: 545.3563842773438 and parameters: {'pretrain_epochs': 0, 'lr': 0.00035630520128115137, 'mc_samples_train': 2, 'prior_scale': 0.0637143680955864, 'q_scale': 0.0008616837036274713, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:18:59,739] Trial 26 finished with value: -50.967220306396484 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006954161670354888, 'mc_samples_train': 2, 'prior_scale': 0.026011168044825464, 'q_scale': 0.0002643328353889324, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:19:50,589] Trial 27 finished with value: 152.63336181640625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006259201138713117, 'mc_samples_train': 2, 'prior_scale': 0.006233958757790093, 'q_scale': 0.0002113476925237594, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:20:40,139] Trial 28 finished with value: 381.1119384765625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00042056556413592165, 'mc_samples_train': 2, 'prior_scale': 0.022963264916847255, 'q_scale': 0.0033383248817059, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:21:30,647] Trial 29 finished with value: 750.7022094726562 and parameters: {'pretrain_epochs': 0, 'lr': 0.00020665216460971823, 'mc_samples_train': 2, 'prior_scale': 0.12511530385012856, 'q_scale': 0.00023132532796375533, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:22:20,661] Trial 30 finished with value: -14.424219131469727 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007264830671177536, 'mc_samples_train': 2, 'prior_scale': 0.24374922294183668, 'q_scale': 0.013519035337525045, 'batch_size': 256}. Best is trial 17 with value: -134.15948486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:23:10,672] Trial 31 finished with value: -134.4296417236328 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007846148150382361, 'mc_samples_train': 2, 'prior_scale': 0.028059292894663905, 'q_scale': 0.00032436640461907406, 'batch_size': 256}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:24:00,849] Trial 32 finished with value: -21.154720306396484 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007364303907983839, 'mc_samples_train': 2, 'prior_scale': 0.007705406853173061, 'q_scale': 0.0008575496082650798, 'batch_size': 256}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:24:40,807] Trial 33 finished with value: 2325.223876953125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004625549128448345, 'mc_samples_train': 2, 'prior_scale': 0.07498413640468249, 'q_scale': 0.00024681105748181135, 'batch_size': 512}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:25:31,891] Trial 34 finished with value: -3.5386338233947754 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006629582933095401, 'mc_samples_train': 2, 'prior_scale': 0.016851279182046788, 'q_scale': 0.0020333412920257956, 'batch_size': 256}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 17:25:49,945] Trial 35 pruned. Trial was pruned at epoch 3.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:26:25,052] Trial 36 finished with value: -102.60403442382812 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007578957514476604, 'mc_samples_train': 1, 'prior_scale': 0.15003262337588077, 'q_scale': 0.0007664627480871191, 'batch_size': 256}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:27:46,723] Trial 37 finished with value: 109.92904663085938 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004934347393063435, 'mc_samples_train': 1, 'prior_scale': 0.10837409618405239, 'q_scale': 0.21193361588766488, 'batch_size': 64}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:28:15,852] Trial 38 finished with value: 3211.9404296875 and parameters: {'pretrain_epochs': 0, 'lr': 3.6429465259517205e-05, 'mc_samples_train': 1, 'prior_scale': 0.3912641335207644, 'q_scale': 0.0003296342971548858, 'batch_size': 512}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:28:51,826] Trial 39 finished with value: 1014.7343139648438 and parameters: {'pretrain_epochs': 0, 'lr': 1.0774254950974043e-05, 'mc_samples_train': 1, 'prior_scale': 0.14638509324689417, 'q_scale': 0.0009216034599415907, 'batch_size': 256}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:29:27,885] Trial 40 finished with value: 598.9627075195312 and parameters: {'pretrain_epochs': 0, 'lr': 0.00022892873171124025, 'mc_samples_train': 1, 'prior_scale': 0.9950731697117731, 'q_scale': 0.005872832303599179, 'batch_size': 256}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:30:07,974] Trial 41 finished with value: -91.3432388305664 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007946428228141068, 'mc_samples_train': 1, 'prior_scale': 0.020140020488069288, 'q_scale': 0.00019449872347482503, 'batch_size': 256}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:30:44,055] Trial 42 finished with value: -57.91120910644531 and parameters: {'pretrain_epochs': 0, 'lr': 0.00080130976213471, 'mc_samples_train': 1, 'prior_scale': 0.05615254688031494, 'q_scale': 0.00018565455630260784, 'batch_size': 256}. Best is trial 31 with value: -134.4296417236328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:31:19,436] Trial 43 finished with value: -166.8891143798828 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008284006247224548, 'mc_samples_train': 1, 'prior_scale': 0.1507395811686956, 'q_scale': 0.00015206099341942377, 'batch_size': 256}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:31:54,780] Trial 44 finished with value: 918.063720703125 and parameters: {'pretrain_epochs': 0, 'lr': 8.43302576989622e-05, 'mc_samples_train': 1, 'prior_scale': 0.2906232617369742, 'q_scale': 0.00016430058065208792, 'batch_size': 256}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:32:29,531] Trial 45 finished with value: 529.6715087890625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00038582057822639216, 'mc_samples_train': 1, 'prior_scale': 0.1664031524382938, 'q_scale': 0.0006684457307929858, 'batch_size': 256}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:33:19,851] Trial 46 finished with value: -105.50817108154297 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005868920054136452, 'mc_samples_train': 1, 'prior_scale': 0.1777726011235426, 'q_scale': 0.0003589252984248486, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
[W 2025-02-24 17:36:20,794] Trial 47 failed with parameters: {'pretrain_epochs': 0, 'lr': 0.0005366483166720731, 'mc_samples_train': 1, 'prior_scale': 0.08858804972118774, 'q_scale': 0.0003975301232059106, 'batch_size': 128} because of the following error: FileNotFoundError(2, 'No such file or directory').
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
OSError(5, 'Input/output error')
full_key: datamodule

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 93, in <lambda>
    lambda trial: objective(trial, cfg, output_dir),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 73, in objective_bnn
    return objective(trial, cfg, output_dir)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 32, in objective
    metric_dict, _ = train(cfg, trial)
                     ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 48, in wrap
    save_file(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 93, in save_file
    with open(path, "w+") as file:
         ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/g15farris/bin/bayesaenet/bnn_aenet/logs/rad_hps_100perc/runs/2025-02-24_16-30-24/047/exec_time.log'
[W 2025-02-24 17:36:21,372] Trial 47 failed with value None.
Error executing job with overrides: ['model=bnn_rad', 'datamodule=TiO', 'hpsearch=bnn_rad', 'task_name=rad_hps_100perc', 'tags=[TiO]', 'datamodule.valid_split=100', 'hpsearch.study.study_name=rad_100perc']
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
OSError(5, 'Input/output error')
full_key: datamodule

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 92, in main
    study.optimize(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/study.py", line 451, in optimize
    _optimize(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 62, in _optimize
    _optimize_sequential(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 159, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 247, in _run_trial
    raise func_err
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 93, in <lambda>
    lambda trial: objective(trial, cfg, output_dir),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 73, in objective_bnn
    return objective(trial, cfg, output_dir)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 32, in objective
    metric_dict, _ = train(cfg, trial)
                     ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 48, in wrap
    save_file(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 93, in save_file
    with open(path, "w+") as file:
         ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/g15farris/bin/bayesaenet/bnn_aenet/logs/rad_hps_100perc/runs/2025-02-24_16-30-24/047/exec_time.log'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-02-25 10:43:51,159] Using an existing study with name 'rad_100perc' instead of creating a new one.
[W 2025-02-25 10:57:00,756] Trial 48 failed with parameters: {'pretrain_epochs': 0, 'lr': 0.0005613407094429722, 'mc_samples_train': 1, 'prior_scale': 0.18129818438333906, 'q_scale': 0.00037902455181819953, 'batch_size': 128} because of the following error: FileNotFoundError(2, 'No such file or directory').
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
OSError(5, 'Input/output error')
full_key: datamodule

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 93, in <lambda>
    lambda trial: objective(trial, cfg, output_dir),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 73, in objective_bnn
    return objective(trial, cfg, output_dir)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 32, in objective
    metric_dict, _ = train(cfg, trial)
                     ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 48, in wrap
    save_file(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 93, in save_file
    with open(path, "w+") as file:
         ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/g15farris/bin/bayesaenet/bnn_aenet/logs/rad_hps_100perc/runs/2025-02-25_10-43-48/048/exec_time.log'
[W 2025-02-25 10:57:00,759] Trial 48 failed with value None.
Error executing job with overrides: ['model=bnn_rad', 'datamodule=TiO', 'hpsearch=bnn_rad', 'task_name=rad_hps_100perc', 'tags=[TiO]', 'datamodule.valid_split=100', 'hpsearch.study.study_name=rad_100perc']
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
OSError(5, 'Input/output error')
full_key: datamodule

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 92, in main
    study.optimize(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/study.py", line 451, in optimize
    _optimize(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 62, in _optimize
    _optimize_sequential(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 159, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 247, in _run_trial
    raise func_err
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 93, in <lambda>
    lambda trial: objective(trial, cfg, output_dir),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 73, in objective_bnn
    return objective(trial, cfg, output_dir)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 32, in objective
    metric_dict, _ = train(cfg, trial)
                     ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 48, in wrap
    save_file(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 93, in save_file
    with open(path, "w+") as file:
         ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/g15farris/bin/bayesaenet/bnn_aenet/logs/rad_hps_100perc/runs/2025-02-25_10-43-48/048/exec_time.log'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-02-25 11:01:20,409] Using an existing study with name 'rad_100perc' instead of creating a new one.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:203: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  agg_scale = precision.reciprocal().mean(dim).add(loc.var(dim)).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:02:26,273] Trial 49 finished with value: -105.80429077148438 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005613407094429722, 'mc_samples_train': 1, 'prior_scale': 0.18129818438333906, 'q_scale': 0.00037902455181819953, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:03:20,889] Trial 50 finished with value: -113.66191101074219 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005504278642518608, 'mc_samples_train': 1, 'prior_scale': 0.08869801365383095, 'q_scale': 0.0003297957589286832, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:04:19,051] Trial 51 finished with value: -24.450719833374023 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003048574049916455, 'mc_samples_train': 1, 'prior_scale': 0.07972164201237604, 'q_scale': 0.00010380787735100396, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:05:16,591] Trial 52 finished with value: -80.6259765625 and parameters: {'pretrain_epochs': 0, 'lr': 0.000536286623244608, 'mc_samples_train': 1, 'prior_scale': 0.09057090300643525, 'q_scale': 0.0024521590833100388, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:06:15,248] Trial 53 finished with value: -105.52009582519531 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005638485029790227, 'mc_samples_train': 1, 'prior_scale': 0.18162169046510418, 'q_scale': 0.000395289286613123, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:07:11,031] Trial 54 finished with value: -29.3430233001709 and parameters: {'pretrain_epochs': 0, 'lr': 0.00047480453693881617, 'mc_samples_train': 1, 'prior_scale': 0.32219383836638055, 'q_scale': 0.00046018792849455455, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:08:07,690] Trial 55 finished with value: -78.29817199707031 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008521321521955661, 'mc_samples_train': 1, 'prior_scale': 0.20580252417824932, 'q_scale': 0.0001344190104607116, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:09:12,032] Trial 56 finished with value: -118.32482147216797 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006079925390383161, 'mc_samples_train': 1, 'prior_scale': 0.49374820992211227, 'q_scale': 0.0003545951505743899, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:10:08,210] Trial 57 finished with value: 451.3868408203125 and parameters: {'pretrain_epochs': 0, 'lr': 5.59425827360431e-05, 'mc_samples_train': 1, 'prior_scale': 0.5614793167673252, 'q_scale': 0.0006196644973683871, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:11:05,823] Trial 58 finished with value: 190.23809814453125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009805539390864814, 'mc_samples_train': 1, 'prior_scale': 0.04594714870723968, 'q_scale': 0.03896144511605216, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:12:02,432] Trial 59 finished with value: -98.42029571533203 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006192189299303569, 'mc_samples_train': 1, 'prior_scale': 0.3990725694014223, 'q_scale': 0.0011274742042407398, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:13:35,970] Trial 60 finished with value: 248.98841857910156 and parameters: {'pretrain_epochs': 0, 'lr': 0.00042646371192664044, 'mc_samples_train': 1, 'prior_scale': 0.0014214803722818502, 'q_scale': 0.0001551436902381684, 'batch_size': 64}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:14:32,616] Trial 61 finished with value: 312.46917724609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.00013598487013221397, 'mc_samples_train': 1, 'prior_scale': 0.0996273367073532, 'q_scale': 0.0002981470189048072, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:15:25,763] Trial 62 finished with value: -35.166893005371094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002577863521568503, 'mc_samples_train': 1, 'prior_scale': 0.9338023382063743, 'q_scale': 0.0005449753048371786, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:16:26,235] Trial 63 finished with value: -130.19822692871094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005588896752710546, 'mc_samples_train': 1, 'prior_scale': 0.20666408199737438, 'q_scale': 0.00039953050229747146, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:17:23,568] Trial 64 finished with value: -132.32772827148438 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008819107841862346, 'mc_samples_train': 1, 'prior_scale': 0.5767204920102603, 'q_scale': 0.0003488071407598603, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:18:19,820] Trial 65 finished with value: -116.95478820800781 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008186668361454661, 'mc_samples_train': 1, 'prior_scale': 0.6280616550905697, 'q_scale': 0.0002789362271663289, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:19:12,997] Trial 66 finished with value: -50.595088958740234 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008676618671986171, 'mc_samples_train': 1, 'prior_scale': 0.6776363694977932, 'q_scale': 0.00023297695210412807, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:19:44,696] Trial 67 finished with value: 1118.093017578125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008562712249295309, 'mc_samples_train': 1, 'prior_scale': 0.7111891685351078, 'q_scale': 0.001102792776388093, 'batch_size': 512}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:22:29,055] Trial 68 finished with value: -39.51170349121094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006532486996326448, 'mc_samples_train': 1, 'prior_scale': 0.3309592471938878, 'q_scale': 0.0001282290739478354, 'batch_size': 32}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:23:24,997] Trial 69 finished with value: -26.957048416137695 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009590272493470675, 'mc_samples_train': 1, 'prior_scale': 0.5112834518144872, 'q_scale': 0.00062103274690036, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:24:57,483] Trial 70 finished with value: -82.95844268798828 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006781706384372672, 'mc_samples_train': 1, 'prior_scale': 0.44758651941531713, 'q_scale': 0.0005144092705777254, 'batch_size': 64}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:26:22,657] Trial 71 finished with value: -81.06729125976562 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008616528246308509, 'mc_samples_train': 2, 'prior_scale': 0.5800447989886094, 'q_scale': 0.000268119078487451, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:27:18,131] Trial 72 finished with value: 122.2187271118164 and parameters: {'pretrain_epochs': 0, 'lr': 0.00035053802964882453, 'mc_samples_train': 1, 'prior_scale': 0.00338235824897984, 'q_scale': 0.00016551096525002157, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:28:15,096] Trial 73 finished with value: -62.131446838378906 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004787677236022952, 'mc_samples_train': 1, 'prior_scale': 0.8299938984602954, 'q_scale': 0.0003702844582912504, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:29:11,278] Trial 74 finished with value: -118.80400848388672 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007250148610726605, 'mc_samples_train': 1, 'prior_scale': 0.24137552197423798, 'q_scale': 0.0003051260533557736, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:30:06,402] Trial 75 finished with value: -101.27973937988281 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007210367589662488, 'mc_samples_train': 1, 'prior_scale': 0.39082634008506933, 'q_scale': 0.00020660282773846017, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:31:08,039] Trial 76 finished with value: -159.13059997558594 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008844203375073474, 'mc_samples_train': 1, 'prior_scale': 0.27476930802852306, 'q_scale': 0.00010199663749829745, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:35:37,343] Trial 77 finished with value: -6.825471878051758 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009845722773888625, 'mc_samples_train': 2, 'prior_scale': 0.12728295033256484, 'q_scale': 0.00010238424722601798, 'batch_size': 32}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:36:08,531] Trial 78 finished with value: 2130.3935546875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006378405170216925, 'mc_samples_train': 1, 'prior_scale': 0.26845915775568846, 'q_scale': 0.00015520054160610006, 'batch_size': 512}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-25 11:37:16,651] Trial 79 pruned. Trial was pruned at epoch 14.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:37:52,854] Trial 80 finished with value: 235.9306640625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005030690579090404, 'mc_samples_train': 1, 'prior_scale': 0.3417502749344994, 'q_scale': 0.0001267442092374008, 'batch_size': 256}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:39:27,943] Trial 81 finished with value: 250.54029846191406 and parameters: {'pretrain_epochs': 0, 'lr': 2.9783953245032526e-05, 'mc_samples_train': 1, 'prior_scale': 0.010444894633697863, 'q_scale': 0.0004758977613573213, 'batch_size': 64}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:40:52,637] Trial 82 finished with value: 500.18798828125 and parameters: {'pretrain_epochs': 0, 'lr': 1.864916196658791e-05, 'mc_samples_train': 2, 'prior_scale': 0.037416272957509815, 'q_scale': 0.0002270124312928411, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:41:49,381] Trial 83 finished with value: -105.7856216430664 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008469067078617973, 'mc_samples_train': 1, 'prior_scale': 0.2375080738951325, 'q_scale': 0.00028383877063662023, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:42:45,409] Trial 84 finished with value: -12.264886856079102 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007899085602464662, 'mc_samples_train': 1, 'prior_scale': 0.634222242240703, 'q_scale': 0.0007608131667524124, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:43:42,629] Trial 85 finished with value: -88.65821075439453 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008760408317413406, 'mc_samples_train': 1, 'prior_scale': 0.8463880559891005, 'q_scale': 0.00019746824020498035, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:44:39,587] Trial 86 finished with value: -164.43931579589844 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006208331217624857, 'mc_samples_train': 1, 'prior_scale': 0.4706817458034839, 'q_scale': 0.00028492735949976327, 'batch_size': 128}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:45:16,595] Trial 87 finished with value: -128.27528381347656 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006212691629465956, 'mc_samples_train': 1, 'prior_scale': 0.4164852815405968, 'q_scale': 0.00042663036872788666, 'batch_size': 256}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:45:55,447] Trial 88 finished with value: 375.6255798339844 and parameters: {'pretrain_epochs': 0, 'lr': 0.00041385500524081736, 'mc_samples_train': 1, 'prior_scale': 0.28906581009331583, 'q_scale': 0.0004544175008804786, 'batch_size': 256}. Best is trial 43 with value: -166.8891143798828.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:46:51,637] Trial 89 finished with value: -210.18203735351562 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006946701915739393, 'mc_samples_train': 2, 'prior_scale': 0.4370624624932002, 'q_scale': 0.00014178811272524028, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:47:47,600] Trial 90 finished with value: 8.72994613647461 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006439684631850528, 'mc_samples_train': 2, 'prior_scale': 0.4012633969096955, 'q_scale': 0.01562832356166279, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:48:44,090] Trial 91 finished with value: 186.64402770996094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005232530315163862, 'mc_samples_train': 2, 'prior_scale': 0.020386591553731348, 'q_scale': 0.00011528433179902608, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:49:39,948] Trial 92 finished with value: 66.2156982421875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005964421309352346, 'mc_samples_train': 2, 'prior_scale': 0.0291571491976118, 'q_scale': 0.004602926276348696, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:50:36,444] Trial 93 finished with value: -121.5926513671875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007405884611712769, 'mc_samples_train': 2, 'prior_scale': 0.45662791803422503, 'q_scale': 0.00017565365765270422, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:51:32,550] Trial 94 finished with value: -73.40869140625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009139193938341889, 'mc_samples_train': 2, 'prior_scale': 0.45513529027964195, 'q_scale': 0.00016613246777992999, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:52:33,622] Trial 95 finished with value: -20.71886444091797 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007274806329915361, 'mc_samples_train': 2, 'prior_scale': 0.34486407857069173, 'q_scale': 0.00019123580674472682, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:53:30,126] Trial 96 finished with value: 8.771991729736328 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007802717800885018, 'mc_samples_train': 2, 'prior_scale': 0.013271930288414701, 'q_scale': 0.0001377287791628607, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:54:26,707] Trial 97 finished with value: -44.412296295166016 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006001794026310795, 'mc_samples_train': 2, 'prior_scale': 0.7611815033237547, 'q_scale': 0.0002465293620127091, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:55:23,457] Trial 98 finished with value: 272.7962646484375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004470815273272695, 'mc_samples_train': 2, 'prior_scale': 0.4570334601110053, 'q_scale': 0.00013896865155212492, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-25 11:56:06,326] Trial 99 pruned. Trial was pruned at epoch 13.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:57:02,910] Trial 100 finished with value: -17.01715850830078 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006624925253656239, 'mc_samples_train': 2, 'prior_scale': 0.5361642714150067, 'q_scale': 0.000996844384291834, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:57:56,369] Trial 101 finished with value: 493.32989501953125 and parameters: {'pretrain_epochs': 0, 'lr': 0.00037921062134637187, 'mc_samples_train': 2, 'prior_scale': 0.06484248044619553, 'q_scale': 0.0003910468923626652, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:58:52,986] Trial 102 finished with value: 161.1352081298828 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005246305207143467, 'mc_samples_train': 2, 'prior_scale': 0.20475260509458515, 'q_scale': 0.00017871661196896928, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:59:31,670] Trial 103 finished with value: -124.38442993164062 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007348431630668557, 'mc_samples_train': 1, 'prior_scale': 0.2537618060743946, 'q_scale': 0.0002373388343417625, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:00:10,589] Trial 104 finished with value: 72.09341430664062 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007757477897180654, 'mc_samples_train': 1, 'prior_scale': 0.3701718482469813, 'q_scale': 0.0007082798853231064, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:00:49,764] Trial 105 finished with value: -162.43519592285156 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009073973723346485, 'mc_samples_train': 1, 'prior_scale': 0.26220316912079705, 'q_scale': 0.00010135828813956661, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:01:28,003] Trial 106 finished with value: -138.3784637451172 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009039811079173144, 'mc_samples_train': 1, 'prior_scale': 0.15541541841700707, 'q_scale': 0.00022942989044644373, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:02:07,084] Trial 107 finished with value: -150.27464294433594 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009227243976030666, 'mc_samples_train': 1, 'prior_scale': 0.15441165543047772, 'q_scale': 0.00010200479611728025, 'batch_size': 256}. Best is trial 89 with value: -210.18203735351562.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:02:37,124] Trial 108 finished with value: 1681.7698974609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008821922776024473, 'mc_samples_train': 1, 'prior_scale': 0.15353728247268095, 'q_scale': 0.00012981513969001702, 'batch_size': 512}. Best is trial 89 with value: -210.18203735351562.
