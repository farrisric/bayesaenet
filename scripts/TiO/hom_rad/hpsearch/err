Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 105, in run
    cfg = self.compose_config(
          ^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 594, in compose_config
    cfg = self.config_loader.load_configuration(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/config_loader_impl.py", line 142, in load_configuration
    return self._load_configuration_impl(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/config_loader_impl.py", line 253, in _load_configuration_impl
    defaults_list = create_defaults_list(
                    ^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/defaults_list.py", line 745, in create_defaults_list
    defaults, tree = _create_defaults_list(
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/defaults_list.py", line 715, in _create_defaults_list
    defaults_tree = _create_defaults_tree(
                    ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/defaults_list.py", line 356, in _create_defaults_tree
    ret = _create_defaults_tree_impl(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/defaults_list.py", line 457, in _create_defaults_tree_impl
    return _expand_virtual_root(repo, root, overrides, skip_missing)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/defaults_list.py", line 280, in _expand_virtual_root
    subtree = _create_defaults_tree_impl(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/defaults_list.py", line 573, in _create_defaults_tree_impl
    add_child(children, new_root)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/defaults_list.py", line 520, in add_child
    subtree_ = _create_defaults_tree_impl(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/defaults_list.py", line 466, in _create_defaults_tree_impl
    update_package_header(repo=repo, node=parent)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/defaults_list.py", line 262, in update_package_header
    loaded = repo.load_config(config_path=node.get_config_path())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/config_repository.py", line 348, in load_config
    ret = self.delegate.load_config(config_path=config_path)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/config_repository.py", line 91, in load_config
    ret = source.load_config(config_path=config_path)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/core_plugins/file_config_source.py", line 31, in load_config
    cfg = OmegaConf.load(f)
          ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/omegaconf/omegaconf.py", line 192, in load
    obj = yaml.load(file_, Loader=get_yaml_loader())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/composer.py", line 36, in get_single_node
    document = self.compose_document()
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/composer.py", line 55, in compose_document
    node = self.compose_node(None, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/composer.py", line 133, in compose_mapping_node
    item_value = self.compose_node(node, item_key)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/composer.py", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/parser.py", line 98, in check_event
    self.current_event = self.state()
                         ^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/parser.py", line 428, in parse_block_mapping_key
    if self.check_token(KeyToken):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/scanner.py", line 116, in check_token
    self.fetch_more_tokens()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/scanner.py", line 223, in fetch_more_tokens
    return self.fetch_value()
           ^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/yaml/scanner.py", line 577, in fetch_value
    raise ScannerError(None, None,
yaml.scanner.ScannerError: mapping values are not allowed here
  in "/home/g15farris/bin/bayesaenet/bnn_aenet/configs/hpsearch/hom_rad.yaml", line 10, column 15
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-05-12 11:50:42,144] A new study created in RDB with name: hom_rad_100perc_big
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:260: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  scale = predictions.var(dim).add(self.scale ** 2).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-12 12:33:44,100] Trial 0 finished with value: 446385696.0 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 0.22702608040701588, 'q_scale': 0.13179630432958683, 'obs_scale': 0.0012313185468743894, 'batch_size': 128}. Best is trial 0 with value: 446385696.0.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-12 13:38:41,047] Trial 1 finished with value: 59216.265625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00010045932391231576, 'mc_samples_train': 2, 'prior_scale': 0.012904829303853454, 'q_scale': 0.017570525244134657, 'obs_scale': 0.010288040405240286, 'batch_size': 128}. Best is trial 1 with value: 59216.265625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-12 14:47:14,581] Trial 2 finished with value: 94.80903625488281 and parameters: {'pretrain_epochs': 0, 'lr': 1.4150196905720475e-05, 'mc_samples_train': 2, 'prior_scale': 0.08997760513084464, 'q_scale': 0.0038798086852341396, 'obs_scale': 0.14286326515987452, 'batch_size': 128}. Best is trial 2 with value: 94.80903625488281.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-12 19:41:50,324] Trial 3 finished with value: 3529372.75 and parameters: {'pretrain_epochs': 0, 'lr': 1.9388028480984598e-05, 'mc_samples_train': 2, 'prior_scale': 0.004532901866195528, 'q_scale': 0.5005765657505178, 'obs_scale': 0.005868985298319507, 'batch_size': 32}. Best is trial 2 with value: 94.80903625488281.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-12 21:52:25,148] Trial 4 finished with value: 48733.12890625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001546142648648772, 'mc_samples_train': 1, 'prior_scale': 0.04833917568085432, 'q_scale': 0.0020829257190366937, 'obs_scale': 0.010277023093936914, 'batch_size': 512}. Best is trial 2 with value: 94.80903625488281.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 03:04:23,621] Trial 5 finished with value: 73930.078125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006671842305866796, 'mc_samples_train': 2, 'prior_scale': 0.7523246408184491, 'q_scale': 0.14718262393979, 'obs_scale': 0.001383578612730786, 'batch_size': 32}. Best is trial 2 with value: 94.80903625488281.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[I 2025-05-13 03:06:27,350] Trial 6 pruned. Trial was pruned at epoch 65.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 03:59:32,548] Trial 7 finished with value: 699.6599731445312 and parameters: {'pretrain_epochs': 0, 'lr': 0.00011795146111975982, 'mc_samples_train': 2, 'prior_scale': 0.020449350394769326, 'q_scale': 0.028092862158076635, 'obs_scale': 0.470752138441911, 'batch_size': 512}. Best is trial 2 with value: 94.80903625488281.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 04:50:44,325] Trial 8 finished with value: 144.777099609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.00018511923116382368, 'mc_samples_train': 1, 'prior_scale': 0.0507257930731161, 'q_scale': 0.012968719897940156, 'obs_scale': 0.6393152366775274, 'batch_size': 256}. Best is trial 2 with value: 94.80903625488281.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 05:34:07,797] Trial 9 finished with value: 659.921630859375 and parameters: {'pretrain_epochs': 0, 'lr': 0.00021819434272647882, 'mc_samples_train': 1, 'prior_scale': 0.24581113447438505, 'q_scale': 0.0170232827909058, 'obs_scale': 0.730039111673774, 'batch_size': 512}. Best is trial 2 with value: 94.80903625488281.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 07:08:41,653] Trial 10 finished with value: -107.5256118774414 and parameters: {'pretrain_epochs': 0, 'lr': 1.1267228743583923e-05, 'mc_samples_train': 2, 'prior_scale': 0.1438501886620343, 'q_scale': 0.0001361190166330691, 'obs_scale': 0.12508757174951737, 'batch_size': 128}. Best is trial 10 with value: -107.5256118774414.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 08:39:51,960] Trial 11 finished with value: -137.54718017578125 and parameters: {'pretrain_epochs': 0, 'lr': 1.0962454827051684e-05, 'mc_samples_train': 2, 'prior_scale': 0.13160546287420333, 'q_scale': 0.00011306332444916999, 'obs_scale': 0.10193657033086194, 'batch_size': 128}. Best is trial 11 with value: -137.54718017578125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 11:02:21,406] Trial 12 finished with value: -81.16398620605469 and parameters: {'pretrain_epochs': 0, 'lr': 1.0170394077008131e-05, 'mc_samples_train': 2, 'prior_scale': 0.2238470043565636, 'q_scale': 0.00012780894352276231, 'obs_scale': 0.07946981200360269, 'batch_size': 64}. Best is trial 11 with value: -137.54718017578125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 12:53:55,586] Trial 13 finished with value: -74.41067504882812 and parameters: {'pretrain_epochs': 0, 'lr': 3.426915319227129e-05, 'mc_samples_train': 2, 'prior_scale': 0.9713491325966092, 'q_scale': 0.00011989003863340673, 'obs_scale': 0.07411551140013035, 'batch_size': 128}. Best is trial 11 with value: -137.54718017578125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-13 13:01:37,535] Trial 14 pruned. Trial was pruned at epoch 174.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 15:37:07,399] Trial 15 finished with value: -26.189516067504883 and parameters: {'pretrain_epochs': 0, 'lr': 1.0003499066452017e-05, 'mc_samples_train': 2, 'prior_scale': 0.14028905483022555, 'q_scale': 0.0005351337564036122, 'obs_scale': 0.24770840369488722, 'batch_size': 64}. Best is trial 11 with value: -137.54718017578125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 17:38:15,400] Trial 16 finished with value: -117.06011199951172 and parameters: {'pretrain_epochs': 0, 'lr': 4.884207813495069e-05, 'mc_samples_train': 2, 'prior_scale': 0.3785957853752036, 'q_scale': 0.0005845252980016993, 'obs_scale': 0.03804737433257131, 'batch_size': 128}. Best is trial 11 with value: -137.54718017578125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 19:33:54,489] Trial 17 finished with value: -146.5489501953125 and parameters: {'pretrain_epochs': 0, 'lr': 0.00038765931042035386, 'mc_samples_train': 2, 'prior_scale': 0.4343628588344081, 'q_scale': 0.000551662647177075, 'obs_scale': 0.03315568936559784, 'batch_size': 128}. Best is trial 17 with value: -146.5489501953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-13 19:39:36,532] Trial 18 pruned. Trial was pruned at epoch 127.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 20:40:31,619] Trial 19 finished with value: 195.9746551513672 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003728050271772191, 'mc_samples_train': 2, 'prior_scale': 0.024870896588286166, 'q_scale': 0.0003307581823536927, 'obs_scale': 0.02751629365086828, 'batch_size': 256}. Best is trial 17 with value: -146.5489501953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-13 20:41:53,548] Trial 20 pruned. Trial was pruned at epoch 18.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 21:57:34,931] Trial 21 finished with value: -56.359214782714844 and parameters: {'pretrain_epochs': 0, 'lr': 4.826286832294235e-05, 'mc_samples_train': 2, 'prior_scale': 0.31747966026722296, 'q_scale': 0.0003415526872390564, 'obs_scale': 0.03408110307600863, 'batch_size': 128}. Best is trial 17 with value: -146.5489501953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-13 23:07:04,723] Trial 22 finished with value: -109.7126235961914 and parameters: {'pretrain_epochs': 0, 'lr': 0.00028820788269183164, 'mc_samples_train': 2, 'prior_scale': 0.43273462620143266, 'q_scale': 0.0009859287161846284, 'obs_scale': 0.04919707176828436, 'batch_size': 128}. Best is trial 17 with value: -146.5489501953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 00:14:30,848] Trial 23 finished with value: -49.6732063293457 and parameters: {'pretrain_epochs': 0, 'lr': 5.238169217845394e-05, 'mc_samples_train': 2, 'prior_scale': 0.4330488602334728, 'q_scale': 0.00028166269235788594, 'obs_scale': 0.250659780213397, 'batch_size': 128}. Best is trial 17 with value: -146.5489501953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 00:19:44,989] Trial 24 pruned. Trial was pruned at epoch 156.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 00:29:32,625] Trial 25 pruned. Trial was pruned at epoch 93.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 01:10:23,468] Trial 26 finished with value: -54.19322204589844 and parameters: {'pretrain_epochs': 0, 'lr': 1.6960704889299183e-05, 'mc_samples_train': 1, 'prior_scale': 0.09011554759866658, 'q_scale': 0.001078753664675607, 'obs_scale': 0.22213860766593982, 'batch_size': 128}. Best is trial 17 with value: -146.5489501953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 02:13:52,923] Trial 27 finished with value: -69.60344696044922 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002832419881966086, 'mc_samples_train': 2, 'prior_scale': 0.19707740386367245, 'q_scale': 0.0005765651018710545, 'obs_scale': 0.05620006458995668, 'batch_size': 128}. Best is trial 17 with value: -146.5489501953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 02:17:54,788] Trial 28 pruned. Trial was pruned at epoch 123.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 02:55:45,024] Trial 29 finished with value: -125.2209701538086 and parameters: {'pretrain_epochs': 0, 'lr': 2.5229410469940185e-05, 'mc_samples_train': 1, 'prior_scale': 0.9551490751052607, 'q_scale': 0.0001964706891732991, 'obs_scale': 0.10402803915417923, 'batch_size': 128}. Best is trial 17 with value: -146.5489501953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 03:02:53,638] Trial 30 pruned. Trial was pruned at epoch 122.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 03:40:12,052] Trial 31 finished with value: -108.62326049804688 and parameters: {'pretrain_epochs': 0, 'lr': 2.7520020320248394e-05, 'mc_samples_train': 1, 'prior_scale': 0.5212065403783638, 'q_scale': 0.0004020615230314697, 'obs_scale': 0.11825917229834086, 'batch_size': 128}. Best is trial 17 with value: -146.5489501953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 03:44:10,900] Trial 32 pruned. Trial was pruned at epoch 214.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 03:52:38,066] Trial 33 pruned. Trial was pruned at epoch 456.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 03:57:45,136] Trial 34 pruned. Trial was pruned at epoch 274.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 04:59:30,462] Trial 35 finished with value: -81.52513885498047 and parameters: {'pretrain_epochs': 0, 'lr': 7.307018218980261e-05, 'mc_samples_train': 2, 'prior_scale': 0.060892016266996626, 'q_scale': 0.0015558551569959016, 'obs_scale': 0.16376442898351837, 'batch_size': 128}. Best is trial 17 with value: -146.5489501953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 04:59:44,715] Trial 36 pruned. Trial was pruned at epoch 8.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 05:16:06,104] Trial 37 pruned. Trial was pruned at epoch 831.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 05:16:22,016] Trial 38 pruned. Trial was pruned at epoch 4.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 05:25:05,908] Trial 39 pruned. Trial was pruned at epoch 144.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 05:29:42,518] Trial 40 pruned. Trial was pruned at epoch 83.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 06:33:31,473] Trial 41 finished with value: -181.38616943359375 and parameters: {'pretrain_epochs': 0, 'lr': 0.00042005140768173045, 'mc_samples_train': 2, 'prior_scale': 0.4221057879233836, 'q_scale': 0.0009685869900113073, 'obs_scale': 0.06685024427612524, 'batch_size': 128}. Best is trial 41 with value: -181.38616943359375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 07:40:31,054] Trial 42 finished with value: -93.42288208007812 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006081877080686448, 'mc_samples_train': 2, 'prior_scale': 0.3923410955596767, 'q_scale': 0.0006110044560783872, 'obs_scale': 0.16575193962845733, 'batch_size': 128}. Best is trial 41 with value: -181.38616943359375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 08:46:39,696] Trial 43 finished with value: -169.76431274414062 and parameters: {'pretrain_epochs': 0, 'lr': 0.00047383773595546905, 'mc_samples_train': 2, 'prior_scale': 0.7504652817461253, 'q_scale': 0.00016884874792017624, 'obs_scale': 0.07789031236421033, 'batch_size': 128}. Best is trial 41 with value: -181.38616943359375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 08:49:06,408] Trial 44 pruned. Trial was pruned at epoch 150.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 09:09:14,296] Trial 45 pruned. Trial was pruned at epoch 603.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 09:14:09,608] Trial 46 pruned. Trial was pruned at epoch 352.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 10:18:55,097] Trial 47 finished with value: -145.7663116455078 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007191400386354072, 'mc_samples_train': 2, 'prior_scale': 0.038700289355471054, 'q_scale': 0.00037601106997545635, 'obs_scale': 0.06922132018801168, 'batch_size': 128}. Best is trial 41 with value: -181.38616943359375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 11:32:38,990] Trial 48 finished with value: -172.02474975585938 and parameters: {'pretrain_epochs': 0, 'lr': 0.000720421366293017, 'mc_samples_train': 2, 'prior_scale': 0.038450340150464515, 'q_scale': 0.0014823543744103553, 'obs_scale': 0.07385835433748311, 'batch_size': 128}. Best is trial 41 with value: -181.38616943359375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 11:34:26,419] Trial 49 pruned. Trial was pruned at epoch 68.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 12:53:40,049] Trial 50 finished with value: 44.452491760253906 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007730702081887369, 'mc_samples_train': 2, 'prior_scale': 0.01152235331935026, 'q_scale': 0.0013421818436830824, 'obs_scale': 0.060808990907371706, 'batch_size': 128}. Best is trial 41 with value: -181.38616943359375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 13:44:55,274] Trial 51 pruned. Trial was pruned at epoch 1417.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 14:03:36,250] Trial 52 pruned. Trial was pruned at epoch 552.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-14 15:13:22,700] Trial 53 finished with value: -144.8254852294922 and parameters: {'pretrain_epochs': 0, 'lr': 0.00022357950439642874, 'mc_samples_train': 2, 'prior_scale': 0.03738307793311759, 'q_scale': 0.0004204450632494787, 'obs_scale': 0.08663771651007465, 'batch_size': 128}. Best is trial 41 with value: -181.38616943359375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 15:16:02,138] Trial 54 pruned. Trial was pruned at epoch 68.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 15:48:38,994] Trial 55 pruned. Trial was pruned at epoch 552.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 15:57:29,509] Trial 56 pruned. Trial was pruned at epoch 247.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 16:08:11,452] Trial 57 pruned. Trial was pruned at epoch 307.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 16:20:03,439] Trial 58 pruned. Trial was pruned at epoch 113.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-14 16:21:40,236] Trial 59 pruned. Trial was pruned at epoch 59.
