cat: /scratch/3203748.1.iqtc09.q/.gpus: No such file or directory
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-05-17_09-23-04/checkpoints/epoch_237-step_68544.ckpt
Error executing job with overrides: ['experiment=bnn_rad', 'trainer.deterministic=False', 'trainer.min_epochs=10000', 'task_name=train_rad', 'datamodule=TiO', 'model.pretrain_epochs=0', 'model.lr=0.0009972712985773986', 'model.mc_samples_train=2', 'model.prior_scale=0.19489418198868783', 'model.q_scale=0.005335664094638404', 'model.obs_scale=0.8659771571548253', 'datamodule.batch_size=32', 'tags=[TiO]', 'ckpt_path=/home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-05-17_09-23-04/checkpoints/epoch_237-step_68544.ckpt']
Traceback (most recent call last):
  File "/home/g15farris/bin/forks/bayesaenet/src/tasks/train.py", line 147, in main
    train(cfg, trial=None)
  File "/home/g15farris/bin/forks/bayesaenet/src/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/forks/bayesaenet/src/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/forks/bayesaenet/src/tasks/train.py", line 89, in train
    trainer.fit(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 956, in _run
    self._checkpoint_connector._restore_modules_and_callbacks(ckpt_path)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py", line 398, in _restore_modules_and_callbacks
    self.restore_model()
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py", line 275, in restore_model
    self.trainer.strategy.load_model_state_dict(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 372, in load_model_state_dict
    self.lightning_module.load_state_dict(checkpoint["state_dict"], strict=strict)
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2189, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for BNN:
	size mismatch for net.functions.0.Linear_Sp1_F1.weight: copying a param with shape torch.Size([15, 60]) from checkpoint, the shape in current model is torch.Size([15, 56]).
	size mismatch for net.functions.0.Linear_Sp1_F2.weight: copying a param with shape torch.Size([10, 15]) from checkpoint, the shape in current model is torch.Size([15, 15]).
	size mismatch for net.functions.0.Linear_Sp1_F2.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([15]).
	size mismatch for net.functions.0.Linear_Sp1_F3.weight: copying a param with shape torch.Size([1, 10]) from checkpoint, the shape in current model is torch.Size([1, 15]).
	size mismatch for net.functions.1.Linear_Sp2_F1.weight: copying a param with shape torch.Size([15, 60]) from checkpoint, the shape in current model is torch.Size([15, 56]).
	size mismatch for net.functions.1.Linear_Sp2_F2.weight: copying a param with shape torch.Size([10, 15]) from checkpoint, the shape in current model is torch.Size([15, 15]).
	size mismatch for net.functions.1.Linear_Sp2_F2.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([15]).
	size mismatch for net.functions.1.Linear_Sp2_F3.weight: copying a param with shape torch.Size([1, 10]) from checkpoint, the shape in current model is torch.Size([1, 15]).

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
cat: /scratch/3203791.1.iqtc09.q/.gpus: No such file or directory
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
`Trainer.fit` stopped: `max_epochs=10000` reached.
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-07-08_14-20-07/checkpoints/epoch_7720-step_193025.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-07-08_14-20-07/checkpoints/epoch_7720-step_193025.ckpt
cat: /scratch/3205507.1.iqtc09.q/.gpus: No such file or directory
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:260: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  scale = predictions.var(dim).add(self.scale ** 2).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Trainer was signaled to stop but the required `min_epochs=10000` or `min_steps=None` has not been met. Training will continue...
`Trainer.fit` stopped: `max_epochs=10000` reached.
Restoring states from the checkpoint path at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-07-10_09-29-38/checkpoints/epoch_2468-step_481455.ckpt
Loaded model weights from the checkpoint at /home/g15farris/bin/forks/bayesaenet/src/logs/train_rad/runs/2024-07-10_09-29-38/checkpoints/epoch_2468-step_481455.ckpt
