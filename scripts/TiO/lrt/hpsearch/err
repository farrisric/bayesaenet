/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-02-24 16:30:18,275] A new study created in RDB with name: lrt_100perc
[I 2025-02-24 16:30:18,281] A new study created in RDB with name: lrt_20perc
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:203: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  agg_scale = precision.reciprocal().mean(dim).add(loc.var(dim)).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:203: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  agg_scale = precision.reciprocal().mean(dim).add(loc.var(dim)).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:30:46,123] Trial 0 finished with value: 660.906005859375 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 0.22702608040701588, 'q_scale': 0.13179630432958683, 'batch_size': 256}. Best is trial 0 with value: 660.906005859375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:09,317] Trial 1 finished with value: 617.3781127929688 and parameters: {'pretrain_epochs': 0, 'lr': 5.195586024325663e-05, 'mc_samples_train': 2, 'prior_scale': 0.1374377640063412, 'q_scale': 0.0030269344709026166, 'batch_size': 512}. Best is trial 1 with value: 617.3781127929688.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:10,702] Trial 0 finished with value: 986.4046630859375 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 0.22702608040701588, 'q_scale': 0.13179630432958683, 'batch_size': 256}. Best is trial 0 with value: 986.4046630859375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:34,122] Trial 2 finished with value: 4258.849609375 and parameters: {'pretrain_epochs': 0, 'lr': 5.367498945698204e-05, 'mc_samples_train': 1, 'prior_scale': 0.012778268753495938, 'q_scale': 0.540207249341968, 'batch_size': 128}. Best is trial 1 with value: 617.3781127929688.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:32:08,332] Trial 3 finished with value: 1070.847900390625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005473369642905611, 'mc_samples_train': 2, 'prior_scale': 0.002699606552195955, 'q_scale': 0.06562115716452717, 'batch_size': 128}. Best is trial 1 with value: 617.3781127929688.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:32:09,351] Trial 1 finished with value: 3103.0 and parameters: {'pretrain_epochs': 0, 'lr': 5.195586024325663e-05, 'mc_samples_train': 2, 'prior_scale': 0.1374377640063412, 'q_scale': 0.0030269344709026166, 'batch_size': 512}. Best is trial 0 with value: 986.4046630859375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:32:33,269] Trial 4 finished with value: 612.0908203125 and parameters: {'pretrain_epochs': 0, 'lr': 1.3170990774640101e-05, 'mc_samples_train': 1, 'prior_scale': 0.10583829396911469, 'q_scale': 0.023905570899706415, 'batch_size': 128}. Best is trial 4 with value: 612.0908203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:32:46,535] Trial 5 pruned. Trial was pruned at epoch 3.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:33:13,373] Trial 6 finished with value: 606.154052734375 and parameters: {'pretrain_epochs': 0, 'lr': 3.71964865643354e-05, 'mc_samples_train': 1, 'prior_scale': 0.0038638969563953266, 'q_scale': 0.0033827492632411567, 'batch_size': 128}. Best is trial 6 with value: 606.154052734375.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:33:14,715] Trial 2 finished with value: 1059.150390625 and parameters: {'pretrain_epochs': 0, 'lr': 5.367498945698204e-05, 'mc_samples_train': 1, 'prior_scale': 0.012778268753495938, 'q_scale': 0.540207249341968, 'batch_size': 128}. Best is trial 0 with value: 986.4046630859375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:34:07,100] Trial 7 finished with value: 147.1673126220703 and parameters: {'pretrain_epochs': 0, 'lr': 0.00029994809793642315, 'mc_samples_train': 1, 'prior_scale': 0.0020959477484878194, 'q_scale': 0.0008257988273685458, 'batch_size': 32}. Best is trial 7 with value: 147.1673126220703.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:34:39,949] Trial 8 finished with value: 597.3457641601562 and parameters: {'pretrain_epochs': 0, 'lr': 7.478015213333154e-05, 'mc_samples_train': 2, 'prior_scale': 0.07537199486913888, 'q_scale': 0.0666791815288874, 'batch_size': 128}. Best is trial 7 with value: 147.1673126220703.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:34:58,443] Trial 3 finished with value: 665.6839599609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005473369642905611, 'mc_samples_train': 2, 'prior_scale': 0.002699606552195955, 'q_scale': 0.06562115716452717, 'batch_size': 128}. Best is trial 3 with value: 665.6839599609375.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:35:03,547] Trial 9 finished with value: 561.09765625 and parameters: {'pretrain_epochs': 0, 'lr': 2.01904291186572e-05, 'mc_samples_train': 1, 'prior_scale': 0.714967484470097, 'q_scale': 0.00834519932080496, 'batch_size': 128}. Best is trial 7 with value: 147.1673126220703.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:36:04,519] Trial 4 finished with value: 502.59698486328125 and parameters: {'pretrain_epochs': 0, 'lr': 1.3170990774640101e-05, 'mc_samples_train': 1, 'prior_scale': 0.10583829396911469, 'q_scale': 0.023905570899706415, 'batch_size': 128}. Best is trial 4 with value: 502.59698486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:36:25,502] Trial 10 finished with value: 96.5496597290039 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002686321798731397, 'mc_samples_train': 2, 'prior_scale': 0.01116798829495977, 'q_scale': 0.00013611901663306908, 'batch_size': 32}. Best is trial 10 with value: 96.5496597290039.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:37:10,320] Trial 5 finished with value: 666.7529296875 and parameters: {'pretrain_epochs': 0, 'lr': 1.674127903856e-05, 'mc_samples_train': 1, 'prior_scale': 0.001047833275436284, 'q_scale': 0.02949625201538112, 'batch_size': 128}. Best is trial 4 with value: 502.59698486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:37:46,385] Trial 11 finished with value: 72.9025650024414 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002668116545003139, 'mc_samples_train': 2, 'prior_scale': 0.015791568646716513, 'q_scale': 0.00013400032806480455, 'batch_size': 32}. Best is trial 11 with value: 72.9025650024414.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:38:16,064] Trial 6 finished with value: 487.36346435546875 and parameters: {'pretrain_epochs': 0, 'lr': 3.71964865643354e-05, 'mc_samples_train': 1, 'prior_scale': 0.0038638969563953266, 'q_scale': 0.0033827492632411567, 'batch_size': 128}. Best is trial 6 with value: 487.36346435546875.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:39:08,882] Trial 12 finished with value: 118.86326599121094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001741204381065012, 'mc_samples_train': 2, 'prior_scale': 0.01578701912640358, 'q_scale': 0.00010569299297450266, 'batch_size': 32}. Best is trial 11 with value: 72.9025650024414.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:39:59,566] Trial 13 finished with value: 59.91932678222656 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009998183169689775, 'mc_samples_train': 2, 'prior_scale': 0.01064012896031672, 'q_scale': 0.00010211784008112604, 'batch_size': 64}. Best is trial 13 with value: 59.91932678222656.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:40:49,361] Trial 14 finished with value: 15.913650512695312 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009813379195690634, 'mc_samples_train': 2, 'prior_scale': 0.03462087235603707, 'q_scale': 0.0004237992367412906, 'batch_size': 64}. Best is trial 14 with value: 15.913650512695312.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:41:20,069] Trial 7 finished with value: 123.37533569335938 and parameters: {'pretrain_epochs': 0, 'lr': 0.00029994809793642315, 'mc_samples_train': 1, 'prior_scale': 0.0020959477484878194, 'q_scale': 0.0008257988273685458, 'batch_size': 32}. Best is trial 7 with value: 123.37533569335938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:41:38,140] Trial 15 finished with value: 17.22635841369629 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009889182716204107, 'mc_samples_train': 2, 'prior_scale': 0.04295889441861082, 'q_scale': 0.0005750345772627247, 'batch_size': 64}. Best is trial 14 with value: 15.913650512695312.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:42:30,609] Trial 16 finished with value: -20.626298904418945 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009485885170359142, 'mc_samples_train': 2, 'prior_scale': 0.050760342213256066, 'q_scale': 0.0005478446090810961, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:43:20,426] Trial 8 finished with value: 425.2478332519531 and parameters: {'pretrain_epochs': 0, 'lr': 7.478015213333154e-05, 'mc_samples_train': 2, 'prior_scale': 0.07537199486913888, 'q_scale': 0.0666791815288874, 'batch_size': 128}. Best is trial 7 with value: 123.37533569335938.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:43:26,677] Trial 17 finished with value: 51.91472625732422 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005082091555483537, 'mc_samples_train': 2, 'prior_scale': 0.36097912828529327, 'q_scale': 0.0006411550999815748, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:44:29,978] Trial 18 finished with value: 282.3091735839844 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001350612004156431, 'mc_samples_train': 2, 'prior_scale': 0.04479669818096754, 'q_scale': 0.00162577924453938, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:44:43,444] Trial 9 finished with value: 438.2763977050781 and parameters: {'pretrain_epochs': 0, 'lr': 2.01904291186572e-05, 'mc_samples_train': 1, 'prior_scale': 0.714967484470097, 'q_scale': 0.00834519932080496, 'batch_size': 128}. Best is trial 7 with value: 123.37533569335938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:45:25,557] Trial 19 finished with value: 85.81464385986328 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005632231128261526, 'mc_samples_train': 2, 'prior_scale': 0.03718801483781791, 'q_scale': 0.0002942702531995975, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:45:50,795] Trial 20 finished with value: 593.814453125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006574375848469725, 'mc_samples_train': 2, 'prior_scale': 0.0688834918800732, 'q_scale': 0.005383502861555352, 'batch_size': 512}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:46:47,975] Trial 21 finished with value: 14.514535903930664 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009674822215525922, 'mc_samples_train': 2, 'prior_scale': 0.028447825080262935, 'q_scale': 0.0004765721941975227, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:47:37,874] Trial 22 finished with value: 203.214599609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.00038903540548520176, 'mc_samples_train': 2, 'prior_scale': 0.025097966623569633, 'q_scale': 0.00033550995264756994, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:48:29,039] Trial 23 finished with value: 22.206480026245117 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007535961514594533, 'mc_samples_train': 2, 'prior_scale': 0.026883396378969467, 'q_scale': 0.0012581973353670143, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:48:57,112] Trial 24 finished with value: 596.397216796875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003558250160840399, 'mc_samples_train': 2, 'prior_scale': 0.006209310061318424, 'q_scale': 0.0003801340996352811, 'batch_size': 256}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:49:48,686] Trial 25 finished with value: 259.7329406738281 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001918736794283924, 'mc_samples_train': 2, 'prior_scale': 0.16500300882824115, 'q_scale': 0.001767415258466624, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:50:37,901] Trial 26 finished with value: 3.471277952194214 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008219238268640003, 'mc_samples_train': 2, 'prior_scale': 0.0701219661498586, 'q_scale': 0.00022317390945526404, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:50:43,875] Trial 10 finished with value: -16.51980209350586 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002686321798731397, 'mc_samples_train': 2, 'prior_scale': 0.01116798829495977, 'q_scale': 0.00013611901663306908, 'batch_size': 32}. Best is trial 10 with value: -16.51980209350586.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:51:29,188] Trial 27 finished with value: 135.48362731933594 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004385283241074326, 'mc_samples_train': 2, 'prior_scale': 0.3101187669774712, 'q_scale': 0.00021254299046878652, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:52:20,770] Trial 28 finished with value: -21.971128463745117 and parameters: {'pretrain_epochs': 0, 'lr': 0.000724443730204724, 'mc_samples_train': 2, 'prior_scale': 0.5368203639253165, 'q_scale': 0.0009958342281810118, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:52:48,351] Trial 29 finished with value: 418.0316162109375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006881081423441657, 'mc_samples_train': 2, 'prior_scale': 0.6670023417651386, 'q_scale': 0.0008731294528918791, 'batch_size': 256}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:53:01,698] Trial 30 pruned. Trial was pruned at epoch 3.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:53:49,088] Trial 31 finished with value: 1.532729983329773 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007504380924840447, 'mc_samples_train': 2, 'prior_scale': 0.07302603787832612, 'q_scale': 0.00019768362488607863, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:54:39,291] Trial 32 finished with value: -6.535558700561523 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007487513495989997, 'mc_samples_train': 2, 'prior_scale': 0.1958565775631171, 'q_scale': 0.00020168380272309736, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:55:30,442] Trial 33 finished with value: 19.887197494506836 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006344273141515181, 'mc_samples_train': 2, 'prior_scale': 0.23774077646075986, 'q_scale': 0.002869432117032144, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:55:48,521] Trial 11 finished with value: -30.28417205810547 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002668116545003139, 'mc_samples_train': 2, 'prior_scale': 0.015791568646716513, 'q_scale': 0.00013400032806480455, 'batch_size': 32}. Best is trial 11 with value: -30.28417205810547.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:55:56,211] Trial 34 finished with value: 576.2787475585938 and parameters: {'pretrain_epochs': 0, 'lr': 0.00046322689176454547, 'mc_samples_train': 2, 'prior_scale': 0.14366689658702228, 'q_scale': 0.00020489167173149444, 'batch_size': 256}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:56:45,546] Trial 35 finished with value: 161.79745483398438 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003543464729623235, 'mc_samples_train': 2, 'prior_scale': 0.19797991285980732, 'q_scale': 0.0009701445897562239, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:57:04,326] Trial 36 finished with value: 505.42437744140625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007432127200109299, 'mc_samples_train': 1, 'prior_scale': 0.9136614367319559, 'q_scale': 0.002498921993623579, 'batch_size': 512}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:57:26,705] Trial 37 pruned. Trial was pruned at epoch 5.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:57:48,938] Trial 38 pruned. Trial was pruned at epoch 9.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:58:15,529] Trial 39 pruned. Trial was pruned at epoch 7.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:58:35,716] Trial 40 finished with value: 600.3446655273438 and parameters: {'pretrain_epochs': 0, 'lr': 0.00020652522290167825, 'mc_samples_train': 1, 'prior_scale': 0.12901340461166597, 'q_scale': 0.0001975015724074028, 'batch_size': 256}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:59:27,286] Trial 41 finished with value: 12.754474639892578 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008001623841521837, 'mc_samples_train': 2, 'prior_scale': 0.06455459535288349, 'q_scale': 0.00024170895358512567, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:00:19,797] Trial 42 finished with value: -8.537113189697266 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008067100945477406, 'mc_samples_train': 2, 'prior_scale': 0.05615254688031494, 'q_scale': 0.00018565455630260784, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:00:53,241] Trial 12 finished with value: -25.458221435546875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001741204381065012, 'mc_samples_train': 2, 'prior_scale': 0.01578701912640358, 'q_scale': 0.00010569299297450266, 'batch_size': 32}. Best is trial 11 with value: -30.28417205810547.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:01:10,902] Trial 43 finished with value: 76.92039489746094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005820610644418714, 'mc_samples_train': 2, 'prior_scale': 0.1077996955687841, 'q_scale': 0.00015351435165738284, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:02:02,349] Trial 44 finished with value: 180.99383544921875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004124831321621341, 'mc_samples_train': 2, 'prior_scale': 0.051388910072198576, 'q_scale': 0.000625714390067751, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:02:37,458] Trial 45 finished with value: 456.5339050292969 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003089199434659764, 'mc_samples_train': 2, 'prior_scale': 0.494804586529123, 'q_scale': 0.0003314861886201554, 'batch_size': 128}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:03:31,211] Trial 46 finished with value: -7.991441249847412 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008365809969780687, 'mc_samples_train': 1, 'prior_scale': 0.10598115165911277, 'q_scale': 0.00014655866571127322, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:03:42,691] Trial 13 finished with value: -22.521520614624023 and parameters: {'pretrain_epochs': 0, 'lr': 0.00015991366116446953, 'mc_samples_train': 2, 'prior_scale': 0.023551654708581973, 'q_scale': 0.00010194432109815798, 'batch_size': 64}. Best is trial 11 with value: -30.28417205810547.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:04:26,871] Trial 47 finished with value: -14.779865264892578 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004901582158569339, 'mc_samples_train': 1, 'prior_scale': 0.18434016661664662, 'q_scale': 0.00011360614395175576, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:05:21,945] Trial 48 finished with value: 20.996742248535156 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004933632794909617, 'mc_samples_train': 1, 'prior_scale': 0.020405542491552745, 'q_scale': 0.00011047000076727068, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:06:12,879] Trial 49 finished with value: 64.02423095703125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002318423262287637, 'mc_samples_train': 1, 'prior_scale': 0.11607667945247209, 'q_scale': 0.00014434003992706498, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:07:06,357] Trial 50 finished with value: -4.277852535247803 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008871351483351918, 'mc_samples_train': 1, 'prior_scale': 0.16041163498374023, 'q_scale': 0.0011172652931689133, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:07:58,630] Trial 51 finished with value: -19.771440505981445 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006650259307840426, 'mc_samples_train': 1, 'prior_scale': 0.20613605240220698, 'q_scale': 0.00010186339574177252, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:08:48,241] Trial 52 finished with value: -9.304269790649414 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006123224953559687, 'mc_samples_train': 1, 'prior_scale': 0.09503694997607778, 'q_scale': 0.00010626733881971644, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:08:48,835] Trial 14 finished with value: -30.421886444091797 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009801615183585152, 'mc_samples_train': 2, 'prior_scale': 0.03777462213368495, 'q_scale': 0.00042903850239015034, 'batch_size': 32}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:09:37,724] Trial 53 finished with value: -0.9196409583091736 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005770641519746715, 'mc_samples_train': 1, 'prior_scale': 0.054916448893760926, 'q_scale': 0.0004395781220958909, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:10:27,872] Trial 54 finished with value: 11.142358779907227 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003097749945698196, 'mc_samples_train': 1, 'prior_scale': 0.3063189159865552, 'q_scale': 0.00028785875540367, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:11:16,956] Trial 55 finished with value: 116.85144805908203 and parameters: {'pretrain_epochs': 0, 'lr': 0.00015253671276923227, 'mc_samples_train': 1, 'prior_scale': 0.09415310255835511, 'q_scale': 0.00010622638533885774, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:12:06,268] Trial 56 finished with value: -15.905345916748047 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006234503695443075, 'mc_samples_train': 1, 'prior_scale': 0.5746791270552903, 'q_scale': 0.0007515326369669672, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:12:54,072] Trial 57 finished with value: -16.40332794189453 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006331440189537419, 'mc_samples_train': 1, 'prior_scale': 0.6055709728691586, 'q_scale': 0.0007833907890241141, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:13:46,412] Trial 58 finished with value: 14.212347030639648 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004713186781202526, 'mc_samples_train': 1, 'prior_scale': 0.8397504651877787, 'q_scale': 0.0016519937294930918, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:13:50,120] Trial 15 finished with value: -17.7005672454834 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009933433836908847, 'mc_samples_train': 2, 'prior_scale': 0.04295889441861082, 'q_scale': 0.0005371630331658818, 'batch_size': 32}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:14:38,224] Trial 59 finished with value: 131.20361328125 and parameters: {'pretrain_epochs': 0, 'lr': 5.518736571418839e-05, 'mc_samples_train': 1, 'prior_scale': 0.5893824062185491, 'q_scale': 0.004160694286668926, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:18:54,729] Trial 16 finished with value: 56.03738784790039 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009418616963254316, 'mc_samples_train': 2, 'prior_scale': 0.0057475489968997415, 'q_scale': 0.0005478446090810961, 'batch_size': 32}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:20:04,889] Trial 17 finished with value: 254.12265014648438 and parameters: {'pretrain_epochs': 0, 'lr': 0.00048726994464372547, 'mc_samples_train': 2, 'prior_scale': 0.03298570457662521, 'q_scale': 0.0013121308489553924, 'batch_size': 256}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:21:02,433] Trial 18 finished with value: 1979.5206298828125 and parameters: {'pretrain_epochs': 0, 'lr': 0.00048212758884668886, 'mc_samples_train': 2, 'prior_scale': 0.27634381419905757, 'q_scale': 0.0002885851817360491, 'batch_size': 512}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:23:49,966] Trial 19 finished with value: 65.56961059570312 and parameters: {'pretrain_epochs': 0, 'lr': 0.00012544540249913088, 'mc_samples_train': 2, 'prior_scale': 0.007133078560557303, 'q_scale': 0.001932611973562815, 'batch_size': 64}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:28:51,520] Trial 20 finished with value: -35.62898254394531 and parameters: {'pretrain_epochs': 0, 'lr': 0.00032043021391109096, 'mc_samples_train': 2, 'prior_scale': 0.05538828055465327, 'q_scale': 0.00024331562128032956, 'batch_size': 32}. Best is trial 20 with value: -35.62898254394531.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:34:08,325] Trial 21 finished with value: -23.364036560058594 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002844592019038099, 'mc_samples_train': 2, 'prior_scale': 0.060077948867644476, 'q_scale': 0.0002925940795318177, 'batch_size': 32}. Best is trial 20 with value: -35.62898254394531.
[W 2025-02-24 17:38:28,066] Trial 22 failed with parameters: {'pretrain_epochs': 0, 'lr': 0.0005377740280800115, 'mc_samples_train': 2, 'prior_scale': 0.025721840905250332, 'q_scale': 0.0002788619313409219, 'batch_size': 32} because of the following error: FileNotFoundError(2, 'No such file or directory').
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
OSError(5, 'Input/output error')
full_key: datamodule

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 93, in <lambda>
    lambda trial: objective(trial, cfg, output_dir),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 73, in objective_bnn
    return objective(trial, cfg, output_dir)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 32, in objective
    metric_dict, _ = train(cfg, trial)
                     ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 48, in wrap
    save_file(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 93, in save_file
    with open(path, "w+") as file:
         ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/g15farris/bin/bayesaenet/bnn_aenet/logs/lrt_hps_100perc/runs/2025-02-24_16-30-10/022/exec_time.log'
[W 2025-02-24 17:38:28,145] Trial 22 failed with value None.
Error executing job with overrides: ['model=bnn_lrt', 'datamodule=TiO', 'hpsearch=bnn_lrt', 'task_name=lrt_hps_100perc', 'tags=[TiO]', 'datamodule.valid_split=100', 'hpsearch.study.study_name=lrt_100perc']
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
OSError(5, 'Input/output error')
full_key: datamodule

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 92, in main
    study.optimize(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/study.py", line 451, in optimize
    _optimize(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 62, in _optimize
    _optimize_sequential(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 159, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 247, in _run_trial
    raise func_err
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 93, in <lambda>
    lambda trial: objective(trial, cfg, output_dir),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 73, in objective_bnn
    return objective(trial, cfg, output_dir)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 32, in objective
    metric_dict, _ = train(cfg, trial)
                     ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 48, in wrap
    save_file(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 93, in save_file
    with open(path, "w+") as file:
         ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/g15farris/bin/bayesaenet/bnn_aenet/logs/lrt_hps_100perc/runs/2025-02-24_16-30-10/022/exec_time.log'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-02-25 10:43:51,160] Using an existing study with name 'lrt_100perc' instead of creating a new one.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-02-25 11:00:42,846] Using an existing study with name 'lrt_100perc' instead of creating a new one.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:05:54,932] Trial 24 finished with value: -44.99856185913086 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006509790984463777, 'mc_samples_train': 2, 'prior_scale': 0.04658057864895187, 'q_scale': 0.0002632382905467797, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:10:21,556] Trial 25 finished with value: -36.278560638427734 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006829298328061672, 'mc_samples_train': 2, 'prior_scale': 0.03324722460692267, 'q_scale': 0.00034558961381588423, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:14:47,120] Trial 26 finished with value: -15.924846649169922 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006238411395857852, 'mc_samples_train': 2, 'prior_scale': 0.18847537475897708, 'q_scale': 0.0059812201173843095, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:19:11,762] Trial 27 finished with value: -28.120494842529297 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006983210809334892, 'mc_samples_train': 2, 'prior_scale': 0.07072483341252464, 'q_scale': 0.0011472396045626744, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:23:39,119] Trial 28 finished with value: -35.11814880371094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003728086209673569, 'mc_samples_train': 2, 'prior_scale': 0.4102783837375459, 'q_scale': 0.0003379845080952342, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:26:06,557] Trial 29 finished with value: -31.6918888092041 and parameters: {'pretrain_epochs': 0, 'lr': 0.00020597547065149416, 'mc_samples_train': 2, 'prior_scale': 0.021794603752404315, 'q_scale': 0.00019803364724850774, 'batch_size': 64}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:26:57,178] Trial 30 finished with value: 2440.457275390625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00040552576010057743, 'mc_samples_train': 2, 'prior_scale': 0.11011957907017718, 'q_scale': 0.0006899810583390177, 'batch_size': 512}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[I 2025-02-25 11:27:28,461] Trial 31 pruned. Trial was pruned at epoch 7.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:28:31,225] Trial 32 finished with value: 923.0511474609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.00010682995798866446, 'mc_samples_train': 2, 'prior_scale': 0.025831207105731828, 'q_scale': 0.002352154084601003, 'batch_size': 256}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:33:01,664] Trial 33 finished with value: -36.36891555786133 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003684161850187871, 'mc_samples_train': 2, 'prior_scale': 0.37378992749314094, 'q_scale': 0.0003212726526118929, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:37:28,168] Trial 34 finished with value: -20.723482131958008 and parameters: {'pretrain_epochs': 0, 'lr': 0.00037904252978057163, 'mc_samples_train': 2, 'prior_scale': 0.8631445024831159, 'q_scale': 0.0002258825207351197, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:41:53,178] Trial 35 finished with value: -36.47572326660156 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007245914220860388, 'mc_samples_train': 2, 'prior_scale': 0.3805723370060441, 'q_scale': 0.0009485522113989473, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:46:18,745] Trial 36 finished with value: -46.62199401855469 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007286170843324783, 'mc_samples_train': 2, 'prior_scale': 0.2974764122719904, 'q_scale': 0.0011561199955338337, 'batch_size': 32}. Best is trial 36 with value: -46.62199401855469.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:47:09,083] Trial 37 finished with value: 2178.370849609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005243619617447778, 'mc_samples_train': 2, 'prior_scale': 0.4928283400121777, 'q_scale': 0.00117112249392426, 'batch_size': 512}. Best is trial 36 with value: -46.62199401855469.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:203: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  agg_scale = precision.reciprocal().mean(dim).add(loc.var(dim)).sqrt()
[I 2025-02-25 11:48:17,939] Trial 38 pruned. Trial was pruned at epoch 7.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:52:45,591] Trial 39 finished with value: -2.344482183456421 and parameters: {'pretrain_epochs': 0, 'lr': 0.000560203106547756, 'mc_samples_train': 2, 'prior_scale': 0.3660045282784296, 'q_scale': 0.005211051528825213, 'batch_size': 32}. Best is trial 36 with value: -46.62199401855469.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-25 11:54:16,839] Trial 40 pruned. Trial was pruned at epoch 10.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:56:46,768] Trial 41 finished with value: 253.29925537109375 and parameters: {'pretrain_epochs': 0, 'lr': 1.0294620903386365e-05, 'mc_samples_train': 2, 'prior_scale': 0.5903418491155675, 'q_scale': 0.0035117890454306813, 'batch_size': 64}. Best is trial 36 with value: -46.62199401855469.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:57:28,261] Trial 42 finished with value: 1006.789306640625 and parameters: {'pretrain_epochs': 0, 'lr': 3.075611820416461e-05, 'mc_samples_train': 1, 'prior_scale': 0.1243042312330324, 'q_scale': 0.0008839908430144221, 'batch_size': 256}. Best is trial 36 with value: -46.62199401855469.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:01:56,134] Trial 43 finished with value: -47.81182861328125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007615317917538199, 'mc_samples_train': 2, 'prior_scale': 0.2997290524017179, 'q_scale': 0.0004382195401082087, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:06:25,217] Trial 44 finished with value: -35.23133087158203 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007940523052301803, 'mc_samples_train': 2, 'prior_scale': 0.29819852329246094, 'q_scale': 0.0018337796577275539, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:10:53,225] Trial 45 finished with value: -12.5316162109375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004346673644455997, 'mc_samples_train': 2, 'prior_scale': 0.9744952982848707, 'q_scale': 0.00017000949050492947, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:15:23,015] Trial 46 finished with value: -35.87584686279297 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005547915996773387, 'mc_samples_train': 2, 'prior_scale': 0.6039419025768079, 'q_scale': 0.0006966133059950005, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:16:54,465] Trial 47 finished with value: 441.498046875 and parameters: {'pretrain_epochs': 0, 'lr': 5.1375879420384186e-05, 'mc_samples_train': 2, 'prior_scale': 0.3621742694732916, 'q_scale': 0.0004621627756722631, 'batch_size': 128}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:19:35,445] Trial 48 finished with value: -22.34197425842285 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008384052381899798, 'mc_samples_train': 1, 'prior_scale': 0.08542144783764337, 'q_scale': 0.0016146632654310278, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:24:04,212] Trial 49 finished with value: -46.978965759277344 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006330373057854226, 'mc_samples_train': 2, 'prior_scale': 0.18036732081542833, 'q_scale': 0.000887761860037635, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:28:34,916] Trial 50 finished with value: -33.61904525756836 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006290102057144817, 'mc_samples_train': 2, 'prior_scale': 0.193010925093162, 'q_scale': 0.0029634572562788025, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:29:26,662] Trial 51 finished with value: 1457.940185546875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008769255259426099, 'mc_samples_train': 2, 'prior_scale': 0.08932033956168513, 'q_scale': 0.021640621136322986, 'batch_size': 512}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:30:57,701] Trial 52 finished with value: 422.7819519042969 and parameters: {'pretrain_epochs': 0, 'lr': 6.585553704010568e-05, 'mc_samples_train': 2, 'prior_scale': 0.1552775640762928, 'q_scale': 0.0009611441399406295, 'batch_size': 128}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:35:30,566] Trial 53 finished with value: -40.28005599975586 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006149970689040431, 'mc_samples_train': 2, 'prior_scale': 0.26512852618136656, 'q_scale': 0.000577010115338762, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:39:59,622] Trial 54 finished with value: -49.54961013793945 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005881574450522152, 'mc_samples_train': 2, 'prior_scale': 0.2591948435707256, 'q_scale': 0.000636963617578096, 'batch_size': 32}. Best is trial 54 with value: -49.54961013793945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:44:29,638] Trial 55 finished with value: -47.559486389160156 and parameters: {'pretrain_epochs': 0, 'lr': 0.00046920795808497797, 'mc_samples_train': 2, 'prior_scale': 0.24889977469572572, 'q_scale': 0.0006548994792766683, 'batch_size': 32}. Best is trial 54 with value: -49.54961013793945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:48:59,293] Trial 56 finished with value: -44.15993881225586 and parameters: {'pretrain_epochs': 0, 'lr': 0.000479176835878153, 'mc_samples_train': 2, 'prior_scale': 0.2164729112422234, 'q_scale': 0.00016255787458547795, 'batch_size': 32}. Best is trial 54 with value: -49.54961013793945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-25 12:50:16,098] Trial 57 pruned. Trial was pruned at epoch 4.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:54:49,118] Trial 58 finished with value: -42.6210823059082 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009953935228166127, 'mc_samples_train': 2, 'prior_scale': 0.5217392174673524, 'q_scale': 0.0005408946508854562, 'batch_size': 32}. Best is trial 54 with value: -49.54961013793945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:57:37,245] Trial 59 finished with value: 129.19342041015625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00033056606661663573, 'mc_samples_train': 1, 'prior_scale': 0.0014818536151051014, 'q_scale': 0.00037168343417977735, 'batch_size': 32}. Best is trial 54 with value: -49.54961013793945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:00:11,964] Trial 60 finished with value: -51.95450210571289 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002482679089394269, 'mc_samples_train': 2, 'prior_scale': 0.16140767274576168, 'q_scale': 0.0014998028529645947, 'batch_size': 64}. Best is trial 60 with value: -51.95450210571289.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:02:45,197] Trial 61 finished with value: -35.48674011230469 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002516412831975507, 'mc_samples_train': 2, 'prior_scale': 0.2904095841242468, 'q_scale': 0.0014341769538948682, 'batch_size': 64}. Best is trial 60 with value: -51.95450210571289.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:05:18,881] Trial 62 finished with value: -10.74655818939209 and parameters: {'pretrain_epochs': 0, 'lr': 0.00014612039095042477, 'mc_samples_train': 2, 'prior_scale': 0.17364561913386883, 'q_scale': 0.002484450015659679, 'batch_size': 64}. Best is trial 60 with value: -51.95450210571289.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:07:52,516] Trial 63 finished with value: -90.33507537841797 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006112237319911698, 'mc_samples_train': 2, 'prior_scale': 0.22656050867842187, 'q_scale': 0.000444594528183239, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:10:25,686] Trial 64 finished with value: -69.40507507324219 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005329084989890526, 'mc_samples_train': 2, 'prior_scale': 0.2212667333874839, 'q_scale': 0.000725736714091057, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:12:56,361] Trial 65 finished with value: -85.56182098388672 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005617342933955713, 'mc_samples_train': 2, 'prior_scale': 0.1446985960420336, 'q_scale': 0.0007371684349194762, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:15:26,872] Trial 66 finished with value: -57.858943939208984 and parameters: {'pretrain_epochs': 0, 'lr': 0.00031266853636797304, 'mc_samples_train': 2, 'prior_scale': 0.10319184201525437, 'q_scale': 0.0007439341763389912, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:17:57,293] Trial 67 finished with value: -67.51888275146484 and parameters: {'pretrain_epochs': 0, 'lr': 0.00032233941289796955, 'mc_samples_train': 2, 'prior_scale': 0.10327493848803794, 'q_scale': 0.00045368614665782187, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:20:30,070] Trial 68 finished with value: -60.81745910644531 and parameters: {'pretrain_epochs': 0, 'lr': 0.00023116626974829078, 'mc_samples_train': 2, 'prior_scale': 0.10684746225783721, 'q_scale': 0.0007301264915899488, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:23:05,120] Trial 69 finished with value: -48.662010192871094 and parameters: {'pretrain_epochs': 0, 'lr': 0.00020338036945597227, 'mc_samples_train': 2, 'prior_scale': 0.06696831950609647, 'q_scale': 0.00041367237001281213, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-25 13:24:39,059] Trial 70 pruned. Trial was pruned at epoch 11.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:27:10,796] Trial 71 finished with value: -65.20162200927734 and parameters: {'pretrain_epochs': 0, 'lr': 0.00025636487944607095, 'mc_samples_train': 2, 'prior_scale': 0.12778168109400143, 'q_scale': 0.00012256660654409398, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:29:40,748] Trial 72 finished with value: -65.58686065673828 and parameters: {'pretrain_epochs': 0, 'lr': 0.00034253305734575966, 'mc_samples_train': 2, 'prior_scale': 0.11057058360008695, 'q_scale': 0.00013526611362317908, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:32:19,329] Trial 73 finished with value: -40.36310577392578 and parameters: {'pretrain_epochs': 0, 'lr': 0.00017999357876446857, 'mc_samples_train': 2, 'prior_scale': 0.12011638078136157, 'q_scale': 0.00011881019455953559, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:35:23,891] Trial 74 finished with value: -71.7834701538086 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003371727945055252, 'mc_samples_train': 2, 'prior_scale': 0.0813305980279409, 'q_scale': 0.00020285551884818566, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:38:21,610] Trial 75 finished with value: -52.69337463378906 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003982713597966943, 'mc_samples_train': 2, 'prior_scale': 0.0690103031092565, 'q_scale': 0.00020159662491373343, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:41:18,454] Trial 76 finished with value: -73.2134017944336 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003492419081791508, 'mc_samples_train': 2, 'prior_scale': 0.1419650934730244, 'q_scale': 0.00013555260976025975, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:44:16,011] Trial 77 finished with value: -74.15149688720703 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003427187858186089, 'mc_samples_train': 2, 'prior_scale': 0.14147699282096687, 'q_scale': 0.0001349612689423686, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:47:17,415] Trial 78 finished with value: -53.30698013305664 and parameters: {'pretrain_epochs': 0, 'lr': 0.00035515811832816205, 'mc_samples_train': 2, 'prior_scale': 0.08298310373108063, 'q_scale': 0.00017007457743319746, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:49:17,343] Trial 79 finished with value: -67.42692565917969 and parameters: {'pretrain_epochs': 0, 'lr': 0.000290829664453999, 'mc_samples_train': 1, 'prior_scale': 0.14076790324402377, 'q_scale': 0.000258743398460186, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:51:05,416] Trial 80 finished with value: -61.04832458496094 and parameters: {'pretrain_epochs': 0, 'lr': 0.00041434390682968005, 'mc_samples_train': 1, 'prior_scale': 0.1427423132374321, 'q_scale': 0.0002489919576897951, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:52:54,841] Trial 81 finished with value: -64.0602798461914 and parameters: {'pretrain_epochs': 0, 'lr': 0.00028049313586589366, 'mc_samples_train': 1, 'prior_scale': 0.042923249866200816, 'q_scale': 0.0002857122359359686, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:54:44,008] Trial 82 finished with value: -87.05892944335938 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005343251405187812, 'mc_samples_train': 1, 'prior_scale': 0.21251938015643568, 'q_scale': 0.00021907339916724436, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:56:30,498] Trial 83 finished with value: -81.32711791992188 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004733634912280229, 'mc_samples_train': 1, 'prior_scale': 0.20910268495933063, 'q_scale': 0.0002134022862276677, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
