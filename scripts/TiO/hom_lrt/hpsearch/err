/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-02-24 16:30:18,275] A new study created in RDB with name: lrt_100perc
[I 2025-02-24 16:30:18,281] A new study created in RDB with name: lrt_20perc
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:203: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  agg_scale = precision.reciprocal().mean(dim).add(loc.var(dim)).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:203: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  agg_scale = precision.reciprocal().mean(dim).add(loc.var(dim)).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:30:46,123] Trial 0 finished with value: 660.906005859375 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 0.22702608040701588, 'q_scale': 0.13179630432958683, 'batch_size': 256}. Best is trial 0 with value: 660.906005859375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:09,317] Trial 1 finished with value: 617.3781127929688 and parameters: {'pretrain_epochs': 0, 'lr': 5.195586024325663e-05, 'mc_samples_train': 2, 'prior_scale': 0.1374377640063412, 'q_scale': 0.0030269344709026166, 'batch_size': 512}. Best is trial 1 with value: 617.3781127929688.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:10,702] Trial 0 finished with value: 986.4046630859375 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 0.22702608040701588, 'q_scale': 0.13179630432958683, 'batch_size': 256}. Best is trial 0 with value: 986.4046630859375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:31:34,122] Trial 2 finished with value: 4258.849609375 and parameters: {'pretrain_epochs': 0, 'lr': 5.367498945698204e-05, 'mc_samples_train': 1, 'prior_scale': 0.012778268753495938, 'q_scale': 0.540207249341968, 'batch_size': 128}. Best is trial 1 with value: 617.3781127929688.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:32:08,332] Trial 3 finished with value: 1070.847900390625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005473369642905611, 'mc_samples_train': 2, 'prior_scale': 0.002699606552195955, 'q_scale': 0.06562115716452717, 'batch_size': 128}. Best is trial 1 with value: 617.3781127929688.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:32:09,351] Trial 1 finished with value: 3103.0 and parameters: {'pretrain_epochs': 0, 'lr': 5.195586024325663e-05, 'mc_samples_train': 2, 'prior_scale': 0.1374377640063412, 'q_scale': 0.0030269344709026166, 'batch_size': 512}. Best is trial 0 with value: 986.4046630859375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:32:33,269] Trial 4 finished with value: 612.0908203125 and parameters: {'pretrain_epochs': 0, 'lr': 1.3170990774640101e-05, 'mc_samples_train': 1, 'prior_scale': 0.10583829396911469, 'q_scale': 0.023905570899706415, 'batch_size': 128}. Best is trial 4 with value: 612.0908203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:32:46,535] Trial 5 pruned. Trial was pruned at epoch 3.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:33:13,373] Trial 6 finished with value: 606.154052734375 and parameters: {'pretrain_epochs': 0, 'lr': 3.71964865643354e-05, 'mc_samples_train': 1, 'prior_scale': 0.0038638969563953266, 'q_scale': 0.0033827492632411567, 'batch_size': 128}. Best is trial 6 with value: 606.154052734375.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:33:14,715] Trial 2 finished with value: 1059.150390625 and parameters: {'pretrain_epochs': 0, 'lr': 5.367498945698204e-05, 'mc_samples_train': 1, 'prior_scale': 0.012778268753495938, 'q_scale': 0.540207249341968, 'batch_size': 128}. Best is trial 0 with value: 986.4046630859375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:34:07,100] Trial 7 finished with value: 147.1673126220703 and parameters: {'pretrain_epochs': 0, 'lr': 0.00029994809793642315, 'mc_samples_train': 1, 'prior_scale': 0.0020959477484878194, 'q_scale': 0.0008257988273685458, 'batch_size': 32}. Best is trial 7 with value: 147.1673126220703.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:34:39,949] Trial 8 finished with value: 597.3457641601562 and parameters: {'pretrain_epochs': 0, 'lr': 7.478015213333154e-05, 'mc_samples_train': 2, 'prior_scale': 0.07537199486913888, 'q_scale': 0.0666791815288874, 'batch_size': 128}. Best is trial 7 with value: 147.1673126220703.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:34:58,443] Trial 3 finished with value: 665.6839599609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005473369642905611, 'mc_samples_train': 2, 'prior_scale': 0.002699606552195955, 'q_scale': 0.06562115716452717, 'batch_size': 128}. Best is trial 3 with value: 665.6839599609375.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:35:03,547] Trial 9 finished with value: 561.09765625 and parameters: {'pretrain_epochs': 0, 'lr': 2.01904291186572e-05, 'mc_samples_train': 1, 'prior_scale': 0.714967484470097, 'q_scale': 0.00834519932080496, 'batch_size': 128}. Best is trial 7 with value: 147.1673126220703.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:36:04,519] Trial 4 finished with value: 502.59698486328125 and parameters: {'pretrain_epochs': 0, 'lr': 1.3170990774640101e-05, 'mc_samples_train': 1, 'prior_scale': 0.10583829396911469, 'q_scale': 0.023905570899706415, 'batch_size': 128}. Best is trial 4 with value: 502.59698486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:36:25,502] Trial 10 finished with value: 96.5496597290039 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002686321798731397, 'mc_samples_train': 2, 'prior_scale': 0.01116798829495977, 'q_scale': 0.00013611901663306908, 'batch_size': 32}. Best is trial 10 with value: 96.5496597290039.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:37:10,320] Trial 5 finished with value: 666.7529296875 and parameters: {'pretrain_epochs': 0, 'lr': 1.674127903856e-05, 'mc_samples_train': 1, 'prior_scale': 0.001047833275436284, 'q_scale': 0.02949625201538112, 'batch_size': 128}. Best is trial 4 with value: 502.59698486328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:37:46,385] Trial 11 finished with value: 72.9025650024414 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002668116545003139, 'mc_samples_train': 2, 'prior_scale': 0.015791568646716513, 'q_scale': 0.00013400032806480455, 'batch_size': 32}. Best is trial 11 with value: 72.9025650024414.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:38:16,064] Trial 6 finished with value: 487.36346435546875 and parameters: {'pretrain_epochs': 0, 'lr': 3.71964865643354e-05, 'mc_samples_train': 1, 'prior_scale': 0.0038638969563953266, 'q_scale': 0.0033827492632411567, 'batch_size': 128}. Best is trial 6 with value: 487.36346435546875.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:39:08,882] Trial 12 finished with value: 118.86326599121094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001741204381065012, 'mc_samples_train': 2, 'prior_scale': 0.01578701912640358, 'q_scale': 0.00010569299297450266, 'batch_size': 32}. Best is trial 11 with value: 72.9025650024414.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:39:59,566] Trial 13 finished with value: 59.91932678222656 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009998183169689775, 'mc_samples_train': 2, 'prior_scale': 0.01064012896031672, 'q_scale': 0.00010211784008112604, 'batch_size': 64}. Best is trial 13 with value: 59.91932678222656.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:40:49,361] Trial 14 finished with value: 15.913650512695312 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009813379195690634, 'mc_samples_train': 2, 'prior_scale': 0.03462087235603707, 'q_scale': 0.0004237992367412906, 'batch_size': 64}. Best is trial 14 with value: 15.913650512695312.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:41:20,069] Trial 7 finished with value: 123.37533569335938 and parameters: {'pretrain_epochs': 0, 'lr': 0.00029994809793642315, 'mc_samples_train': 1, 'prior_scale': 0.0020959477484878194, 'q_scale': 0.0008257988273685458, 'batch_size': 32}. Best is trial 7 with value: 123.37533569335938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:41:38,140] Trial 15 finished with value: 17.22635841369629 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009889182716204107, 'mc_samples_train': 2, 'prior_scale': 0.04295889441861082, 'q_scale': 0.0005750345772627247, 'batch_size': 64}. Best is trial 14 with value: 15.913650512695312.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:42:30,609] Trial 16 finished with value: -20.626298904418945 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009485885170359142, 'mc_samples_train': 2, 'prior_scale': 0.050760342213256066, 'q_scale': 0.0005478446090810961, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:43:20,426] Trial 8 finished with value: 425.2478332519531 and parameters: {'pretrain_epochs': 0, 'lr': 7.478015213333154e-05, 'mc_samples_train': 2, 'prior_scale': 0.07537199486913888, 'q_scale': 0.0666791815288874, 'batch_size': 128}. Best is trial 7 with value: 123.37533569335938.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:43:26,677] Trial 17 finished with value: 51.91472625732422 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005082091555483537, 'mc_samples_train': 2, 'prior_scale': 0.36097912828529327, 'q_scale': 0.0006411550999815748, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:44:29,978] Trial 18 finished with value: 282.3091735839844 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001350612004156431, 'mc_samples_train': 2, 'prior_scale': 0.04479669818096754, 'q_scale': 0.00162577924453938, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:44:43,444] Trial 9 finished with value: 438.2763977050781 and parameters: {'pretrain_epochs': 0, 'lr': 2.01904291186572e-05, 'mc_samples_train': 1, 'prior_scale': 0.714967484470097, 'q_scale': 0.00834519932080496, 'batch_size': 128}. Best is trial 7 with value: 123.37533569335938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:45:25,557] Trial 19 finished with value: 85.81464385986328 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005632231128261526, 'mc_samples_train': 2, 'prior_scale': 0.03718801483781791, 'q_scale': 0.0002942702531995975, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:45:50,795] Trial 20 finished with value: 593.814453125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006574375848469725, 'mc_samples_train': 2, 'prior_scale': 0.0688834918800732, 'q_scale': 0.005383502861555352, 'batch_size': 512}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:46:47,975] Trial 21 finished with value: 14.514535903930664 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009674822215525922, 'mc_samples_train': 2, 'prior_scale': 0.028447825080262935, 'q_scale': 0.0004765721941975227, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:47:37,874] Trial 22 finished with value: 203.214599609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.00038903540548520176, 'mc_samples_train': 2, 'prior_scale': 0.025097966623569633, 'q_scale': 0.00033550995264756994, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:48:29,039] Trial 23 finished with value: 22.206480026245117 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007535961514594533, 'mc_samples_train': 2, 'prior_scale': 0.026883396378969467, 'q_scale': 0.0012581973353670143, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:48:57,112] Trial 24 finished with value: 596.397216796875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003558250160840399, 'mc_samples_train': 2, 'prior_scale': 0.006209310061318424, 'q_scale': 0.0003801340996352811, 'batch_size': 256}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:49:48,686] Trial 25 finished with value: 259.7329406738281 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001918736794283924, 'mc_samples_train': 2, 'prior_scale': 0.16500300882824115, 'q_scale': 0.001767415258466624, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:50:37,901] Trial 26 finished with value: 3.471277952194214 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008219238268640003, 'mc_samples_train': 2, 'prior_scale': 0.0701219661498586, 'q_scale': 0.00022317390945526404, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:50:43,875] Trial 10 finished with value: -16.51980209350586 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002686321798731397, 'mc_samples_train': 2, 'prior_scale': 0.01116798829495977, 'q_scale': 0.00013611901663306908, 'batch_size': 32}. Best is trial 10 with value: -16.51980209350586.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:51:29,188] Trial 27 finished with value: 135.48362731933594 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004385283241074326, 'mc_samples_train': 2, 'prior_scale': 0.3101187669774712, 'q_scale': 0.00021254299046878652, 'batch_size': 64}. Best is trial 16 with value: -20.626298904418945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:52:20,770] Trial 28 finished with value: -21.971128463745117 and parameters: {'pretrain_epochs': 0, 'lr': 0.000724443730204724, 'mc_samples_train': 2, 'prior_scale': 0.5368203639253165, 'q_scale': 0.0009958342281810118, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:52:48,351] Trial 29 finished with value: 418.0316162109375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006881081423441657, 'mc_samples_train': 2, 'prior_scale': 0.6670023417651386, 'q_scale': 0.0008731294528918791, 'batch_size': 256}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:53:01,698] Trial 30 pruned. Trial was pruned at epoch 3.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:53:49,088] Trial 31 finished with value: 1.532729983329773 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007504380924840447, 'mc_samples_train': 2, 'prior_scale': 0.07302603787832612, 'q_scale': 0.00019768362488607863, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:54:39,291] Trial 32 finished with value: -6.535558700561523 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007487513495989997, 'mc_samples_train': 2, 'prior_scale': 0.1958565775631171, 'q_scale': 0.00020168380272309736, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:55:30,442] Trial 33 finished with value: 19.887197494506836 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006344273141515181, 'mc_samples_train': 2, 'prior_scale': 0.23774077646075986, 'q_scale': 0.002869432117032144, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:55:48,521] Trial 11 finished with value: -30.28417205810547 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002668116545003139, 'mc_samples_train': 2, 'prior_scale': 0.015791568646716513, 'q_scale': 0.00013400032806480455, 'batch_size': 32}. Best is trial 11 with value: -30.28417205810547.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:55:56,211] Trial 34 finished with value: 576.2787475585938 and parameters: {'pretrain_epochs': 0, 'lr': 0.00046322689176454547, 'mc_samples_train': 2, 'prior_scale': 0.14366689658702228, 'q_scale': 0.00020489167173149444, 'batch_size': 256}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:56:45,546] Trial 35 finished with value: 161.79745483398438 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003543464729623235, 'mc_samples_train': 2, 'prior_scale': 0.19797991285980732, 'q_scale': 0.0009701445897562239, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:57:04,326] Trial 36 finished with value: 505.42437744140625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007432127200109299, 'mc_samples_train': 1, 'prior_scale': 0.9136614367319559, 'q_scale': 0.002498921993623579, 'batch_size': 512}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:57:26,705] Trial 37 pruned. Trial was pruned at epoch 5.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:57:48,938] Trial 38 pruned. Trial was pruned at epoch 9.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-24 16:58:15,529] Trial 39 pruned. Trial was pruned at epoch 7.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:58:35,716] Trial 40 finished with value: 600.3446655273438 and parameters: {'pretrain_epochs': 0, 'lr': 0.00020652522290167825, 'mc_samples_train': 1, 'prior_scale': 0.12901340461166597, 'q_scale': 0.0001975015724074028, 'batch_size': 256}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 16:59:27,286] Trial 41 finished with value: 12.754474639892578 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008001623841521837, 'mc_samples_train': 2, 'prior_scale': 0.06455459535288349, 'q_scale': 0.00024170895358512567, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:00:19,797] Trial 42 finished with value: -8.537113189697266 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008067100945477406, 'mc_samples_train': 2, 'prior_scale': 0.05615254688031494, 'q_scale': 0.00018565455630260784, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:00:53,241] Trial 12 finished with value: -25.458221435546875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001741204381065012, 'mc_samples_train': 2, 'prior_scale': 0.01578701912640358, 'q_scale': 0.00010569299297450266, 'batch_size': 32}. Best is trial 11 with value: -30.28417205810547.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:01:10,902] Trial 43 finished with value: 76.92039489746094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005820610644418714, 'mc_samples_train': 2, 'prior_scale': 0.1077996955687841, 'q_scale': 0.00015351435165738284, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:02:02,349] Trial 44 finished with value: 180.99383544921875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004124831321621341, 'mc_samples_train': 2, 'prior_scale': 0.051388910072198576, 'q_scale': 0.000625714390067751, 'batch_size': 64}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:02:37,458] Trial 45 finished with value: 456.5339050292969 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003089199434659764, 'mc_samples_train': 2, 'prior_scale': 0.494804586529123, 'q_scale': 0.0003314861886201554, 'batch_size': 128}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:03:31,211] Trial 46 finished with value: -7.991441249847412 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008365809969780687, 'mc_samples_train': 1, 'prior_scale': 0.10598115165911277, 'q_scale': 0.00014655866571127322, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:03:42,691] Trial 13 finished with value: -22.521520614624023 and parameters: {'pretrain_epochs': 0, 'lr': 0.00015991366116446953, 'mc_samples_train': 2, 'prior_scale': 0.023551654708581973, 'q_scale': 0.00010194432109815798, 'batch_size': 64}. Best is trial 11 with value: -30.28417205810547.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:04:26,871] Trial 47 finished with value: -14.779865264892578 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004901582158569339, 'mc_samples_train': 1, 'prior_scale': 0.18434016661664662, 'q_scale': 0.00011360614395175576, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:05:21,945] Trial 48 finished with value: 20.996742248535156 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004933632794909617, 'mc_samples_train': 1, 'prior_scale': 0.020405542491552745, 'q_scale': 0.00011047000076727068, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:06:12,879] Trial 49 finished with value: 64.02423095703125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002318423262287637, 'mc_samples_train': 1, 'prior_scale': 0.11607667945247209, 'q_scale': 0.00014434003992706498, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:07:06,357] Trial 50 finished with value: -4.277852535247803 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008871351483351918, 'mc_samples_train': 1, 'prior_scale': 0.16041163498374023, 'q_scale': 0.0011172652931689133, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:07:58,630] Trial 51 finished with value: -19.771440505981445 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006650259307840426, 'mc_samples_train': 1, 'prior_scale': 0.20613605240220698, 'q_scale': 0.00010186339574177252, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:08:48,241] Trial 52 finished with value: -9.304269790649414 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006123224953559687, 'mc_samples_train': 1, 'prior_scale': 0.09503694997607778, 'q_scale': 0.00010626733881971644, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:08:48,835] Trial 14 finished with value: -30.421886444091797 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009801615183585152, 'mc_samples_train': 2, 'prior_scale': 0.03777462213368495, 'q_scale': 0.00042903850239015034, 'batch_size': 32}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:09:37,724] Trial 53 finished with value: -0.9196409583091736 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005770641519746715, 'mc_samples_train': 1, 'prior_scale': 0.054916448893760926, 'q_scale': 0.0004395781220958909, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:10:27,872] Trial 54 finished with value: 11.142358779907227 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003097749945698196, 'mc_samples_train': 1, 'prior_scale': 0.3063189159865552, 'q_scale': 0.00028785875540367, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:11:16,956] Trial 55 finished with value: 116.85144805908203 and parameters: {'pretrain_epochs': 0, 'lr': 0.00015253671276923227, 'mc_samples_train': 1, 'prior_scale': 0.09415310255835511, 'q_scale': 0.00010622638533885774, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:12:06,268] Trial 56 finished with value: -15.905345916748047 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006234503695443075, 'mc_samples_train': 1, 'prior_scale': 0.5746791270552903, 'q_scale': 0.0007515326369669672, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:12:54,072] Trial 57 finished with value: -16.40332794189453 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006331440189537419, 'mc_samples_train': 1, 'prior_scale': 0.6055709728691586, 'q_scale': 0.0007833907890241141, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:13:46,412] Trial 58 finished with value: 14.212347030639648 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004713186781202526, 'mc_samples_train': 1, 'prior_scale': 0.8397504651877787, 'q_scale': 0.0016519937294930918, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:13:50,120] Trial 15 finished with value: -17.7005672454834 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009933433836908847, 'mc_samples_train': 2, 'prior_scale': 0.04295889441861082, 'q_scale': 0.0005371630331658818, 'batch_size': 32}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:14:38,224] Trial 59 finished with value: 131.20361328125 and parameters: {'pretrain_epochs': 0, 'lr': 5.518736571418839e-05, 'mc_samples_train': 1, 'prior_scale': 0.5893824062185491, 'q_scale': 0.004160694286668926, 'batch_size': 32}. Best is trial 28 with value: -21.971128463745117.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:18:54,729] Trial 16 finished with value: 56.03738784790039 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009418616963254316, 'mc_samples_train': 2, 'prior_scale': 0.0057475489968997415, 'q_scale': 0.0005478446090810961, 'batch_size': 32}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:20:04,889] Trial 17 finished with value: 254.12265014648438 and parameters: {'pretrain_epochs': 0, 'lr': 0.00048726994464372547, 'mc_samples_train': 2, 'prior_scale': 0.03298570457662521, 'q_scale': 0.0013121308489553924, 'batch_size': 256}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:21:02,433] Trial 18 finished with value: 1979.5206298828125 and parameters: {'pretrain_epochs': 0, 'lr': 0.00048212758884668886, 'mc_samples_train': 2, 'prior_scale': 0.27634381419905757, 'q_scale': 0.0002885851817360491, 'batch_size': 512}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:23:49,966] Trial 19 finished with value: 65.56961059570312 and parameters: {'pretrain_epochs': 0, 'lr': 0.00012544540249913088, 'mc_samples_train': 2, 'prior_scale': 0.007133078560557303, 'q_scale': 0.001932611973562815, 'batch_size': 64}. Best is trial 14 with value: -30.421886444091797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:28:51,520] Trial 20 finished with value: -35.62898254394531 and parameters: {'pretrain_epochs': 0, 'lr': 0.00032043021391109096, 'mc_samples_train': 2, 'prior_scale': 0.05538828055465327, 'q_scale': 0.00024331562128032956, 'batch_size': 32}. Best is trial 20 with value: -35.62898254394531.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-24 17:34:08,325] Trial 21 finished with value: -23.364036560058594 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002844592019038099, 'mc_samples_train': 2, 'prior_scale': 0.060077948867644476, 'q_scale': 0.0002925940795318177, 'batch_size': 32}. Best is trial 20 with value: -35.62898254394531.
[W 2025-02-24 17:38:28,066] Trial 22 failed with parameters: {'pretrain_epochs': 0, 'lr': 0.0005377740280800115, 'mc_samples_train': 2, 'prior_scale': 0.025721840905250332, 'q_scale': 0.0002788619313409219, 'batch_size': 32} because of the following error: FileNotFoundError(2, 'No such file or directory').
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
OSError(5, 'Input/output error')
full_key: datamodule

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 93, in <lambda>
    lambda trial: objective(trial, cfg, output_dir),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 73, in objective_bnn
    return objective(trial, cfg, output_dir)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 32, in objective
    metric_dict, _ = train(cfg, trial)
                     ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 48, in wrap
    save_file(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 93, in save_file
    with open(path, "w+") as file:
         ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/g15farris/bin/bayesaenet/bnn_aenet/logs/lrt_hps_100perc/runs/2025-02-24_16-30-10/022/exec_time.log'
[W 2025-02-24 17:38:28,145] Trial 22 failed with value None.
Error executing job with overrides: ['model=bnn_lrt', 'datamodule=TiO', 'hpsearch=bnn_lrt', 'task_name=lrt_hps_100perc', 'tags=[TiO]', 'datamodule.valid_split=100', 'hpsearch.study.study_name=lrt_100perc']
Traceback (most recent call last):
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 27, in __init__
    self.load_db()
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet_datamodule.py", line 44, in load_db
    self.list_structures_energy, self.list_structures_forces, self.list_removed, self.max_nnb, self.tin = read_list_structures(self.tin)
                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/prepare_batches.py", line 15, in read_list_structures
    list_structures_energy, list_removed, max_nnb, tin = read_train(tin)
                                                         ^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/datamodule/aenet/read_trainset.py", line 104, in read_train
    with open (trainfile, "r") as tf:
         ^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 5] Input/output error: '/work/g15farris/database/TiO/data.train.ascii'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 42, in wrap
    raise ex
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 39, in wrap
    metric_dict, object_dict = task_func(cfg=cfg, trial=trial)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/train.py", line 46, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
           ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'bnn_aenet.datamodule.aenet_datamodule.AenetDataModule':
OSError(5, 'Input/output error')
full_key: datamodule

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 92, in main
    study.optimize(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/study.py", line 451, in optimize
    _optimize(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 62, in _optimize
    _optimize_sequential(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 159, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 247, in _run_trial
    raise func_err
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 93, in <lambda>
    lambda trial: objective(trial, cfg, output_dir),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 73, in objective_bnn
    return objective(trial, cfg, output_dir)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/hpsearch.py", line 32, in objective
    metric_dict, _ = train(cfg, trial)
                     ^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 48, in wrap
    save_file(
  File "/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/g15farris/bin/bayesaenet/bnn_aenet/tasks/utils/utils.py", line 93, in save_file
    with open(path, "w+") as file:
         ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/g15farris/bin/bayesaenet/bnn_aenet/logs/lrt_hps_100perc/runs/2025-02-24_16-30-10/022/exec_time.log'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-02-25 10:43:51,160] Using an existing study with name 'lrt_100perc' instead of creating a new one.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-02-25 11:00:42,846] Using an existing study with name 'lrt_100perc' instead of creating a new one.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:05:54,932] Trial 24 finished with value: -44.99856185913086 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006509790984463777, 'mc_samples_train': 2, 'prior_scale': 0.04658057864895187, 'q_scale': 0.0002632382905467797, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:10:21,556] Trial 25 finished with value: -36.278560638427734 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006829298328061672, 'mc_samples_train': 2, 'prior_scale': 0.03324722460692267, 'q_scale': 0.00034558961381588423, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:14:47,120] Trial 26 finished with value: -15.924846649169922 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006238411395857852, 'mc_samples_train': 2, 'prior_scale': 0.18847537475897708, 'q_scale': 0.0059812201173843095, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:19:11,762] Trial 27 finished with value: -28.120494842529297 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006983210809334892, 'mc_samples_train': 2, 'prior_scale': 0.07072483341252464, 'q_scale': 0.0011472396045626744, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:23:39,119] Trial 28 finished with value: -35.11814880371094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003728086209673569, 'mc_samples_train': 2, 'prior_scale': 0.4102783837375459, 'q_scale': 0.0003379845080952342, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:26:06,557] Trial 29 finished with value: -31.6918888092041 and parameters: {'pretrain_epochs': 0, 'lr': 0.00020597547065149416, 'mc_samples_train': 2, 'prior_scale': 0.021794603752404315, 'q_scale': 0.00019803364724850774, 'batch_size': 64}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:26:57,178] Trial 30 finished with value: 2440.457275390625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00040552576010057743, 'mc_samples_train': 2, 'prior_scale': 0.11011957907017718, 'q_scale': 0.0006899810583390177, 'batch_size': 512}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[I 2025-02-25 11:27:28,461] Trial 31 pruned. Trial was pruned at epoch 7.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:28:31,225] Trial 32 finished with value: 923.0511474609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.00010682995798866446, 'mc_samples_train': 2, 'prior_scale': 0.025831207105731828, 'q_scale': 0.002352154084601003, 'batch_size': 256}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:33:01,664] Trial 33 finished with value: -36.36891555786133 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003684161850187871, 'mc_samples_train': 2, 'prior_scale': 0.37378992749314094, 'q_scale': 0.0003212726526118929, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:37:28,168] Trial 34 finished with value: -20.723482131958008 and parameters: {'pretrain_epochs': 0, 'lr': 0.00037904252978057163, 'mc_samples_train': 2, 'prior_scale': 0.8631445024831159, 'q_scale': 0.0002258825207351197, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:41:53,178] Trial 35 finished with value: -36.47572326660156 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007245914220860388, 'mc_samples_train': 2, 'prior_scale': 0.3805723370060441, 'q_scale': 0.0009485522113989473, 'batch_size': 32}. Best is trial 24 with value: -44.99856185913086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:46:18,745] Trial 36 finished with value: -46.62199401855469 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007286170843324783, 'mc_samples_train': 2, 'prior_scale': 0.2974764122719904, 'q_scale': 0.0011561199955338337, 'batch_size': 32}. Best is trial 36 with value: -46.62199401855469.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:47:09,083] Trial 37 finished with value: 2178.370849609375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005243619617447778, 'mc_samples_train': 2, 'prior_scale': 0.4928283400121777, 'q_scale': 0.00117112249392426, 'batch_size': 512}. Best is trial 36 with value: -46.62199401855469.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:203: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  agg_scale = precision.reciprocal().mean(dim).add(loc.var(dim)).sqrt()
[I 2025-02-25 11:48:17,939] Trial 38 pruned. Trial was pruned at epoch 7.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:52:45,591] Trial 39 finished with value: -2.344482183456421 and parameters: {'pretrain_epochs': 0, 'lr': 0.000560203106547756, 'mc_samples_train': 2, 'prior_scale': 0.3660045282784296, 'q_scale': 0.005211051528825213, 'batch_size': 32}. Best is trial 36 with value: -46.62199401855469.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-25 11:54:16,839] Trial 40 pruned. Trial was pruned at epoch 10.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:56:46,768] Trial 41 finished with value: 253.29925537109375 and parameters: {'pretrain_epochs': 0, 'lr': 1.0294620903386365e-05, 'mc_samples_train': 2, 'prior_scale': 0.5903418491155675, 'q_scale': 0.0035117890454306813, 'batch_size': 64}. Best is trial 36 with value: -46.62199401855469.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 11:57:28,261] Trial 42 finished with value: 1006.789306640625 and parameters: {'pretrain_epochs': 0, 'lr': 3.075611820416461e-05, 'mc_samples_train': 1, 'prior_scale': 0.1243042312330324, 'q_scale': 0.0008839908430144221, 'batch_size': 256}. Best is trial 36 with value: -46.62199401855469.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:01:56,134] Trial 43 finished with value: -47.81182861328125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007615317917538199, 'mc_samples_train': 2, 'prior_scale': 0.2997290524017179, 'q_scale': 0.0004382195401082087, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:06:25,217] Trial 44 finished with value: -35.23133087158203 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007940523052301803, 'mc_samples_train': 2, 'prior_scale': 0.29819852329246094, 'q_scale': 0.0018337796577275539, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:10:53,225] Trial 45 finished with value: -12.5316162109375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004346673644455997, 'mc_samples_train': 2, 'prior_scale': 0.9744952982848707, 'q_scale': 0.00017000949050492947, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:15:23,015] Trial 46 finished with value: -35.87584686279297 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005547915996773387, 'mc_samples_train': 2, 'prior_scale': 0.6039419025768079, 'q_scale': 0.0006966133059950005, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:16:54,465] Trial 47 finished with value: 441.498046875 and parameters: {'pretrain_epochs': 0, 'lr': 5.1375879420384186e-05, 'mc_samples_train': 2, 'prior_scale': 0.3621742694732916, 'q_scale': 0.0004621627756722631, 'batch_size': 128}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:19:35,445] Trial 48 finished with value: -22.34197425842285 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008384052381899798, 'mc_samples_train': 1, 'prior_scale': 0.08542144783764337, 'q_scale': 0.0016146632654310278, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:24:04,212] Trial 49 finished with value: -46.978965759277344 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006330373057854226, 'mc_samples_train': 2, 'prior_scale': 0.18036732081542833, 'q_scale': 0.000887761860037635, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:28:34,916] Trial 50 finished with value: -33.61904525756836 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006290102057144817, 'mc_samples_train': 2, 'prior_scale': 0.193010925093162, 'q_scale': 0.0029634572562788025, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:29:26,662] Trial 51 finished with value: 1457.940185546875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008769255259426099, 'mc_samples_train': 2, 'prior_scale': 0.08932033956168513, 'q_scale': 0.021640621136322986, 'batch_size': 512}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:30:57,701] Trial 52 finished with value: 422.7819519042969 and parameters: {'pretrain_epochs': 0, 'lr': 6.585553704010568e-05, 'mc_samples_train': 2, 'prior_scale': 0.1552775640762928, 'q_scale': 0.0009611441399406295, 'batch_size': 128}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:35:30,566] Trial 53 finished with value: -40.28005599975586 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006149970689040431, 'mc_samples_train': 2, 'prior_scale': 0.26512852618136656, 'q_scale': 0.000577010115338762, 'batch_size': 32}. Best is trial 43 with value: -47.81182861328125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:39:59,622] Trial 54 finished with value: -49.54961013793945 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005881574450522152, 'mc_samples_train': 2, 'prior_scale': 0.2591948435707256, 'q_scale': 0.000636963617578096, 'batch_size': 32}. Best is trial 54 with value: -49.54961013793945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:44:29,638] Trial 55 finished with value: -47.559486389160156 and parameters: {'pretrain_epochs': 0, 'lr': 0.00046920795808497797, 'mc_samples_train': 2, 'prior_scale': 0.24889977469572572, 'q_scale': 0.0006548994792766683, 'batch_size': 32}. Best is trial 54 with value: -49.54961013793945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:48:59,293] Trial 56 finished with value: -44.15993881225586 and parameters: {'pretrain_epochs': 0, 'lr': 0.000479176835878153, 'mc_samples_train': 2, 'prior_scale': 0.2164729112422234, 'q_scale': 0.00016255787458547795, 'batch_size': 32}. Best is trial 54 with value: -49.54961013793945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-25 12:50:16,098] Trial 57 pruned. Trial was pruned at epoch 4.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:54:49,118] Trial 58 finished with value: -42.6210823059082 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009953935228166127, 'mc_samples_train': 2, 'prior_scale': 0.5217392174673524, 'q_scale': 0.0005408946508854562, 'batch_size': 32}. Best is trial 54 with value: -49.54961013793945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 12:57:37,245] Trial 59 finished with value: 129.19342041015625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00033056606661663573, 'mc_samples_train': 1, 'prior_scale': 0.0014818536151051014, 'q_scale': 0.00037168343417977735, 'batch_size': 32}. Best is trial 54 with value: -49.54961013793945.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:00:11,964] Trial 60 finished with value: -51.95450210571289 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002482679089394269, 'mc_samples_train': 2, 'prior_scale': 0.16140767274576168, 'q_scale': 0.0014998028529645947, 'batch_size': 64}. Best is trial 60 with value: -51.95450210571289.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:02:45,197] Trial 61 finished with value: -35.48674011230469 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002516412831975507, 'mc_samples_train': 2, 'prior_scale': 0.2904095841242468, 'q_scale': 0.0014341769538948682, 'batch_size': 64}. Best is trial 60 with value: -51.95450210571289.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:05:18,881] Trial 62 finished with value: -10.74655818939209 and parameters: {'pretrain_epochs': 0, 'lr': 0.00014612039095042477, 'mc_samples_train': 2, 'prior_scale': 0.17364561913386883, 'q_scale': 0.002484450015659679, 'batch_size': 64}. Best is trial 60 with value: -51.95450210571289.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:07:52,516] Trial 63 finished with value: -90.33507537841797 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006112237319911698, 'mc_samples_train': 2, 'prior_scale': 0.22656050867842187, 'q_scale': 0.000444594528183239, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:10:25,686] Trial 64 finished with value: -69.40507507324219 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005329084989890526, 'mc_samples_train': 2, 'prior_scale': 0.2212667333874839, 'q_scale': 0.000725736714091057, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:12:56,361] Trial 65 finished with value: -85.56182098388672 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005617342933955713, 'mc_samples_train': 2, 'prior_scale': 0.1446985960420336, 'q_scale': 0.0007371684349194762, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:15:26,872] Trial 66 finished with value: -57.858943939208984 and parameters: {'pretrain_epochs': 0, 'lr': 0.00031266853636797304, 'mc_samples_train': 2, 'prior_scale': 0.10319184201525437, 'q_scale': 0.0007439341763389912, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:17:57,293] Trial 67 finished with value: -67.51888275146484 and parameters: {'pretrain_epochs': 0, 'lr': 0.00032233941289796955, 'mc_samples_train': 2, 'prior_scale': 0.10327493848803794, 'q_scale': 0.00045368614665782187, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:20:30,070] Trial 68 finished with value: -60.81745910644531 and parameters: {'pretrain_epochs': 0, 'lr': 0.00023116626974829078, 'mc_samples_train': 2, 'prior_scale': 0.10684746225783721, 'q_scale': 0.0007301264915899488, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:23:05,120] Trial 69 finished with value: -48.662010192871094 and parameters: {'pretrain_epochs': 0, 'lr': 0.00020338036945597227, 'mc_samples_train': 2, 'prior_scale': 0.06696831950609647, 'q_scale': 0.00041367237001281213, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-02-25 13:24:39,059] Trial 70 pruned. Trial was pruned at epoch 11.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:27:10,796] Trial 71 finished with value: -65.20162200927734 and parameters: {'pretrain_epochs': 0, 'lr': 0.00025636487944607095, 'mc_samples_train': 2, 'prior_scale': 0.12778168109400143, 'q_scale': 0.00012256660654409398, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:29:40,748] Trial 72 finished with value: -65.58686065673828 and parameters: {'pretrain_epochs': 0, 'lr': 0.00034253305734575966, 'mc_samples_train': 2, 'prior_scale': 0.11057058360008695, 'q_scale': 0.00013526611362317908, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:32:19,329] Trial 73 finished with value: -40.36310577392578 and parameters: {'pretrain_epochs': 0, 'lr': 0.00017999357876446857, 'mc_samples_train': 2, 'prior_scale': 0.12011638078136157, 'q_scale': 0.00011881019455953559, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:35:23,891] Trial 74 finished with value: -71.7834701538086 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003371727945055252, 'mc_samples_train': 2, 'prior_scale': 0.0813305980279409, 'q_scale': 0.00020285551884818566, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:38:21,610] Trial 75 finished with value: -52.69337463378906 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003982713597966943, 'mc_samples_train': 2, 'prior_scale': 0.0690103031092565, 'q_scale': 0.00020159662491373343, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:41:18,454] Trial 76 finished with value: -73.2134017944336 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003492419081791508, 'mc_samples_train': 2, 'prior_scale': 0.1419650934730244, 'q_scale': 0.00013555260976025975, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:44:16,011] Trial 77 finished with value: -74.15149688720703 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003427187858186089, 'mc_samples_train': 2, 'prior_scale': 0.14147699282096687, 'q_scale': 0.0001349612689423686, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:47:17,415] Trial 78 finished with value: -53.30698013305664 and parameters: {'pretrain_epochs': 0, 'lr': 0.00035515811832816205, 'mc_samples_train': 2, 'prior_scale': 0.08298310373108063, 'q_scale': 0.00017007457743319746, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:49:17,343] Trial 79 finished with value: -67.42692565917969 and parameters: {'pretrain_epochs': 0, 'lr': 0.000290829664453999, 'mc_samples_train': 1, 'prior_scale': 0.14076790324402377, 'q_scale': 0.000258743398460186, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:51:05,416] Trial 80 finished with value: -61.04832458496094 and parameters: {'pretrain_epochs': 0, 'lr': 0.00041434390682968005, 'mc_samples_train': 1, 'prior_scale': 0.1427423132374321, 'q_scale': 0.0002489919576897951, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:52:54,841] Trial 81 finished with value: -64.0602798461914 and parameters: {'pretrain_epochs': 0, 'lr': 0.00028049313586589366, 'mc_samples_train': 1, 'prior_scale': 0.042923249866200816, 'q_scale': 0.0002857122359359686, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:54:44,008] Trial 82 finished with value: -87.05892944335938 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005343251405187812, 'mc_samples_train': 1, 'prior_scale': 0.21251938015643568, 'q_scale': 0.00021907339916724436, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-02-25 13:56:30,498] Trial 83 finished with value: -81.32711791992188 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004733634912280229, 'mc_samples_train': 1, 'prior_scale': 0.20910268495933063, 'q_scale': 0.0002134022862276677, 'batch_size': 64}. Best is trial 63 with value: -90.33507537841797.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-05-09 14:04:32,911] Using an existing study with name 'hom_lrt_100perc' instead of creating a new one.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:08:59,473] Trial 1 finished with value: 17799575552.0 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 4, 'prior_scale': 0.22702608040701588, 'q_scale': 0.13179630432958683, 'obs_scale': 0.0012313185468743894, 'batch_size': 128}. Best is trial 1 with value: 17799575552.0.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:13:36,159] Trial 2 finished with value: 65473808.0 and parameters: {'pretrain_epochs': 0, 'lr': 0.00010045932391231576, 'mc_samples_train': 8, 'prior_scale': 0.012904829303853454, 'q_scale': 0.017570525244134657, 'obs_scale': 0.010288040405240286, 'batch_size': 128}. Best is trial 2 with value: 65473808.0.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-05-09 14:16:55,543] A new study created in RDB with name: hom_lrt_100perc
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:260: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  scale = predictions.var(dim).add(self.scale ** 2).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:17:39,727] Trial 0 finished with value: 20886499328.0 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 0.22702608040701588, 'q_scale': 0.13179630432958683, 'obs_scale': 0.0012313185468743894, 'batch_size': 128}. Best is trial 0 with value: 20886499328.0.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:18:34,755] Trial 1 finished with value: 60626456.0 and parameters: {'pretrain_epochs': 0, 'lr': 0.00010045932391231576, 'mc_samples_train': 2, 'prior_scale': 0.012904829303853454, 'q_scale': 0.017570525244134657, 'obs_scale': 0.010288040405240286, 'batch_size': 128}. Best is trial 1 with value: 60626456.0.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:19:29,795] Trial 2 finished with value: 1515853.375 and parameters: {'pretrain_epochs': 0, 'lr': 1.4150196905720475e-05, 'mc_samples_train': 2, 'prior_scale': 0.08997760513084464, 'q_scale': 0.0038798086852341396, 'obs_scale': 0.14286326515987452, 'batch_size': 128}. Best is trial 2 with value: 1515853.375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:22:00,537] Trial 3 finished with value: 481005792.0 and parameters: {'pretrain_epochs': 0, 'lr': 1.9388028480984598e-05, 'mc_samples_train': 2, 'prior_scale': 0.004532901866195528, 'q_scale': 0.5005765657505178, 'obs_scale': 0.005868985298319507, 'batch_size': 32}. Best is trial 2 with value: 1515853.375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:22:23,232] Trial 4 finished with value: 1395468672.0 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001546142648648772, 'mc_samples_train': 1, 'prior_scale': 0.04833917568085432, 'q_scale': 0.0020829257190366937, 'obs_scale': 0.010277023093936914, 'batch_size': 512}. Best is trial 2 with value: 1515853.375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:24:54,355] Trial 5 finished with value: 5633704.0 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006671842305866796, 'mc_samples_train': 2, 'prior_scale': 0.7523246408184491, 'q_scale': 0.14718262393979, 'obs_scale': 0.001383578612730786, 'batch_size': 32}. Best is trial 2 with value: 1515853.375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:25:21,988] Trial 6 finished with value: 103639632.0 and parameters: {'pretrain_epochs': 0, 'lr': 8.003810848108467e-05, 'mc_samples_train': 1, 'prior_scale': 0.0022810914347080207, 'q_scale': 0.08996886145567823, 'obs_scale': 0.022346758408902257, 'batch_size': 256}. Best is trial 2 with value: 1515853.375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:25:54,179] Trial 7 finished with value: 708647.5625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00011795146111975982, 'mc_samples_train': 2, 'prior_scale': 0.020449350394769326, 'q_scale': 0.028092862158076635, 'obs_scale': 0.470752138441911, 'batch_size': 512}. Best is trial 7 with value: 708647.5625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:26:20,690] Trial 8 finished with value: 33117.25390625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00018511923116382368, 'mc_samples_train': 1, 'prior_scale': 0.0507257930731161, 'q_scale': 0.012968719897940156, 'obs_scale': 0.6393152366775274, 'batch_size': 256}. Best is trial 8 with value: 33117.25390625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:26:43,561] Trial 9 finished with value: 161092.65625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00021819434272647882, 'mc_samples_train': 1, 'prior_scale': 0.24581113447438505, 'q_scale': 0.0170232827909058, 'obs_scale': 0.730039111673774, 'batch_size': 512}. Best is trial 8 with value: 33117.25390625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:27:10,322] Trial 10 finished with value: 2046565120.0 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007052698205666697, 'mc_samples_train': 1, 'prior_scale': 0.0012033601967563817, 'q_scale': 0.0001361190166330691, 'obs_scale': 0.00017249055645610024, 'batch_size': 256}. Best is trial 8 with value: 33117.25390625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:27:32,972] Trial 11 finished with value: 69537.265625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003201413211508342, 'mc_samples_train': 1, 'prior_scale': 0.2174903379342743, 'q_scale': 0.0010602949178692943, 'obs_scale': 0.7229975507410528, 'batch_size': 512}. Best is trial 8 with value: 33117.25390625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:28:27,530] Trial 12 finished with value: 119.92971801757812 and parameters: {'pretrain_epochs': 0, 'lr': 0.00031379702063305813, 'mc_samples_train': 1, 'prior_scale': 0.11328026753038115, 'q_scale': 0.00036289005904029424, 'obs_scale': 0.10166977624893378, 'batch_size': 64}. Best is trial 12 with value: 119.92971801757812.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:29:21,103] Trial 13 finished with value: 394561.8125 and parameters: {'pretrain_epochs': 0, 'lr': 4.951927991510018e-05, 'mc_samples_train': 1, 'prior_scale': 0.05624378187977906, 'q_scale': 0.0002561091114086167, 'obs_scale': 0.08355106470728492, 'batch_size': 64}. Best is trial 12 with value: 119.92971801757812.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:30:15,191] Trial 14 finished with value: 433.26141357421875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004012358719846703, 'mc_samples_train': 1, 'prior_scale': 0.010348041552448863, 'q_scale': 0.0006945380684913704, 'obs_scale': 0.06964993352407617, 'batch_size': 64}. Best is trial 12 with value: 119.92971801757812.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:31:09,697] Trial 15 finished with value: 381.2350769042969 and parameters: {'pretrain_epochs': 0, 'lr': 0.00040189306792831733, 'mc_samples_train': 1, 'prior_scale': 0.008470212946932433, 'q_scale': 0.0005478446090810961, 'obs_scale': 0.07615625232359143, 'batch_size': 64}. Best is trial 12 with value: 119.92971801757812.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:32:03,318] Trial 16 finished with value: 65.76288604736328 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003564681699476287, 'mc_samples_train': 1, 'prior_scale': 0.005786234538849157, 'q_scale': 0.00048564715929177754, 'obs_scale': 0.17020180391246936, 'batch_size': 64}. Best is trial 16 with value: 65.76288604736328.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:32:57,406] Trial 17 finished with value: 42.50979232788086 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007298662778869769, 'mc_samples_train': 1, 'prior_scale': 0.8243090352929017, 'q_scale': 0.00438900888415861, 'obs_scale': 0.19048442316907535, 'batch_size': 64}. Best is trial 17 with value: 42.50979232788086.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:33:51,658] Trial 18 finished with value: 41.263065338134766 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008719487970817801, 'mc_samples_train': 1, 'prior_scale': 0.0037429833888929973, 'q_scale': 0.005113553365715799, 'obs_scale': 0.2578494248433724, 'batch_size': 64}. Best is trial 18 with value: 41.263065338134766.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:34:45,823] Trial 19 finished with value: 2448.423583984375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009686092886665959, 'mc_samples_train': 1, 'prior_scale': 0.8687125746974788, 'q_scale': 0.003876476079860623, 'obs_scale': 0.0311907381751181, 'batch_size': 64}. Best is trial 18 with value: 41.263065338134766.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:35:40,626] Trial 20 finished with value: 83.43441772460938 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009721875780405707, 'mc_samples_train': 1, 'prior_scale': 0.0024389699271548013, 'q_scale': 0.004206578480016768, 'obs_scale': 0.2150083166847527, 'batch_size': 64}. Best is trial 18 with value: 41.263065338134766.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:36:34,670] Trial 21 finished with value: 31.112049102783203 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005757045754386834, 'mc_samples_train': 1, 'prior_scale': 0.0046469643273162825, 'q_scale': 0.0013091521263877367, 'obs_scale': 0.28961692331013555, 'batch_size': 64}. Best is trial 21 with value: 31.112049102783203.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:37:28,339] Trial 22 finished with value: 40.36191177368164 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005921740951288565, 'mc_samples_train': 1, 'prior_scale': 0.0030446162685201037, 'q_scale': 0.001724588376112509, 'obs_scale': 0.30667527800197275, 'batch_size': 64}. Best is trial 21 with value: 31.112049102783203.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:38:22,536] Trial 23 finished with value: 165.19715881347656 and parameters: {'pretrain_epochs': 0, 'lr': 0.000536822575062936, 'mc_samples_train': 1, 'prior_scale': 0.0011413267404526077, 'q_scale': 0.0012960015699229562, 'obs_scale': 0.3879995067830569, 'batch_size': 64}. Best is trial 21 with value: 31.112049102783203.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:39:16,830] Trial 24 finished with value: 2049.7646484375 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005144119114336097, 'mc_samples_train': 1, 'prior_scale': 0.003461631611228056, 'q_scale': 0.0019871033605073853, 'obs_scale': 0.03673163516541704, 'batch_size': 64}. Best is trial 21 with value: 31.112049102783203.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:40:10,247] Trial 25 finished with value: 172.11143493652344 and parameters: {'pretrain_epochs': 0, 'lr': 0.00026593649915112417, 'mc_samples_train': 1, 'prior_scale': 0.002015607541881247, 'q_scale': 0.007464355757734977, 'obs_scale': 0.28919447884402816, 'batch_size': 64}. Best is trial 21 with value: 31.112049102783203.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:42:41,379] Trial 26 finished with value: 54.6760139465332 and parameters: {'pretrain_epochs': 0, 'lr': 0.000522068302954025, 'mc_samples_train': 2, 'prior_scale': 0.017215876506916007, 'q_scale': 0.04229908972243146, 'obs_scale': 0.8975382384492414, 'batch_size': 32}. Best is trial 21 with value: 31.112049102783203.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:43:34,846] Trial 27 finished with value: 6.856269836425781 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009525115571499119, 'mc_samples_train': 1, 'prior_scale': 0.006978069871237481, 'q_scale': 0.00010188974074302672, 'obs_scale': 0.32717655705233817, 'batch_size': 64}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:44:29,251] Trial 28 finished with value: 276784960.0 and parameters: {'pretrain_epochs': 0, 'lr': 6.0039391784605726e-05, 'mc_samples_train': 1, 'prior_scale': 0.007394758200036625, 'q_scale': 0.00011479722491811582, 'obs_scale': 0.0025930907963194845, 'batch_size': 64}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:45:05,539] Trial 29 finished with value: 201432256.0 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005482802194723169, 'mc_samples_train': 1, 'prior_scale': 0.026213287708079246, 'q_scale': 0.00019984445993127497, 'obs_scale': 0.00021428674377736294, 'batch_size': 128}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:46:00,347] Trial 30 finished with value: 58556088320.0 and parameters: {'pretrain_epochs': 0, 'lr': 2.786310118520716e-05, 'mc_samples_train': 1, 'prior_scale': 0.0016676712783183854, 'q_scale': 0.0011895877730921038, 'obs_scale': 0.000449867292531109, 'batch_size': 64}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:46:53,403] Trial 31 finished with value: 50.3008918762207 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009760976782580093, 'mc_samples_train': 1, 'prior_scale': 0.003617994045918817, 'q_scale': 0.006800810933663895, 'obs_scale': 0.3420929813792762, 'batch_size': 64}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:47:47,251] Trial 32 finished with value: 764.7837524414062 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007279310750376537, 'mc_samples_train': 1, 'prior_scale': 0.0034042712103071983, 'q_scale': 0.0021013828099196155, 'obs_scale': 0.044817111600374576, 'batch_size': 64}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:48:23,155] Trial 33 finished with value: 119.2334213256836 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007534664171841039, 'mc_samples_train': 1, 'prior_scale': 0.005818287184640438, 'q_scale': 0.0007657116452652392, 'obs_scale': 0.15168859084710956, 'batch_size': 128}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:49:50,495] Trial 34 finished with value: 13.305643081665039 and parameters: {'pretrain_epochs': 0, 'lr': 0.000435024830440006, 'mc_samples_train': 2, 'prior_scale': 0.01051079371859997, 'q_scale': 0.0002760701984226494, 'obs_scale': 0.2943814504152221, 'batch_size': 64}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:52:20,164] Trial 35 finished with value: 11.920197486877441 and parameters: {'pretrain_epochs': 0, 'lr': 0.00024338012322294106, 'mc_samples_train': 2, 'prior_scale': 0.014391229988015402, 'q_scale': 0.0002598386084498242, 'obs_scale': 0.4226496915118811, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-09 14:54:49,244] Trial 36 pruned. Trial was pruned at epoch 19.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:57:18,253] Trial 37 finished with value: 52394.82421875 and parameters: {'pretrain_epochs': 0, 'lr': 0.000146637924849408, 'mc_samples_train': 2, 'prior_scale': 0.03342391344628967, 'q_scale': 0.00010923277746588155, 'obs_scale': 0.005001221932139372, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 14:59:49,760] Trial 38 finished with value: 13.309886932373047 and parameters: {'pretrain_epochs': 0, 'lr': 0.00044408014782242635, 'mc_samples_train': 2, 'prior_scale': 0.012807507043864774, 'q_scale': 0.00034947196470685326, 'obs_scale': 0.4911719125883501, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:02:19,503] Trial 39 finished with value: 15.68399715423584 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004242692454594696, 'mc_samples_train': 2, 'prior_scale': 0.01330090846550967, 'q_scale': 0.00017277449507773968, 'obs_scale': 0.5278963637518143, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-09 15:03:15,726] Trial 40 pruned. Trial was pruned at epoch 6.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:05:46,498] Trial 41 finished with value: 16.83669090270996 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002649718114210343, 'mc_samples_train': 2, 'prior_scale': 0.01261055234386262, 'q_scale': 0.00035121581813330545, 'obs_scale': 0.5161040981611065, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:08:18,226] Trial 42 finished with value: 17.45623207092285 and parameters: {'pretrain_epochs': 0, 'lr': 0.00038016476362123867, 'mc_samples_train': 2, 'prior_scale': 0.018475147000319058, 'q_scale': 0.00017519530538603277, 'obs_scale': 0.5851423969879924, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:10:47,800] Trial 43 finished with value: 30.085914611816406 and parameters: {'pretrain_epochs': 0, 'lr': 0.00044950121490198156, 'mc_samples_train': 2, 'prior_scale': 0.008492360208277823, 'q_scale': 0.000326704394046656, 'obs_scale': 0.843598263501086, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:13:16,816] Trial 44 finished with value: 39.08932113647461 and parameters: {'pretrain_epochs': 0, 'lr': 0.00012248376599174776, 'mc_samples_train': 2, 'prior_scale': 0.01399002000404684, 'q_scale': 0.00016355965302613, 'obs_scale': 0.9595714657589971, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:15:46,040] Trial 45 finished with value: 8.59325122833252 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001966976620736794, 'mc_samples_train': 2, 'prior_scale': 0.032704371520486296, 'q_scale': 0.00011023469114744368, 'obs_scale': 0.3922462746069821, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:16:25,387] Trial 46 finished with value: 2943794.0 and parameters: {'pretrain_epochs': 0, 'lr': 0.00019537781207961414, 'mc_samples_train': 2, 'prior_scale': 0.07276512736946335, 'q_scale': 0.0004453655779809253, 'obs_scale': 0.05343784426933948, 'batch_size': 256}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:16:57,230] Trial 47 finished with value: 11448232.0 and parameters: {'pretrain_epochs': 0, 'lr': 8.179449801951318e-05, 'mc_samples_train': 2, 'prior_scale': 0.03902810287183117, 'q_scale': 0.00010285514414457964, 'obs_scale': 0.12229482494959486, 'batch_size': 512}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:19:27,245] Trial 48 finished with value: 16.140411376953125 and parameters: {'pretrain_epochs': 0, 'lr': 0.000290812295227968, 'mc_samples_train': 2, 'prior_scale': 0.006361142192426126, 'q_scale': 0.00025451618111637633, 'obs_scale': 0.42212729551997635, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:20:23,731] Trial 49 finished with value: 3761.590087890625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002089646085115131, 'mc_samples_train': 2, 'prior_scale': 0.024154714333464312, 'q_scale': 0.000611193581176132, 'obs_scale': 0.18989621812734603, 'batch_size': 128}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:22:53,391] Trial 50 finished with value: 2731.8125 and parameters: {'pretrain_epochs': 0, 'lr': 0.00014514940333178829, 'mc_samples_train': 2, 'prior_scale': 0.11579206118788156, 'q_scale': 0.0008515926746262949, 'obs_scale': 0.019472279411561325, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:25:24,965] Trial 51 finished with value: 19.78343391418457 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004425571259478007, 'mc_samples_train': 2, 'prior_scale': 0.010231512234399026, 'q_scale': 0.00017054354034715675, 'obs_scale': 0.5874005494225982, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:27:54,428] Trial 52 finished with value: 10.729240417480469 and parameters: {'pretrain_epochs': 0, 'lr': 0.00032642966636080864, 'mc_samples_train': 2, 'prior_scale': 0.01771360889552445, 'q_scale': 0.00025485457679477725, 'obs_scale': 0.4334485088512814, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:30:25,584] Trial 53 finished with value: 1220.7364501953125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002988837349590806, 'mc_samples_train': 2, 'prior_scale': 0.04288144256918964, 'q_scale': 0.17809483523567055, 'obs_scale': 0.4116909794890455, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:32:54,866] Trial 54 finished with value: 9.663515090942383 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003376565285954643, 'mc_samples_train': 2, 'prior_scale': 0.017423016450418365, 'q_scale': 0.00028831748866903743, 'obs_scale': 0.22258574982428683, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:33:35,516] Trial 55 finished with value: 32673.595703125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003437077594531547, 'mc_samples_train': 2, 'prior_scale': 0.019906501775661174, 'q_scale': 0.00021655294804892952, 'obs_scale': 0.22284922755732922, 'batch_size': 256}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:34:08,286] Trial 56 finished with value: 8392652.0 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001159728999155321, 'mc_samples_train': 2, 'prior_scale': 0.06530134379454934, 'q_scale': 0.00014130356482682392, 'obs_scale': 0.13648687636389653, 'batch_size': 512}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:36:37,173] Trial 57 finished with value: 8.274279594421387 and parameters: {'pretrain_epochs': 0, 'lr': 0.00023986858287954225, 'mc_samples_train': 2, 'prior_scale': 0.009058036536686713, 'q_scale': 0.00042892619470915884, 'obs_scale': 0.24031925936506676, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:39:07,722] Trial 58 finished with value: 449.09356689453125 and parameters: {'pretrain_epochs': 0, 'lr': 9.015300072201408e-05, 'mc_samples_train': 2, 'prior_scale': 0.03205588495241683, 'q_scale': 0.0004723586703904734, 'obs_scale': 0.06810677397263011, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=20` reached.
[I 2025-05-09 15:41:38,978] Trial 59 finished with value: 73.12128448486328 and parameters: {'pretrain_epochs': 0, 'lr': 0.00023509102574953728, 'mc_samples_train': 2, 'prior_scale': 0.0048273074953930975, 'q_scale': 0.00012774242030345184, 'obs_scale': 0.10247462413095178, 'batch_size': 32}. Best is trial 27 with value: 6.856269836425781.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-05-09 16:07:32,334] A new study created in RDB with name: hom_lrt_100perc_big
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:260: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  scale = predictions.var(dim).add(self.scale ** 2).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-09 17:04:18,106] Trial 0 finished with value: 21384418.0 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 0.22702608040701588, 'q_scale': 0.13179630432958683, 'obs_scale': 0.0012313185468743894, 'batch_size': 128}. Best is trial 0 with value: 21384418.0.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-09 18:54:03,432] Trial 1 finished with value: 934.2886352539062 and parameters: {'pretrain_epochs': 0, 'lr': 0.00010045932391231576, 'mc_samples_train': 2, 'prior_scale': 0.012904829303853454, 'q_scale': 0.017570525244134657, 'obs_scale': 0.010288040405240286, 'batch_size': 128}. Best is trial 1 with value: 934.2886352539062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-09 20:40:31,471] Trial 2 finished with value: -51.491310119628906 and parameters: {'pretrain_epochs': 0, 'lr': 1.4150196905720475e-05, 'mc_samples_train': 2, 'prior_scale': 0.08997760513084464, 'q_scale': 0.0038798086852341396, 'obs_scale': 0.14286326515987452, 'batch_size': 128}. Best is trial 2 with value: -51.491310119628906.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 01:25:07,353] Trial 3 finished with value: 64969.2734375 and parameters: {'pretrain_epochs': 0, 'lr': 1.9388028480984598e-05, 'mc_samples_train': 2, 'prior_scale': 0.004532901866195528, 'q_scale': 0.5005765657505178, 'obs_scale': 0.005868985298319507, 'batch_size': 32}. Best is trial 2 with value: -51.491310119628906.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 01:52:59,195] Trial 4 finished with value: 70734.7265625 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001546142648648772, 'mc_samples_train': 1, 'prior_scale': 0.04833917568085432, 'q_scale': 0.0020829257190366937, 'obs_scale': 0.010277023093936914, 'batch_size': 512}. Best is trial 2 with value: -51.491310119628906.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 06:14:19,521] Trial 5 finished with value: 101780.3828125 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006671842305866796, 'mc_samples_train': 2, 'prior_scale': 0.7523246408184491, 'q_scale': 0.14718262393979, 'obs_scale': 0.001383578612730786, 'batch_size': 32}. Best is trial 2 with value: -51.491310119628906.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[I 2025-05-10 06:16:32,712] Trial 6 pruned. Trial was pruned at epoch 118.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 07:04:35,836] Trial 7 finished with value: 198.37306213378906 and parameters: {'pretrain_epochs': 0, 'lr': 0.00011795146111975982, 'mc_samples_train': 2, 'prior_scale': 0.020449350394769326, 'q_scale': 0.028092862158076635, 'obs_scale': 0.470752138441911, 'batch_size': 512}. Best is trial 2 with value: -51.491310119628906.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 07:40:32,434] Trial 8 finished with value: 113.8629150390625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00018511923116382368, 'mc_samples_train': 1, 'prior_scale': 0.0507257930731161, 'q_scale': 0.012968719897940156, 'obs_scale': 0.6393152366775274, 'batch_size': 256}. Best is trial 2 with value: -51.491310119628906.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 08:09:54,955] Trial 9 finished with value: 435.34814453125 and parameters: {'pretrain_epochs': 0, 'lr': 0.00021819434272647882, 'mc_samples_train': 1, 'prior_scale': 0.24581113447438505, 'q_scale': 0.0170232827909058, 'obs_scale': 0.730039111673774, 'batch_size': 512}. Best is trial 2 with value: -51.491310119628906.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 09:42:14,234] Trial 10 finished with value: -122.037841796875 and parameters: {'pretrain_epochs': 0, 'lr': 1.1267228743583923e-05, 'mc_samples_train': 2, 'prior_scale': 0.1438501886620343, 'q_scale': 0.0001361190166330691, 'obs_scale': 0.12508757174951737, 'batch_size': 128}. Best is trial 10 with value: -122.037841796875.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 11:10:16,200] Trial 11 finished with value: -137.9308624267578 and parameters: {'pretrain_epochs': 0, 'lr': 1.0962454827051684e-05, 'mc_samples_train': 2, 'prior_scale': 0.13160546287420333, 'q_scale': 0.00011306332444916999, 'obs_scale': 0.10193657033086194, 'batch_size': 128}. Best is trial 11 with value: -137.9308624267578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 13:33:46,318] Trial 12 finished with value: -85.18567657470703 and parameters: {'pretrain_epochs': 0, 'lr': 1.0170394077008131e-05, 'mc_samples_train': 2, 'prior_scale': 0.2238470043565636, 'q_scale': 0.00012780894352276231, 'obs_scale': 0.07946981200360269, 'batch_size': 64}. Best is trial 11 with value: -137.9308624267578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 15:16:25,350] Trial 13 finished with value: -101.4126205444336 and parameters: {'pretrain_epochs': 0, 'lr': 3.426915319227129e-05, 'mc_samples_train': 2, 'prior_scale': 0.9713491325966092, 'q_scale': 0.00011989003863340673, 'obs_scale': 0.07411551140013035, 'batch_size': 128}. Best is trial 11 with value: -137.9308624267578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-10 15:28:06,648] Trial 14 pruned. Trial was pruned at epoch 223.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 18:11:03,509] Trial 15 finished with value: -25.811628341674805 and parameters: {'pretrain_epochs': 0, 'lr': 1.0003499066452017e-05, 'mc_samples_train': 2, 'prior_scale': 0.14028905483022555, 'q_scale': 0.0005351337564036122, 'obs_scale': 0.24770840369488722, 'batch_size': 64}. Best is trial 11 with value: -137.9308624267578.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 19:38:49,026] Trial 16 finished with value: -188.49461364746094 and parameters: {'pretrain_epochs': 0, 'lr': 4.884207813495069e-05, 'mc_samples_train': 2, 'prior_scale': 0.3785957853752036, 'q_scale': 0.0005845252980016993, 'obs_scale': 0.03804737433257131, 'batch_size': 128}. Best is trial 16 with value: -188.49461364746094.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 21:16:47,256] Trial 17 finished with value: -96.59168243408203 and parameters: {'pretrain_epochs': 0, 'lr': 0.00038765931042035386, 'mc_samples_train': 2, 'prior_scale': 0.4343628588344081, 'q_scale': 0.000551662647177075, 'obs_scale': 0.03315568936559784, 'batch_size': 128}. Best is trial 16 with value: -188.49461364746094.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-10 21:24:30,427] Trial 18 pruned. Trial was pruned at epoch 156.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-10 21:34:39,503] Trial 19 pruned. Trial was pruned at epoch 300.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-10 21:40:50,109] Trial 20 pruned. Trial was pruned at epoch 74.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-10 23:17:14,525] Trial 21 finished with value: -83.12610626220703 and parameters: {'pretrain_epochs': 0, 'lr': 1.554421813485148e-05, 'mc_samples_train': 2, 'prior_scale': 0.14476542296269565, 'q_scale': 0.00017542322464727102, 'obs_scale': 0.18365416213343191, 'batch_size': 128}. Best is trial 16 with value: -188.49461364746094.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 00:40:15,838] Trial 22 finished with value: -136.3812713623047 and parameters: {'pretrain_epochs': 0, 'lr': 3.3872947669490424e-05, 'mc_samples_train': 2, 'prior_scale': 0.43273462620143266, 'q_scale': 0.00027103806874359673, 'obs_scale': 0.08049730072875742, 'batch_size': 128}. Best is trial 16 with value: -188.49461364746094.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 01:58:52,116] Trial 23 finished with value: -143.91429138183594 and parameters: {'pretrain_epochs': 0, 'lr': 3.5478825632710625e-05, 'mc_samples_train': 2, 'prior_scale': 0.4330488602334728, 'q_scale': 0.0011774928251096944, 'obs_scale': 0.0535803958285364, 'batch_size': 128}. Best is trial 16 with value: -188.49461364746094.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 02:07:13,846] Trial 24 pruned. Trial was pruned at epoch 210.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 03:29:21,872] Trial 25 pruned. Trial was pruned at epoch 698.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 03:33:25,731] Trial 26 pruned. Trial was pruned at epoch 169.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 03:59:02,479] Trial 27 pruned. Trial was pruned at epoch 630.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 04:05:33,256] Trial 28 pruned. Trial was pruned at epoch 157.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 04:13:17,652] Trial 29 pruned. Trial was pruned at epoch 312.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 04:17:21,753] Trial 30 pruned. Trial was pruned at epoch 32.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 05:46:41,186] Trial 31 finished with value: -137.91697692871094 and parameters: {'pretrain_epochs': 0, 'lr': 3.046969806246848e-05, 'mc_samples_train': 2, 'prior_scale': 0.3685714140127881, 'q_scale': 0.0002803378799238707, 'obs_scale': 0.09641286009172191, 'batch_size': 128}. Best is trial 16 with value: -188.49461364746094.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 07:17:44,977] Trial 32 finished with value: -196.8789520263672 and parameters: {'pretrain_epochs': 0, 'lr': 7.189310828280248e-05, 'mc_samples_train': 2, 'prior_scale': 0.3096382352319342, 'q_scale': 0.0009643342929542669, 'obs_scale': 0.048927991669799116, 'batch_size': 128}. Best is trial 32 with value: -196.8789520263672.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 07:25:20,348] Trial 33 pruned. Trial was pruned at epoch 176.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 07:33:34,647] Trial 34 pruned. Trial was pruned at epoch 172.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 09:07:01,274] Trial 35 finished with value: -95.09027862548828 and parameters: {'pretrain_epochs': 0, 'lr': 8.892138214422471e-05, 'mc_samples_train': 2, 'prior_scale': 0.060892016266996626, 'q_scale': 0.0031527035490130488, 'obs_scale': 0.16389837195335485, 'batch_size': 128}. Best is trial 32 with value: -196.8789520263672.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 09:10:36,916] Trial 36 pruned. Trial was pruned at epoch 155.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 09:32:48,327] Trial 37 pruned. Trial was pruned at epoch 734.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 09:34:48,834] Trial 38 pruned. Trial was pruned at epoch 71.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 09:53:06,475] Trial 39 pruned. Trial was pruned at epoch 136.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 11:34:11,326] Trial 40 finished with value: -82.80487823486328 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006180881794674558, 'mc_samples_train': 1, 'prior_scale': 0.6594716365513221, 'q_scale': 0.0004270100208835666, 'obs_scale': 0.05252412068747322, 'batch_size': 64}. Best is trial 32 with value: -196.8789520263672.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 13:16:01,747] Trial 41 finished with value: -127.63471984863281 and parameters: {'pretrain_epochs': 0, 'lr': 2.7637808617877186e-05, 'mc_samples_train': 2, 'prior_scale': 0.31982323882285774, 'q_scale': 0.00010102698116099761, 'obs_scale': 0.1228748479996835, 'batch_size': 128}. Best is trial 32 with value: -196.8789520263672.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 13:24:06,796] Trial 42 pruned. Trial was pruned at epoch 170.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 14:57:21,903] Trial 43 finished with value: -122.6467514038086 and parameters: {'pretrain_epochs': 0, 'lr': 7.751454597234306e-05, 'mc_samples_train': 2, 'prior_scale': 0.17356928270353214, 'q_scale': 0.0007248382042715108, 'obs_scale': 0.12397874566775884, 'batch_size': 128}. Best is trial 32 with value: -196.8789520263672.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 15:44:21,033] Trial 44 finished with value: -681.6134033203125 and parameters: {'pretrain_epochs': 0, 'lr': 1.3265605241311912e-05, 'mc_samples_train': 2, 'prior_scale': 0.34950577673579863, 'q_scale': 0.0001766287299973241, 'obs_scale': 0.09431618719246625, 'batch_size': 512}. Best is trial 44 with value: -681.6134033203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 15:44:38,908] Trial 45 pruned. Trial was pruned at epoch 6.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 16:01:21,353] Trial 46 pruned. Trial was pruned at epoch 721.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 16:01:34,152] Trial 47 pruned. Trial was pruned at epoch 3.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 16:25:02,684] Trial 48 pruned. Trial was pruned at epoch 1007.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 16:27:59,493] Trial 49 pruned. Trial was pruned at epoch 138.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 16:56:04,226] Trial 50 pruned. Trial was pruned at epoch 1120.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 18:33:07,835] Trial 51 finished with value: -151.64404296875 and parameters: {'pretrain_epochs': 0, 'lr': 3.0959847134748525e-05, 'mc_samples_train': 2, 'prior_scale': 0.36356782177531866, 'q_scale': 0.0003667217333469405, 'obs_scale': 0.08834644344870037, 'batch_size': 128}. Best is trial 44 with value: -681.6134033203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 18:48:25,598] Trial 52 pruned. Trial was pruned at epoch 347.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 18:58:26,247] Trial 53 pruned. Trial was pruned at epoch 224.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 19:46:13,412] Trial 54 pruned. Trial was pruned at epoch 615.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 20:11:48,308] Trial 55 pruned. Trial was pruned at epoch 545.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-11 21:45:21,840] Trial 56 finished with value: -111.1927490234375 and parameters: {'pretrain_epochs': 0, 'lr': 4.874333547532515e-05, 'mc_samples_train': 2, 'prior_scale': 0.549752453676041, 'q_scale': 0.0008193131746662824, 'obs_scale': 0.08570759445425542, 'batch_size': 128}. Best is trial 44 with value: -681.6134033203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-11 23:12:45,911] Trial 57 pruned. Trial was pruned at epoch 645.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-12 00:13:48,606] Trial 58 finished with value: -278.8021545410156 and parameters: {'pretrain_epochs': 0, 'lr': 0.00016757607450412502, 'mc_samples_train': 2, 'prior_scale': 0.26960469101215995, 'q_scale': 0.0017248765248022743, 'obs_scale': 0.06916467727661295, 'batch_size': 256}. Best is trial 44 with value: -681.6134033203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-12 00:18:19,386] Trial 59 pruned. Trial was pruned at epoch 140.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-05-15 10:34:01,527] A new study created in RDB with name: hom_lrt_bigrange
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:260: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  scale = predictions.var(dim).add(self.scale ** 2).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-05-15 11:12:12,364] A new study created in RDB with name: hom_lrt_small_newspace
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:260: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  scale = predictions.var(dim).add(self.scale ** 2).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/hydra/_internal/instantiate/_instantiate2.py:92: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.
  return _target_(*args, **kwargs)
[I 2025-05-15 11:18:47,057] Using an existing study with name 'hom_lrt_small_newspace' instead of creating a new one.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/tyxe/likelihoods.py:260: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)
  scale = predictions.var(dim).add(self.scale ** 2).sqrt()
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 11:25:39,597] Trial 1 finished with value: 15197565952.0 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 6.10040381100861, 'q_scale': 0.021873960541613266, 'obs_scale': 0.002306550812550155, 'batch_size': 128}. Best is trial 1 with value: 15197565952.0.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 11:34:34,288] Trial 0 finished with value: 989234560.0 and parameters: {'pretrain_epochs': 0, 'lr': 2.4156772025296228e-05, 'mc_samples_train': 1, 'prior_scale': 6.10040381100861, 'q_scale': 0.021873960541613266, 'obs_scale': 0.002306550812550155, 'batch_size': 128}. Best is trial 0 with value: 989234560.0.
[I 2025-05-15 11:34:34,613] Trial 2 finished with value: 173707.515625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00010045932391231576, 'mc_samples_train': 2, 'prior_scale': 2.3455827269243748, 'q_scale': 0.004826013786176772, 'obs_scale': 0.03276542658291345, 'batch_size': 128}. Best is trial 2 with value: 173707.515625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 11:43:21,995] Trial 3 finished with value: 17385.76953125 and parameters: {'pretrain_epochs': 0, 'lr': 1.4150196905720475e-05, 'mc_samples_train': 2, 'prior_scale': 4.48103301026688, 'q_scale': 0.0015545608747938977, 'obs_scale': 0.8783158391722686, 'batch_size': 128}. Best is trial 3 with value: 17385.76953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 11:59:42,494] Trial 4 finished with value: 1417281.125 and parameters: {'pretrain_epochs': 0, 'lr': 1.9388028480984598e-05, 'mc_samples_train': 2, 'prior_scale': 1.6549775453786335, 'q_scale': 0.059511772548294796, 'obs_scale': 0.016244403817471303, 'batch_size': 32}. Best is trial 3 with value: 17385.76953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 12:05:41,724] Trial 5 finished with value: 2402148.0 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001546142648648772, 'mc_samples_train': 1, 'prior_scale': 3.6427811454849492, 'q_scale': 0.0009750016190297515, 'obs_scale': 0.0327215724382531, 'batch_size': 512}. Best is trial 3 with value: 17385.76953125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 12:06:48,796] Trial 6 pruned. Trial was pruned at epoch 28.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 12:13:03,139] Trial 7 finished with value: 10585.8037109375 and parameters: {'pretrain_epochs': 0, 'lr': 8.003810848108467e-05, 'mc_samples_train': 1, 'prior_scale': 1.316378855976843, 'q_scale': 0.016427412719943545, 'obs_scale': 0.08640088029719115, 'batch_size': 256}. Best is trial 7 with value: 10585.8037109375.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 12:20:32,359] Trial 8 finished with value: 1609.0760498046875 and parameters: {'pretrain_epochs': 0, 'lr': 0.00011795146111975982, 'mc_samples_train': 2, 'prior_scale': 2.734595987222456, 'q_scale': 0.006861941183128952, 'obs_scale': 3.8993334643772415, 'batch_size': 512}. Best is trial 8 with value: 1609.0760498046875.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 12:27:37,133] Trial 9 finished with value: 448.7425231933594 and parameters: {'pretrain_epochs': 0, 'lr': 0.00018511923116382368, 'mc_samples_train': 1, 'prior_scale': 3.701771566369369, 'q_scale': 0.003843021664581486, 'obs_scale': 5.716679159905746, 'batch_size': 256}. Best is trial 9 with value: 448.7425231933594.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 12:34:31,882] Trial 10 finished with value: 1077.7984619140625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00021819434272647882, 'mc_samples_train': 1, 'prior_scale': 6.26422262434414, 'q_scale': 0.004712837715557199, 'obs_scale': 6.7481178717922905, 'batch_size': 512}. Best is trial 9 with value: 448.7425231933594.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 12:35:33,809] Trial 11 pruned. Trial was pruned at epoch 89.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 12:40:56,323] Trial 12 finished with value: 568.8834838867188 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003201413211508342, 'mc_samples_train': 1, 'prior_scale': 6.013767809058866, 'q_scale': 0.000587584065566266, 'obs_scale': 6.666855352870317, 'batch_size': 512}. Best is trial 9 with value: 448.7425231933594.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 12:49:49,111] Trial 13 finished with value: 1080.0352783203125 and parameters: {'pretrain_epochs': 0, 'lr': 0.00031379702063305813, 'mc_samples_train': 1, 'prior_scale': 4.838581812629556, 'q_scale': 0.0002629246494268504, 'obs_scale': 0.5741030243571359, 'batch_size': 64}. Best is trial 9 with value: 448.7425231933594.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 12:54:54,621] Trial 14 pruned. Trial was pruned at epoch 437.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-15 12:58:50,660] Trial 1 finished with value: 5637.80322265625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00010045932391231576, 'mc_samples_train': 2, 'prior_scale': 2.3455827269243748, 'q_scale': 0.004826013786176772, 'obs_scale': 0.03276542658291345, 'batch_size': 128}. Best is trial 1 with value: 5637.80322265625.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 13:03:33,166] Trial 15 finished with value: 232.0890350341797 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004012358719846703, 'mc_samples_train': 1, 'prior_scale': 2.1791446264338408, 'q_scale': 0.0018815840503807549, 'obs_scale': 9.380010481040799, 'batch_size': 64}. Best is trial 15 with value: 232.0890350341797.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 13:11:32,437] Trial 16 finished with value: 220.98475646972656 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004200396467372965, 'mc_samples_train': 1, 'prior_scale': 2.0384408298837555, 'q_scale': 0.002133566126351855, 'obs_scale': 0.21908571532961182, 'batch_size': 64}. Best is trial 16 with value: 220.98475646972656.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 13:19:13,143] Trial 17 finished with value: 358.2311706542969 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004726028568708066, 'mc_samples_train': 1, 'prior_scale': 1.9949091678096584, 'q_scale': 0.0020990266261572974, 'obs_scale': 0.20432716715920374, 'batch_size': 64}. Best is trial 16 with value: 220.98475646972656.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 13:27:37,505] Trial 18 finished with value: 407.7785339355469 and parameters: {'pretrain_epochs': 0, 'lr': 0.00038765931042035386, 'mc_samples_train': 1, 'prior_scale': 1.8452981197961493, 'q_scale': 0.009038950645092924, 'obs_scale': 0.22868230384179622, 'batch_size': 64}. Best is trial 16 with value: 220.98475646972656.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 13:35:37,357] Trial 19 finished with value: 107.1319808959961 and parameters: {'pretrain_epochs': 0, 'lr': 0.0008087548008284178, 'mc_samples_train': 1, 'prior_scale': 1.4370113365243, 'q_scale': 0.0014457685462721688, 'obs_scale': 1.5298139169157052, 'batch_size': 64}. Best is trial 19 with value: 107.1319808959961.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 13:44:11,262] Trial 20 finished with value: 92.1880874633789 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009668688900184896, 'mc_samples_train': 1, 'prior_scale': 1.486430727119093, 'q_scale': 0.0003729091193968364, 'obs_scale': 1.188435747984914, 'batch_size': 64}. Best is trial 20 with value: 92.1880874633789.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 13:53:16,949] Trial 21 finished with value: 111.15059661865234 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009741188514849866, 'mc_samples_train': 1, 'prior_scale': 1.4186388680823283, 'q_scale': 0.00022424930462467066, 'obs_scale': 1.677584463003035, 'batch_size': 64}. Best is trial 20 with value: 92.1880874633789.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 14:00:49,919] Trial 22 finished with value: 108.00186920166016 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009833364009851503, 'mc_samples_train': 1, 'prior_scale': 1.4224087767370361, 'q_scale': 0.00021331330678444192, 'obs_scale': 1.582765899157312, 'batch_size': 64}. Best is trial 20 with value: 92.1880874633789.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 14:08:54,875] Trial 23 finished with value: 121.70600891113281 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009141783542577661, 'mc_samples_train': 1, 'prior_scale': 1.0582133570486707, 'q_scale': 0.0003397364430724783, 'obs_scale': 1.9827417175597677, 'batch_size': 64}. Best is trial 20 with value: 92.1880874633789.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 14:17:34,970] Trial 24 finished with value: 32.27064514160156 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006069981109078901, 'mc_samples_train': 1, 'prior_scale': 1.3701269278990773, 'q_scale': 0.00012616225178961078, 'obs_scale': 0.45210334181790124, 'batch_size': 64}. Best is trial 24 with value: 32.27064514160156.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-15 14:23:48,178] Trial 2 finished with value: 1275.90283203125 and parameters: {'pretrain_epochs': 0, 'lr': 1.4150196905720475e-05, 'mc_samples_train': 2, 'prior_scale': 4.48103301026688, 'q_scale': 0.0015545608747938977, 'obs_scale': 0.8783158391722686, 'batch_size': 128}. Best is trial 2 with value: 1275.90283203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 14:25:06,312] Trial 25 finished with value: 33.73063659667969 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005690681077489627, 'mc_samples_train': 1, 'prior_scale': 1.2202754731889462, 'q_scale': 0.0005136459063409122, 'obs_scale': 0.47639982046185597, 'batch_size': 64}. Best is trial 24 with value: 32.27064514160156.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 14:32:29,147] Trial 26 finished with value: 27.966567993164062 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005522021623669313, 'mc_samples_train': 1, 'prior_scale': 1.1521605384189892, 'q_scale': 0.00010512849925947591, 'obs_scale': 0.30705780155360796, 'batch_size': 64}. Best is trial 26 with value: 27.966567993164062.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 14:36:56,162] Trial 27 pruned. Trial was pruned at epoch 216.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 14:47:53,945] Trial 28 finished with value: 12.850348472595215 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005256749849349466, 'mc_samples_train': 1, 'prior_scale': 1.006199039931011, 'q_scale': 0.0001637674913709052, 'obs_scale': 0.35567153783613414, 'batch_size': 32}. Best is trial 28 with value: 12.850348472595215.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 14:50:22,401] Trial 29 pruned. Trial was pruned at epoch 108.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 14:53:30,739] Trial 30 pruned. Trial was pruned at epoch 143.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 15:03:57,773] Trial 31 finished with value: 90.37602233886719 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005388056324201093, 'mc_samples_train': 1, 'prior_scale': 2.565945705006257, 'q_scale': 0.00010665151378682466, 'obs_scale': 0.3406226357808599, 'batch_size': 32}. Best is trial 28 with value: 12.850348472595215.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 15:09:10,937] Trial 32 finished with value: 59.522850036621094 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005931435942335716, 'mc_samples_train': 1, 'prior_scale': 1.253304736499758, 'q_scale': 0.0005874543929289564, 'obs_scale': 0.4526840191266038, 'batch_size': 128}. Best is trial 28 with value: 12.850348472595215.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 15:10:15,943] Trial 33 pruned. Trial was pruned at epoch 44.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 15:15:35,618] Trial 34 finished with value: 120.1719741821289 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002824378284938445, 'mc_samples_train': 1, 'prior_scale': 1.6757907287423381, 'q_scale': 0.0001861112875094787, 'obs_scale': 0.4851901060017365, 'batch_size': 128}. Best is trial 28 with value: 12.850348472595215.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 15:25:14,273] Trial 35 finished with value: 25.26595115661621 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006604477588766001, 'mc_samples_train': 2, 'prior_scale': 1.1664651855665698, 'q_scale': 0.0009125017909566811, 'obs_scale': 0.13659158934556742, 'batch_size': 64}. Best is trial 28 with value: 12.850348472595215.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 15:30:11,669] Trial 36 pruned. Trial was pruned at epoch 153.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 15:36:22,031] Trial 37 finished with value: 6.439757823944092 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007044783661020397, 'mc_samples_train': 2, 'prior_scale': 1.0210792875063517, 'q_scale': 0.0003013335510297768, 'obs_scale': 0.14357866000590572, 'batch_size': 128}. Best is trial 37 with value: 6.439757823944092.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 15:42:17,270] Trial 38 pruned. Trial was pruned at epoch 443.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 15:49:02,486] Trial 39 finished with value: 95599.0390625 and parameters: {'pretrain_epochs': 0, 'lr': 2.5400247166969838e-05, 'mc_samples_train': 2, 'prior_scale': 1.0013499493797713, 'q_scale': 0.00031409269302168553, 'obs_scale': 0.011206425308415832, 'batch_size': 128}. Best is trial 37 with value: 6.439757823944092.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 15:49:58,093] Trial 40 pruned. Trial was pruned at epoch 62.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 15:51:52,817] Trial 41 pruned. Trial was pruned at epoch 137.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 15:52:08,710] Trial 42 pruned. Trial was pruned at epoch 7.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 16:06:23,422] Trial 43 finished with value: 79.98507690429688 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003696049053799565, 'mc_samples_train': 2, 'prior_scale': 1.1254887761563712, 'q_scale': 0.00015304843628748336, 'obs_scale': 3.163365727649365, 'batch_size': 32}. Best is trial 37 with value: 6.439757823944092.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 16:11:20,654] Trial 44 pruned. Trial was pruned at epoch 292.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 16:12:34,065] Trial 45 pruned. Trial was pruned at epoch 111.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 16:21:29,470] Trial 46 finished with value: 14.51132869720459 and parameters: {'pretrain_epochs': 0, 'lr': 0.000626806049342454, 'mc_samples_train': 2, 'prior_scale': 1.1304341139141614, 'q_scale': 0.0004402910185547618, 'obs_scale': 0.30719648499374236, 'batch_size': 64}. Best is trial 37 with value: 6.439757823944092.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 16:22:55,690] Trial 47 pruned. Trial was pruned at epoch 96.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 16:25:40,135] Trial 48 pruned. Trial was pruned at epoch 272.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 16:28:10,836] Trial 49 pruned. Trial was pruned at epoch 85.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 16:29:31,380] Trial 50 pruned. Trial was pruned at epoch 59.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 16:34:32,083] Trial 51 finished with value: 110.66643524169922 and parameters: {'pretrain_epochs': 0, 'lr': 0.0007837641102909872, 'mc_samples_train': 2, 'prior_scale': 1.1139510435604374, 'q_scale': 0.0004507052366760774, 'obs_scale': 0.7505975796519628, 'batch_size': 256}. Best is trial 37 with value: 6.439757823944092.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 16:43:21,173] Trial 52 finished with value: 41.66533660888672 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006369191310024562, 'mc_samples_train': 2, 'prior_scale': 1.3420835647749638, 'q_scale': 0.000286226844981477, 'obs_scale': 0.2893232813188484, 'batch_size': 64}. Best is trial 37 with value: 6.439757823944092.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 16:44:47,184] Trial 53 pruned. Trial was pruned at epoch 81.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 16:54:04,890] Trial 54 finished with value: 74.34581756591797 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004637537315809219, 'mc_samples_train': 2, 'prior_scale': 1.002782071491227, 'q_scale': 0.00010404107766598972, 'obs_scale': 0.9521716742955046, 'batch_size': 64}. Best is trial 37 with value: 6.439757823944092.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 16:54:21,287] Trial 55 pruned. Trial was pruned at epoch 11.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=500` reached.
[I 2025-05-15 17:00:26,612] Trial 56 finished with value: 19.651775360107422 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003694506284832782, 'mc_samples_train': 1, 'prior_scale': 1.111475566235369, 'q_scale': 0.0004140370868207283, 'obs_scale': 0.38440462181347446, 'batch_size': 64}. Best is trial 37 with value: 6.439757823944092.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 17:10:50,091] Trial 57 pruned. Trial was pruned at epoch 285.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 17:12:12,010] Trial 58 pruned. Trial was pruned at epoch 95.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 17:13:40,437] Trial 59 pruned. Trial was pruned at epoch 180.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 17:14:20,661] Trial 60 pruned. Trial was pruned at epoch 44.
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-15 18:17:19,485] Trial 3 finished with value: 50650.87890625 and parameters: {'pretrain_epochs': 0, 'lr': 1.9388028480984598e-05, 'mc_samples_train': 2, 'prior_scale': 1.6549775453786335, 'q_scale': 0.059511772548294796, 'obs_scale': 0.016244403817471303, 'batch_size': 32}. Best is trial 2 with value: 1275.90283203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-15 18:44:20,282] Trial 4 finished with value: 606964.1875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0001546142648648772, 'mc_samples_train': 1, 'prior_scale': 3.6427811454849492, 'q_scale': 0.0009750016190297515, 'obs_scale': 0.0327215724382531, 'batch_size': 512}. Best is trial 2 with value: 1275.90283203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 18:51:24,288] Trial 5 pruned. Trial was pruned at epoch 61.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/g15farris/.conda/envs/bnn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-15 19:24:51,416] Trial 6 finished with value: 10938.9287109375 and parameters: {'pretrain_epochs': 0, 'lr': 8.003810848108467e-05, 'mc_samples_train': 1, 'prior_scale': 1.316378855976843, 'q_scale': 0.016427412719943545, 'obs_scale': 0.08640088029719115, 'batch_size': 256}. Best is trial 2 with value: 1275.90283203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-15 20:02:27,199] Trial 7 finished with value: 1633.025390625 and parameters: {'pretrain_epochs': 0, 'lr': 0.00011795146111975982, 'mc_samples_train': 2, 'prior_scale': 2.734595987222456, 'q_scale': 0.006861941183128952, 'obs_scale': 3.8993334643772415, 'batch_size': 512}. Best is trial 2 with value: 1275.90283203125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-15 20:34:49,678] Trial 8 finished with value: 629.0326538085938 and parameters: {'pretrain_epochs': 0, 'lr': 0.00018511923116382368, 'mc_samples_train': 1, 'prior_scale': 3.701771566369369, 'q_scale': 0.003843021664581486, 'obs_scale': 5.716679159905746, 'batch_size': 256}. Best is trial 8 with value: 629.0326538085938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-15 21:01:33,697] Trial 9 finished with value: 2080.776123046875 and parameters: {'pretrain_epochs': 0, 'lr': 0.00021819434272647882, 'mc_samples_train': 1, 'prior_scale': 6.26422262434414, 'q_scale': 0.004712837715557199, 'obs_scale': 6.7481178717922905, 'batch_size': 512}. Best is trial 8 with value: 629.0326538085938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-15 21:02:17,976] Trial 10 pruned. Trial was pruned at epoch 34.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-15 22:19:32,549] Trial 11 finished with value: 2795.728271484375 and parameters: {'pretrain_epochs': 0, 'lr': 1.1537437375871489e-05, 'mc_samples_train': 2, 'prior_scale': 4.330742478220608, 'q_scale': 0.0008300101842799139, 'obs_scale': 0.7061140731600912, 'batch_size': 128}. Best is trial 8 with value: 629.0326538085938.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-16 00:34:11,818] Trial 12 finished with value: 428.2314453125 and parameters: {'pretrain_epochs': 0, 'lr': 4.758304712295703e-05, 'mc_samples_train': 2, 'prior_scale': 4.856006343613426, 'q_scale': 0.001030925833118815, 'obs_scale': 0.7088999768594483, 'batch_size': 64}. Best is trial 12 with value: 428.2314453125.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-16 01:46:45,709] Trial 13 pruned. Trial was pruned at epoch 1767.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-16 04:06:10,133] Trial 14 finished with value: 203.72837829589844 and parameters: {'pretrain_epochs': 0, 'lr': 0.0002580404985341335, 'mc_samples_train': 2, 'prior_scale': 2.1791446264338408, 'q_scale': 0.000427830794601344, 'obs_scale': 9.574905454632638, 'batch_size': 64}. Best is trial 14 with value: 203.72837829589844.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-16 06:31:15,111] Trial 15 finished with value: -27.96796989440918 and parameters: {'pretrain_epochs': 0, 'lr': 0.0003591012416576731, 'mc_samples_train': 2, 'prior_scale': 2.0384408298837555, 'q_scale': 0.00035809070187242095, 'obs_scale': 0.22160903410519733, 'batch_size': 64}. Best is trial 15 with value: -27.96796989440918.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-16 08:48:15,854] Trial 16 finished with value: -39.65057373046875 and parameters: {'pretrain_epochs': 0, 'lr': 0.00033370459474621456, 'mc_samples_train': 2, 'prior_scale': 1.9949091678096584, 'q_scale': 0.00031337897669181657, 'obs_scale': 0.17300273363355814, 'batch_size': 64}. Best is trial 16 with value: -39.65057373046875.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-16 11:06:18,648] Trial 17 finished with value: -54.217491149902344 and parameters: {'pretrain_epochs': 0, 'lr': 0.00038888020024747065, 'mc_samples_train': 2, 'prior_scale': 1.7622417874511072, 'q_scale': 0.00012801123487127776, 'obs_scale': 0.12830479633397657, 'batch_size': 64}. Best is trial 17 with value: -54.217491149902344.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-16 11:08:54,265] Trial 18 pruned. Trial was pruned at epoch 29.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-16 13:23:39,774] Trial 19 finished with value: -50.98189926147461 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009389829043943205, 'mc_samples_train': 2, 'prior_scale': 1.512103119895933, 'q_scale': 0.00020863203185700482, 'obs_scale': 0.15255869353523924, 'batch_size': 64}. Best is trial 17 with value: -54.217491149902344.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-16 13:26:07,867] Trial 20 pruned. Trial was pruned at epoch 29.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-16 15:50:05,125] Trial 21 finished with value: -53.069252014160156 and parameters: {'pretrain_epochs': 0, 'lr': 0.00039425816574514984, 'mc_samples_train': 2, 'prior_scale': 1.6098632021911004, 'q_scale': 0.00024386451277926541, 'obs_scale': 0.1577096548215707, 'batch_size': 64}. Best is trial 17 with value: -54.217491149902344.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-16 18:19:55,105] Trial 22 finished with value: -53.227256774902344 and parameters: {'pretrain_epochs': 0, 'lr': 0.000584358751788919, 'mc_samples_train': 2, 'prior_scale': 1.424927458827327, 'q_scale': 0.0002123682410945653, 'obs_scale': 0.10544993303035524, 'batch_size': 64}. Best is trial 17 with value: -54.217491149902344.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-16 21:01:49,029] Trial 23 finished with value: -41.54705810546875 and parameters: {'pretrain_epochs': 0, 'lr': 0.0004923283124356126, 'mc_samples_train': 2, 'prior_scale': 1.3699140545681905, 'q_scale': 0.0004938130048929932, 'obs_scale': 0.0582752879098563, 'batch_size': 64}. Best is trial 17 with value: -54.217491149902344.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-16 21:05:24,599] Trial 24 pruned. Trial was pruned at epoch 40.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-16 23:32:01,385] Trial 25 finished with value: -18.448448181152344 and parameters: {'pretrain_epochs': 0, 'lr': 0.00028579231459383296, 'mc_samples_train': 2, 'prior_scale': 1.8520587873209926, 'q_scale': 0.0006158168735045409, 'obs_scale': 0.26998903832593946, 'batch_size': 64}. Best is trial 17 with value: -54.217491149902344.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-17 03:18:19,370] Trial 26 finished with value: 53.88884353637695 and parameters: {'pretrain_epochs': 0, 'lr': 0.000603287349452318, 'mc_samples_train': 2, 'prior_scale': 2.591723736999003, 'q_scale': 0.0018465216491067545, 'obs_scale': 2.1094872074674034, 'batch_size': 32}. Best is trial 17 with value: -54.217491149902344.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 03:25:42,502] Trial 27 pruned. Trial was pruned at epoch 116.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 03:30:09,748] Trial 28 pruned. Trial was pruned at epoch 68.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 03:31:42,085] Trial 29 pruned. Trial was pruned at epoch 21.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-17 07:39:27,918] Trial 30 finished with value: 3.7162153720855713 and parameters: {'pretrain_epochs': 0, 'lr': 6.684274405846161e-05, 'mc_samples_train': 2, 'prior_scale': 1.5167114490946971, 'q_scale': 0.0005737943823614455, 'obs_scale': 0.3962658843721328, 'batch_size': 32}. Best is trial 17 with value: -54.217491149902344.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-17 09:38:05,834] Trial 31 finished with value: -60.091495513916016 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009572472725206141, 'mc_samples_train': 2, 'prior_scale': 1.4403371206497544, 'q_scale': 0.0001904111536190179, 'obs_scale': 0.13803458999151103, 'batch_size': 64}. Best is trial 31 with value: -60.091495513916016.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-17 11:35:01,122] Trial 32 finished with value: 92.1330795288086 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009674074970189419, 'mc_samples_train': 2, 'prior_scale': 1.4604731571882343, 'q_scale': 0.00016352029904680303, 'obs_scale': 1.6663871268587633, 'batch_size': 64}. Best is trial 31 with value: -60.091495513916016.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 11:36:18,041] Trial 33 pruned. Trial was pruned at epoch 32.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 11:37:58,371] Trial 34 pruned. Trial was pruned at epoch 25.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 11:41:15,034] Trial 35 pruned. Trial was pruned at epoch 93.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 11:43:07,327] Trial 36 pruned. Trial was pruned at epoch 110.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 11:48:29,064] Trial 37 pruned. Trial was pruned at epoch 87.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 11:50:16,638] Trial 38 pruned. Trial was pruned at epoch 117.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-17 15:16:53,788] Trial 39 finished with value: -27.215843200683594 and parameters: {'pretrain_epochs': 0, 'lr': 0.00021538428089371814, 'mc_samples_train': 2, 'prior_scale': 1.6170303061663525, 'q_scale': 0.00021810873383498828, 'obs_scale': 0.11346926113639255, 'batch_size': 32}. Best is trial 31 with value: -60.091495513916016.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 15:17:20,682] Trial 40 pruned. Trial was pruned at epoch 19.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 15:18:52,227] Trial 41 pruned. Trial was pruned at epoch 25.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-17 17:08:01,752] Trial 42 finished with value: -24.41724967956543 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005714042648626631, 'mc_samples_train': 2, 'prior_scale': 1.903236394268488, 'q_scale': 0.00013189123044558075, 'obs_scale': 0.21836606711504933, 'batch_size': 64}. Best is trial 31 with value: -60.091495513916016.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 17:10:18,050] Trial 43 pruned. Trial was pruned at epoch 35.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-17 19:09:17,764] Trial 44 finished with value: 7.9832377433776855 and parameters: {'pretrain_epochs': 0, 'lr': 0.0005540840673019842, 'mc_samples_train': 2, 'prior_scale': 1.4075558237759545, 'q_scale': 0.0002901133980800174, 'obs_scale': 0.4223511973662088, 'batch_size': 64}. Best is trial 31 with value: -60.091495513916016.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-17 19:56:37,405] Trial 45 finished with value: -193.52183532714844 and parameters: {'pretrain_epochs': 0, 'lr': 0.0009466820778744179, 'mc_samples_train': 2, 'prior_scale': 1.1639862578024707, 'q_scale': 0.0001319366234617197, 'obs_scale': 0.15950250120290407, 'batch_size': 256}. Best is trial 45 with value: -193.52183532714844.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 19:58:35,750] Trial 46 pruned. Trial was pruned at epoch 93.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 19:59:55,964] Trial 47 pruned. Trial was pruned at epoch 62.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 20:01:02,543] Trial 48 pruned. Trial was pruned at epoch 41.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 20:04:55,313] Trial 49 pruned. Trial was pruned at epoch 171.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-17 20:54:04,687] Trial 50 finished with value: -52.29701232910156 and parameters: {'pretrain_epochs': 0, 'lr': 0.00010561090099968037, 'mc_samples_train': 1, 'prior_scale': 1.1102430459668788, 'q_scale': 0.00048413799569015076, 'obs_scale': 0.19720890494533935, 'batch_size': 128}. Best is trial 45 with value: -193.52183532714844.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 20:57:50,242] Trial 51 pruned. Trial was pruned at epoch 152.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 21:01:23,292] Trial 52 pruned. Trial was pruned at epoch 135.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 21:09:15,656] Trial 53 pruned. Trial was pruned at epoch 324.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 21:20:32,617] Trial 54 pruned. Trial was pruned at epoch 462.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 21:22:20,859] Trial 55 pruned. Trial was pruned at epoch 110.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 21:35:43,662] Trial 56 pruned. Trial was pruned at epoch 743.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer.fit` stopped: `max_epochs=2000` reached.
[I 2025-05-17 22:48:21,991] Trial 57 finished with value: -32.36250686645508 and parameters: {'pretrain_epochs': 0, 'lr': 0.0006480670950893355, 'mc_samples_train': 2, 'prior_scale': 1.2543831109081807, 'q_scale': 0.0003267409539088462, 'obs_scale': 0.2883188793738213, 'batch_size': 128}. Best is trial 45 with value: -193.52183532714844.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 22:51:32,691] Trial 58 pruned. Trial was pruned at epoch 47.
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[I 2025-05-17 22:54:46,016] Trial 59 pruned. Trial was pruned at epoch 47.
